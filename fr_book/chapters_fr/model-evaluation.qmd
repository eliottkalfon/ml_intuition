---
title: "Évaluation des Modèles"
---

Nous avons maintenant construit notre premier modèle, mais est-il vraiment bon ? Pour répondre à cette question, revenons à la fonction principale d'un modèle de Machine Learning. Elle est de générer des prédictions pour de **nouvelles observations** qui soient aussi **proches** de la vérité que possible.

$$
\text{Entrée} \longrightarrow \text{Modèle} \longrightarrow \text{Prédiction}
$$

Un bon modèle fait des prédictions avec une **faible distance** par rapport à la vérité.

En tenant compte de cela, comment pouvons-nous déterminer quel modèle remplit le mieux sa fonction ?

Il y a deux éléments importants à considérer :

- Les nouvelles observations ou données non vues
- La proximité avec la vérité

## Données Non Vues

Les données non vues sont, par définition, **non vues**. Nous ne pouvons pas y avoir accès, car dès que nous les voyons, elles deviennent **vues**. Cela signifie-t-il que nous ne pouvons pas évaluer un modèle sur des données non vues ? Devons-nous simplement attendre que de nouvelles observations arrivent ?

### Séparation Entraînement-Test

Une façon d'estimer la performance d'un modèle sur de nouvelles données est de mettre de côté une **portion des données d'entraînement** pour les tests. Celle-ci restera invisible pour le modèle et ne sera **pas** utilisée pour l'entraînement.

À la fin du processus d'entraînement, le modèle entraîné (comme [K-Plus Proches Voisins](neighbours.qmd){target=_blank}) peut être utilisé pour générer des prédictions sur les données de test. Pour cette portion de données non vues, nous avons à la fois les prédictions du modèle et la vraie étiquette.

Un bon modèle aura des prédictions **aussi proches que possible** des vraies étiquettes.

### Représentativité

#### Simuler les Conditions de Prédiction

Nous utilisons un ensemble de test pour **estimer** la performance du modèle sur des données non vues.

Pour que cette estimation ait un sens, l'ensemble de test doit être une **représentation fidèle** de ce à quoi ressembleront ces données non vues.

Pour rendre cela plus concret, pour évaluer un modèle entraîné à prédire les prix immobiliers aux États-Unis, cela n'a aucun sens d'utiliser des appartements berlinois comme ensemble de test. C'est un exemple un peu extrême, mais il illustre l'idée principale.

Dans le processus d'entraînement, le modèle apprend la relation entre l'entrée et la sortie en utilisant les observations des données d'entraînement.

$$
\text{Entrée} \longrightarrow \text{Modèle} \longrightarrow \text{Prédiction}
$$

Si les données d'entraînement ne sont pas représentatives des données sur lesquelles le modèle générera des prédictions, il ne peut pas apprendre correctement la relation entre l'entrée et la sortie.

Si les données de test ne sont pas représentatives des données non vues sur lesquelles le modèle sera utilisé pour générer des prédictions, la performance du modèle sur cet ensemble de données ne sera pas une bonne estimation de la performance future.

#### Assurer la Représentativité

Mais comment s'assurer que les ensembles d'entraînement et de test sont **cohérents** ? Un seul jeu de données peut contenir beaucoup de variations. Dans l'exemple de la tarification immobilière, il peut y avoir de nombreux types de propriétés avec des caractéristiques très différentes (par exemple, la surface ou le nombre de pièces). Certaines sont plus faciles à évaluer que d'autres. Comment s'assurer que l'ensemble d'entraînement et l'ensemble de test se ressemblent ?

Ici, la **loi des grands nombres** vient à la rescousse. La loi des grands nombres stipule que si nous **échantillonnons aléatoirement** un nombre suffisamment grand d'observations d'une population (ici, les données d'entraînement), l'échantillon sélectionné aléatoirement sera représentatif de la population d'origine.

C'est le fondement statistique des **sondages**. Avant les élections, les instituts de sondage **échantillonnent aléatoirement** un grand groupe de personnes pour obtenir un échantillon représentatif de la population.

::: {.callout-note collapse="true"}
## Quelques problèmes avec les sondages

Les sondages souffrent de biais de sélection. Certaines personnes ne répondront pas à un appel d'un institut de sondage. Certains groupes sont plus susceptibles de répondre que d'autres, ce qui introduit un biais dans la composition de l'échantillon. Par exemple, les retraités passent plus de temps chez eux près de leur téléphone et sont plus susceptibles de décrocher.

Pour s'assurer d'obtenir des échantillons représentatifs, ces instituts doivent utiliser des astuces mathématiques sophistiquées pour garantir la représentativité. Avec un succès mitigé.

:::

Pour revenir au Machine Learning, en sélectionnant aléatoirement un grand nombre d'observations de l'ensemble d'entraînement, vous pouvez vous assurer que l'ensemble de test est un échantillon représentatif des données d'entraînement.

Cela peut être visualisé avec un exemple simple. Imaginons que les données d'entraînement contiennent à la fois des appartements et des maisons individuelles. Ces types de propriétés ont généralement des surfaces différentes. Le graphique suivant montre la distribution des surfaces des appartements et des maisons pour l'ensemble des données :

![Distribution des surfaces pour les Appartements et les Maisons](/images/model-evaluation/whole_dataset.png)

En échantillonnant aléatoirement 200 observations pour créer un ensemble de test, les ensembles d'entraînement et de test ont des distributions à peu près similaires :

![Distributions des surfaces dans les ensembles d'entraînement et de test](/images/model-evaluation/two_sets.png)

C'est la loi des grands nombres en action ! Plus le jeu de données et l'ensemble de test sont grands, plus les deux distributions se ressembleront.

### Fuite d'Information

Sélectionner aléatoirement une portion des données d'entraînement semble être une bonne idée. Mais voyez-vous un problème avec cette méthode ?

Revenons à l'exemple des prix immobiliers. Les prix immobiliers évoluent dans le temps. Si je sélectionne aléatoirement 20% des transactions passées comme ensemble de test, l'ensemble d'entraînement verra des prix de toute la période. Par exemple, si les données d'entraînement contiennent des transactions de janvier 2023 à janvier 2025, l'ensemble de test sélectionné aléatoirement contiendra également des transactions de janvier 2023 à janvier 2025.

Cela donnera au modèle l'opportunité d'apprendre la tendance des prix sur toute la période, puis de prédire des **prix passés**. Un modèle entraîné jusqu'en janvier 2025 qui prédit le prix d'une propriété vendue en 2024 bénéficiera d'une **connaissance du futur**.

La performance du modèle dans l'évaluation des **transactions passées** ne sera pas une bonne estimation de la façon dont le modèle prédira les **prix immobiliers futurs**. Comment estimeriez-vous la performance d'un modèle dans la prédiction des prix futurs ?

Une façon de le faire est d'utiliser une séparation entraînement/test **basée sur le temps**. Si les données d'entraînement contiennent des données de janvier 2023 à janvier 2025, vous pourriez garder les deux derniers mois de données (décembre 2024 et janvier 2025) comme ensemble de test et utiliser le reste pour l'entraînement.

De cette façon, vous estimez la performance du modèle sur la prédiction de prix futurs. C'est exactement ainsi que le modèle sera utilisé une fois déployé. Après un entraînement de janvier 2023 à janvier 2025, le modèle sera utilisé pour prédire les prix de février 2025 et au-delà.

![Séparation aléatoire vs séparation basée sur le temps](/images/model-evaluation/time_based_split.png)

### Récapitulatif

L'évaluation des modèles est un aspect critique de la pratique du Machine Learning. Avant d'utiliser les prédictions d'un modèle dans le monde réel, il est nécessaire d'évaluer la précision du modèle ; la qualité des prédictions du modèle.

Pour ce faire, une partie des données d'entraînement doit être conservée comme **ensemble de test**. Cet ensemble de test doit être représentatif des données d'entraînement. Vous devez éviter la fuite d'information en vous assurant que le modèle n'aura pas accès à des **informations futures** lors de la prédiction sur l'ensemble de test.

## Distance à la Vérité

Mesurer la distance entre les prédictions et les vraies étiquettes dépend du type de problème. Pour les tâches de classification, les prédictions et la vérité terrain seront des étiquettes de classe comme « spam » ou « malin ». Dans les problèmes de régression, les prédictions et la vérité terrain seront des nombres.

Comme montré dans un [chapitre](distance.qmd){target=_blank} précédent, calculer des distances est un sujet fascinant, qui sera davantage exploré dans les deux prochains chapitres.

## Réflexions Finales

Pour évaluer la performance d'un modèle de prédiction, ce chapitre a passé en revue la méthode suivante :

- Mettre de côté une fraction du jeu de données d'entraînement comme ensemble de test
- Entraîner le modèle sur l'ensemble d'entraînement, ne pas utiliser l'ensemble de test
- Utiliser le modèle entraîné pour générer des prédictions sur l'ensemble de test
- Calculer la distance entre les prédictions et les étiquettes de l'ensemble de test

Comment mesurer exactement cette distance sera le sujet des deux prochains chapitres.
