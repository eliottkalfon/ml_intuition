---
title: "Probabilités et Régression avec les Arbres de Décision"
---

L'algorithme des Decision Trees introduit dans cette section a deux composantes :

- Identifier la meilleure division avec un **critère d'évaluation** (par exemple, le Coefficient d'Impureté de Gini)
- Diviser les données **récursivement** jusqu'à ce qu'aucune division supplémentaire ne soit possible

Le chapitre suivant explorera comment utiliser cet algorithme pour produire des probabilités au lieu d'étiquettes de classe, et l'étendre à la régression avec les Decision Trees.

## Des étiquettes aux probabilités

L'algorithme des Decision Trees présenté ci-dessus ne produit que des étiquettes de classe comme prédiction ("malin" ou "bénin"). Dans la version simple décrite ci-dessus, une observation se voit attribuer une prédiction en utilisant un **vote majoritaire** au sein de la feuille. Cela est similaire au [modèle KNN](neighbours.qmd). Comment l'algorithme des Decision Trees pourrait-il être utilisé pour produire des **probabilités** ?

Réfléchissons à ce problème en utilisant une version modifiée des données d'exemple :

![Données d'exemple non séparables](/images/trees/non_separable_data.png){width=60%}

Dans cet exemple, les feuilles numéro 3 et 4 contiennent toutes deux des observations des deux classes. Maintenant, imaginez qu'une nouvelle observation a $x_1=2.1$ et $x_2=0.5$, quelle prédiction produirions-nous ?

En fonction des valeurs des caractéristiques de cette observation, elle atterrira dans la feuille numéro 3. Dans cette feuille, la grande majorité des observations d'entraînement sont de classe $\times$ (10 sur 11) avec une seule observation $\circ$.

En utilisant une approche similaire à celle utilisée dans KNN, au lieu d'appliquer un vote majoritaire au sein d'une feuille, nous pourrions utiliser la **fréquence** de la classe comme probabilité prédite.

Dans cet exemple, la nouvelle observation aura une probabilité de $\times$ de :

$$
\frac{10}{11} \approx 0.909
$$

:::{.exercise #exr-proba}
Calculez la probabilité de $\times$ pour une observation avec les caractéristiques : $x_1 = 3$ et $x_2 = 2$. Montrez qu'elle est approximativement de 0.17
:::

C'est tout, rien de plus complexe.

## De la classification à la régression

Jusqu'à présent, nous nous sommes concentrés sur les problèmes de classification, dans lesquels l'objectif est d'attribuer une étiquette (comme la malignité d'une tumeur) aux nouvelles observations. Comment le même modèle peut-il être appliqué à un problème de régression, c'est-à-dire à la prédiction d'une variable continue, comme le prix d'une propriété.

L'idée centrale resterait la même : diviser les données récursivement en sous-groupes pour les rendre aussi "purs" que possible. Ce concept de "pureté" ou d'"homogénéité" est facile à définir dans un problème de classification. Un groupe qui est "pur" est un groupe qui contient une grande majorité d'une classe.

Dans les problèmes de régression, comment pourrait-on définir ce concept d'homogénéité ? Pensez-y en termes de prix immobiliers. Un groupe homogène serait un groupe qui contient des propriétés avec des **prix similaires**. Dans un tel groupe, chaque élément aurait peu de différence avec le prix moyen.

Maintenant, comment générer des prédictions de **valeurs continues** à partir de ces sous-groupes ? Dans le cas de la classification, nous pourrions simplement utiliser le vote majoritaire ou la moyenne pour les prédictions de probabilité. En accord avec ce qui a été montré avec KNN, le même principe de moyenne peut être utilisé pour les problèmes de régression.

Le prix prédit de chaque nouvelle propriété serait la **moyenne de tous les prix des propriétés dans le même sous-groupe.** Cela est similaire à la logique de prédiction de probabilité décrite précédemment.

Une division est bonne si la moyenne dans chaque feuille a une faible erreur, si la moyenne est une prédiction précise des observations dans cette feuille.

Illustrons cela avec un exemple simple.

#### Division des Données

Imaginons que nous construisions un modèle prédisant les prix immobiliers en fonction de la Taille de la Propriété ($m^2$) et de la Distance au Centre ($km$) :

![Propriétés par Taille et Distance colorées par Prix de la Propriété](/images/trees/property_prices.png)

Nous pourrions prendre un sous-ensemble de ces données (prix maintenant en **k €**) :

| Taille (m²) | Distance (km) | Prix (k €) |
|:-----------:|:-------------:|:----------:|
| 60          | 2.0           | 320        |
| 80          | 1.5           | 400        |
| 120         | 5.0           | 350        |
| 70          | 3.0           | 310        |
| 150         | 1.0           | 600        |
| 90          | 4.0           | 330        |

Supposons que la première division soit sur `taille < 100`. Cela divise les données en deux groupes :

- **Tailles du Groupe A** (`taille < 100`) : 60, 80, 70, 90
- **Tailles du Groupe B** (`taille ≥ 100`) : 120, 150

Cette division est-elle bonne ? Pour le savoir, nous calculerions la moyenne des Groupes A et B, ce seront nos prédictions pour les deux groupes.

La prédiction pour le Groupe A est le prix moyen dans le Groupe A :

$\text{Moyenne Groupe A} = \frac{320 + 400 + 310 + 330}{4} = 340$

La prédiction pour le Groupe B est le prix moyen dans le Groupe B :

$\text{Moyenne Groupe B} = \frac{350 + 600}{2} = 475$

Nous devons maintenant mesurer à quel point ces moyennes sont de bonnes prédictions. Comment feriez-vous ?

Si vous avez pensé à l'Erreur Quadratique Moyenne, bravo ! Vous pouvez vous référer au [chapitre sur l'Évaluation des Modèles](evaluation-regression.qmd) si ce concept n'est pas encore assez clair.

Pour chaque groupe, nous pouvons calculer l'**Erreur Quadratique Moyenne (MSE)** des prix :

- Groupe A :
$$
  \begin{aligned}
  \text{MSE}_a &= \frac{(320-340)^2 + (400-340)^2 + (310-340)^2 + (330-340)^2}{4} \\
  &= \frac{400 + 3600 + 900 + 100}{4}  \\
  &= \frac{5000}{4} = 1250
  \end{aligned}
$$

- Groupe B :
$$
  \begin{aligned}
  \text{MSE}_b &= \frac{(350-475)^2 + (600-475)^2}{2} \\
  &= \frac{(-125)^2 + (125)^2}{2} \\
  &= \frac{15625 + 15625}{2} \\
  &= \frac{31250}{2} = 15625
  \end{aligned}
$$

C'est un bon début, mais cela nous laisse avec deux nombres MSE. Comment pourrait-on les résumer en un seul ?

Une idée est de calculer la **MSE moyenne**. C'est une moyenne pondérée des MSE des deux groupes (pondérée par le nombre d'observations dans chaque groupe) :

$$ \begin{aligned}
\text{MSE Pondérée} &= \frac{n_a}{n_a + n_b} \cdot \text{MSE}_a + \frac{n_b}{n_a + n_b} \cdot \text{MSE}_b \\
&=\frac{4}{6} \cdot 1250 + \frac{2}{6} \cdot 15625 = 833.33 + 5208.33 = 6041.67
\end{aligned}
$$

### Essayer Différentes Divisions

À chaque étape de division, l'algorithme essaierait différentes caractéristiques et valeurs de division, et choisirait celle qui minimise la MSE moyenne.

Ce processus peut être visualisé en montrant la MSE moyenne pour chaque valeur de division de la caractéristique **taille** :

![Essayer différentes valeurs de division de la caractéristique taille](/images/trees/property_mse_vs_size.png)

En suivant le même **processus récursif** que celui montré dans le cas de la classification, nous pouvons construire un Decision Tree qui partitionne l'espace global en sous-groupes.

Ce processus d'apprentissage par étapes est montré ci-dessous :

![L'algorithme d'apprentissage du Decision Tree sélectionne la meilleure division à chaque étape](/images/trees/property_regression_splits_stepwise.png)

Comme décrit dans cette section, les Decision Trees peuvent être facilement appliqués aux tâches de régression. Le seul changement significatif est l'évaluation de la qualité de division avec l'**Erreur Quadratique Moyenne** au lieu du Coefficient d'Impureté de Gini.

## Conclusion

Ce chapitre conclut notre exploration des Decision Trees. Résumant une fois de plus l'algorithme d'apprentissage des Decision Trees :

- Évaluer les divisions en utilisant un critère d'homogénéité : Gini ou MSE
- Diviser les groupes récursivement
- Produire soit des étiquettes de classe soit des probabilités en utilisant la moyenne

Pour tester vos connaissances, vous pouvez essayer l'exercice pratique ci-dessous.

En regardant en arrière, nous savons maintenant comment entraîner et évaluer deux modèles de Machine Learning différents. C'est déjà beaucoup. La prochaine section explorera la dernière pièce manquante du puzzle : le Prétraitement des Données, pour rendre les données prêtes pour la modélisation.

## Exercice Pratique

:::{.exercise #exr-tree-fraud-proba}
Supposons que vous construisez un Decision Tree pour détecter les transactions frauduleuses. Vous utilisez deux caractéristiques, toutes deux mesurées sur une échelle de 0-100 :

- **Montant de la Transaction (\$)** (0-100)
- **Âge du Client (années)** (0-100)

Vous disposez des 10 transactions suivantes dans vos données d'entraînement :

| Montant Transaction | Âge Client | Frauduleuse ? |
|:-------------------:|:----------:|:-------------:|
| 95                  | 22         | Oui           |
| 90                  | 25         | Oui           |
| 92                  | 23         | Oui           |
| 97                  | 21         | Oui           |
| 93                  | 24         | Oui           |
| 94                  | 23         | Non           |
| 20                  | 80         | Non           |
| 25                  | 78         | Non           |
| 18                  | 82         | Non           |
| 23                  | 77         | Non           |

Une nouvelle transaction a lieu avec un montant de **93** et un âge client de **23**.

1. Construisez un Decision Tree avec une **seule division** (pour simplifier) en utilisant les données d'entraînement et en trouvant les divisions qui minimisent le Coefficient d'Impureté de Gini ?
2. En utilisant l'arbre généré, quelle est la probabilité prédite de fraude de la nouvelle observation ?

@sol-tree-fraud-proba

:::

## Solutions
:::{.solution #sol-proba}
@exr-proba

$\frac{2}{12} = \frac{1}{6} \approx = 0.17$
:::

:::{.solution #sol-tree-fraud-proba}
@exr-tree-fraud-proba

1. Nous essayons de diviser les données en utilisant différentes caractéristiques et valeurs de caractéristiques. Pour chaque division, nous calculons le Coefficient d'Impureté de Gini pondéré. Les coefficients résultants peuvent être vus dans les deux tableaux ci-dessous. Pour chaque tableau, les symboles $\leq$ et $>$ représentent les deux sous-groupes résultants :

- $\leq$ : observations avec une valeur de caractéristique inférieure ou égale à la valeur de division
- $>$ : observations avec une valeur de caractéristique supérieure à la valeur de division

Divisions pour le **Montant de la Transaction**

| Division à | ≤ Division Fraude | ≤ Division Non-Fraude | > Division Fraude | > Division Non-Fraude | Gini Pondéré |
|:----------:|:-----------------:|:---------------------:|:-----------------:|:---------------------:|:-------------|
| 19.00 | 0 | 1 | 5 | 4 | 0.444 |
| 21.50 | 0 | 2 | 5 | 3 | 0.375 |
| 24.00 | 0 | 3 | 5 | 2 | 0.286 |
| 57.50 | 0 | 4 | 5 | 1 | **0.167** |
| 91.00 | 1 | 4 | 4 | 1 | 0.320 |
| 92.50 | 2 | 4 | 3 | 1 | 0.417 |
| 93.50 | 3 | 4 | 2 | 1 | 0.476 |
| 94.50 | 3 | 5 | 2 | 0 | 0.375 |
| 96.00 | 4 | 5 | 1 | 0 | 0.444 |


Divisions pour l'**Âge du Client**

| Division à | ≤ Division Fraude | ≤ Division Non-Fraude | > Division Fraude | > Division Non-Fraude | Gini Pondéré |
|:----------:|:-----------------:|:---------------------:|:-----------------:|:---------------------:|:-------------|
| 21.50 | 1 | 0 | 4 | 5 | 0.444 |
| 22.50 | 2 | 0 | 3 | 5 | 0.375 |
| 23.00 | 3 | 1 | 2 | 4 | 0.417 |
| 23.50 | 3 | 1 | 2 | 4 | 0.417 |
| 24.50 | 4 | 1 | 1 | 4 | 0.320 |
| 51.00 | 5 | 1 | 0 | 4 | **0.167** |
| 77.50 | 5 | 2 | 0 | 3 | 0.286 |
| 79.00 | 5 | 3 | 0 | 2 | 0.375 |
| 81.00 | 5 | 4 | 0 | 1 | 0.444 |

Le Montant de Transaction de 57.5 et l'Âge du Client de 51 atteignent tous deux un Coefficient de Gini de 0.167. Les deux fonctionneraient. Cette solution choisira Âge du Client = 51 comme valeur de division.

L'arbre résultant est :

![Arbre résultant divisant sur Âge du Client ≤ 51](/images/trees/fraud_tree_example.png)

1. Génération de la prédiction pour l'observation : Âge du Client : 23 et Montant de Transaction : 93, l'observation tombera dans la feuille gauche, car $23 \leq 51$.

La probabilité prédite pour cette observation est alors la probabilité moyenne de Fraude dans la feuille :

$$
P(\text{Fraude}) = \frac{5}{5+1} \approx 83\%
$$

:::
