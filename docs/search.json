[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Intuition",
    "section": "",
    "text": "Preface\nMachine Learning and AI have taken our world by storm. And yet, so few people truly understand these technologies.\nI have spent a large part of my career solving practical problems with Machine Learning. This book is my attempt to explain the intuition behind those solutions for family, friends and curious readers.\nIf you want to understand how an algorithm can learn from examples and make useful predictions, welcome! You are in the right place. Expect clear explanations, simple visualisations, and practical insights rather than heavy mathematics.\nThe first part is a self-contained introduction to prediction with Machine Learning. It explains the core ideas you need to make sense of models that learn from data.\nThe following chapters explore important aspects of applied Machine Learning practice — for example: modelling approaches, model evaluation, and data preprocessing.\nThe final section brings these pieces together with an accessible end-to-end Machine Learning project.\nThe last chapter reflects on how modern generative models connect with the ideas presented here, and what that means for the future.\n\nNotes\n\nThis is a practical primer: the book focuses on intuition and real-world thinking rather than formal proofs. If you would like a more code-heavy or mathematical approach, please get in touch.\nSynthetic data: all examples use synthetic data for clarity. Nothing in this book is medical, legal or financial advice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  First Prediction with Machine Learning",
    "section": "",
    "text": "1.1 A visual approach\nWith Machine Learning, we can generate predictions based on historical data. These predictions can solve problems in the real world.\nImagine this scenario: you are an early-career doctor and you receive a biopsy report. The lab lists two measurements from the cell nuclei of a suspicious mass — an average perimeter of 100 µm and an average area of 1200 µm². From just those two numbers: is the mass more likely benign (non-cancerous) or malignant (cancerous)?\nIf that vocabulary sounds unfamiliar, don’t worry — the question is straightforward: can we predict the diagnosis of this tumour from two measurements?\nAt first it might feel impossible. But suppose we have a database of many tumours, each labelled benign or malignant. A natural first step is to plot those examples using the two measurements from the report: perimeter and area. Each point on the chart would carry a label.\nWe could then plot the new observation on the same chart. We label it with a question mark as its diagnosis is still unknown.\nWould you now feel comfortable making an educated guess?\nLook carefully: most of the unknown point’s nearest neighbours are malignant. From the data’s perspective, the most reasonable guess is that the tumour is malignant. That is not a certainty — it’s a probabilistic judgement based on the available examples.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Prediction with Machine Learning</span>"
    ]
  },
  {
    "objectID": "intro.html#a-visual-approach",
    "href": "intro.html#a-visual-approach",
    "title": "1  First Prediction with Machine Learning",
    "section": "",
    "text": "Plotting historical observations\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nbenign_center = [80, 700]\nmalignant_center = [110, 1200]\n\nn_samples = 70\n\nX_benign, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=1)\nX_malignant, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=2)\n\nbenign_std = [10, 120]\nmalignant_std = [12, 300]\n\nX_benign = X_benign * benign_std + benign_center\nX_malignant = X_malignant * malignant_std + malignant_center\n\nplt.figure(figsize=(8,6))\nplt.scatter(X_benign[:,0], X_benign[:,1], marker='o', color='blue', label='Benign')\nplt.scatter(X_malignant[:,0], X_malignant[:,1], marker='x', color='red', label='Malignant')\nplt.xlabel('Perimeter Mean (µm)', fontsize=16)\nplt.ylabel('Area Mean (µm²)', fontsize=16)\nplt.title('Tumours: Perimeter Mean vs Area Mean', fontsize=18)\nplt.legend(fontsize=12)\nplt.grid(True)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()\n\n\n\n\n\nLooking at the unknown observation\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\n\nbenign_center = [80, 700]\nmalignant_center = [110, 1200]\n\nn_samples = 70\n\nX_benign, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=1)\nX_malignant, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=2)\n\nbenign_std = [10, 120]\nmalignant_std = [12, 300]\n\nX_benign = X_benign * benign_std + benign_center\nX_malignant = X_malignant * malignant_std + malignant_center\n\nplt.figure(figsize=(8,6))\nplt.scatter(X_benign[:,0], X_benign[:,1], marker='o', color='blue', label='Benign')\nplt.scatter(X_malignant[:,0], X_malignant[:,1], marker='x', color='red', label='Malignant')\n\nplt.scatter(100, 1200, marker=r'$\\mathbf{?}$', color='green', s=400, label='New Tumour')\n\nplt.xlabel('Perimeter Mean (µm)', fontsize=16)\nplt.ylabel('Area Mean (µm²)', fontsize=16)\nplt.title('Tumours: Perimeter Mean vs Area Mean (New Tumour)', fontsize=18)\nplt.legend(fontsize=12)\nplt.grid(True)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Prediction with Machine Learning</span>"
    ]
  },
  {
    "objectID": "intro.html#from-intuition-to-algorithms",
    "href": "intro.html#from-intuition-to-algorithms",
    "title": "1  First Prediction with Machine Learning",
    "section": "1.2 From intuition to algorithms",
    "text": "1.2 From intuition to algorithms\nHow could computers do the same? Computers do not (yet) have eyes, or an understanding of distance and closeness.\nA prediction algorithm would need to find an observation’s closest neighbours. To do so, it would need to measure the distance between this observation and others, and pick the ones with the shortest distance.\nThese distance calculations will be studied later. As a brain teaser, how would you calculate the distance between point A and B on this figure?\nHint: the Pythagorean theorem should be useful\n\n\n\nCalculating the distance between A and B\n\n\n\n\nFigure code\n\nplt.figure(figsize=(6,6))\nplt.scatter([1,5], [1,4], color=['blue','red'], s=100)\n\nplt.plot([1,5],[1,4],'g-')\nplt.text(1,1.2, 'A (1,1)', fontsize=16, ha='center')\nplt.text(5,4.2, 'B (5,4)', fontsize=16, ha='center')\nplt.xlabel('x', fontsize=16)\nplt.ylabel('y', fontsize=16)\nplt.title('Distance Between Two Points', fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlim(0, 6)\nplt.ylim(0, 6)\nplt.grid(True)\n\nplt.show()\n\n\n\nAnswer\n\n\n\n\nThinking of distance as the hypotenuse of a right-angle triangle\n\n\nTo find the distance between point A \\((1,1)\\) and point B \\((5,4)\\) using the Pythagorean theorem, we can consider these points as two vertices of a right-angled triangle.\nThe horizontal distance (\\(\\Delta x\\)) between the points is: \\[\n\\Delta x = x_2 - x_1 = 5 - 1 = 4\n\\]\nThe vertical distance (\\(\\Delta y\\)) between the points is: \\[\n\\Delta y = y_2 - y_1 = 4 - 1 = 3\n\\]\nAccording to the Pythagorean theorem, the square of the hypotenuse (the distance \\(d\\) between points A and B) is equal to the sum of the squares of the other two sides (\\(\\Delta x\\) and \\(\\Delta y\\)): \\[\nd^2 = (\\Delta x)^2 + (\\Delta y)^2\n\\]\nSubstituting the values: \\[\\begin{aligned}\nd^2 &= (4)^2 + (3)^2 \\\\\nd^2 &= 16 + 9 \\\\\nd^2 &= 25\n\\end{aligned}\n\\]\nTo find \\(d\\), we take the square root of both sides: \\[\\begin{aligned}\nd &= \\sqrt{25} = 5 \\\\\n\\end{aligned}\n\\]\nThe distance between point A \\((1,1)\\) and point B \\((5,4)\\) is \\(5\\).\n\nWith the list of closest neighbours, how would you make a prediction? Before coming up with an answer, think of how you would classify the following examples:\n\n\n\nDifferent neighbour scenarios\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_neighbours(num_malignant, ax, total_neighbours=5):\n    num_malignant = min(num_malignant, total_neighbours)\n    num_benign = total_neighbours - num_malignant\n    angles_malignant = np.linspace(0, np.pi, num_malignant + 1, endpoint=False)[:-1] if num_malignant &gt; 0 else []\n    angles_benign = np.linspace(np.pi, 2*np.pi, num_benign + 1, endpoint=False)[:-1] if num_benign &gt; 0 else []\n    radius = 0.5\n    \n    # Plot malignant neighbours\n    for i in range(num_malignant):\n        ax.plot(radius * np.cos(angles_malignant[i]), radius * np.sin(angles_malignant[i]), 'rx', markersize=10)\n    \n    # Plot benign neighbours\n    for i in range(num_benign):\n        ax.plot(radius * np.cos(angles_benign[i]), radius * np.sin(angles_benign[i]), 'bo', markersize=10)\n    \n    # Central unknown point\n    ax.plot(0, 0, 'g', markersize=18, marker=r'$\\mathbf{?}$') \n    \n    ax.set_title(f'{num_malignant} Malignant, {num_benign} Benign', fontsize=18)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlim([-1, 1])\n    ax.set_ylim([-1, 1])\n    ax.set_aspect('equal', adjustable='box')\n\n# Create a 2x3 subplot layout\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfig.suptitle('Classification based on Nearest Neighbours', fontsize=20, y=0.98)\n\n# Flatten the axes array for easier iteration\naxes = axes.flatten()\n\nfor i in range(6):\n    plot_neighbours(i, axes[i])\n\n# Add a legend to the first subplot\naxes[0].plot([], [], 'rx', markersize=10, label='Malignant')\naxes[0].plot([], [], 'bo', markersize=10, label='Benign')\naxes[0].plot([], [], ' ', markersize=18, marker=r'$\\mathbf{?}$', label='Unknown')\naxes[0].legend(loc='upper right', fontsize=12)\n\n\nplt.subplots_adjust(top=0.9)\nplt.show()\n\nAs you may have figured out, a simple majority vote should do. To generate predictions, count the number of neighbours belonging to each diagnosis, and assign the diagnosis with the highest number of neighbours.\nThat is it! We have our first model that can generate predictions about the world based on historical observations. This method could generate a diagnosis for any observation based on the value of the two measurements of its cell nuclei: average area and average perimeter.\nThis is only our first step into the fascinating world of Machine Learning. ML models can map any input, such as the above measurements, to any output, like tumour diagnosis.\nReady? Let’s get started.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First Prediction with Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/defining-prediction.html",
    "href": "chapters/defining-prediction.html",
    "title": "2  Defining Prediction",
    "section": "",
    "text": "2.1 The Prediction Task\nPre-diction, or, “to say before”. To be able to describe the future, to make the unknown known.\nPredictions are all around us. When you unlock your phone and look at the weather forecast (another word for “prediction”). When you open your email inbox, a prediction algorithm has already classified your incoming messages as “spam” or “non-spam”. But you do not need your phone to be exposed to predictions.\nIn many regards, prediction is indistinguishable from perception. When we perceive the world around us, we are constantly predicting what we will be perceiving next. For example, as you read this sentence, you are predicting its next ___.\nThinking about predictions, there are two main prediction tasks:\nWe do not necessarily need Machine Learning to make these predictions. Historically, we have relied on both (1) hand-crafted rules and (2) human intuition.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Defining Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/defining-prediction.html#the-prediction-task",
    "href": "chapters/defining-prediction.html#the-prediction-task",
    "title": "2  Defining Prediction",
    "section": "",
    "text": "Regression: prediction of a continuous value, a real number\n\nThe temperature tomorrow\nThe price of a property\n\nClassification: prediction of class labels\n\nEmails: “spam” or “non-spam”\nImages: “dog” or “cat”\nType of tumour: “malignant” or “benign”\nThe following word in this ___\n\n\n\nExercise 2.1 Come up with more examples of problems for:\n\nClassification\nRegression\n\n\n\n\n2.1.1 Hand-crafted rules\nThese hand-crafted rules are simplified models of reality. As an example, to price a property, I could simply multiply the average price per square metre in the neighbourhood by the number of square metres of the property. Rules like this one can work surprisingly well.\nThe number of special characters in an email address can be a relatively reliable spam filter. For instance, an address like !_!urgent$!_deal@secure.offer.xyz immediately raises red flags due to its chaotic combination of punctuation and a suspicious, non-standard top-level domain. These red flags could be coded into the spam filter program to classify incoming messages as “spam”.\n\n\n2.1.2 Human intuition\nThe Merriam-Webster dictionary defines intuition as:\n\n“intuition, noun:\n\nThe power or faculty of attaining direct knowledge or cognition without evident rational thought and inference\n\nimmediate apprehension or cognition”\n(Merriam-Webster Dictionary 2024)\n\n\nIntuition is access to knowledge, making the unknown known, without apparent effort. It is generally built over time from the following (Parrish 2016):\n\nA slow and unchanging environment\nLots of practice and a large sample size\nFrequent and accurate feedback\n\nMaking this concrete, after 20 years of experience, a good oncologist can spot a malignant tumour on an x-ray without even having to think about it. They have seen so many examples that they have built an intuition over time. A seasoned real estate agent (in a slowly changing market) can “feel” the price of a property. An experienced recruiter can spot highly talented individuals after only a short conversation.\nLooking at the example of spam prevention, if I receive an email from the address !_!urgent$!_deal@secure.offer.xyz telling me I have won the lottery and just need to click a link to claim my prize, I will be sceptical. I do not need to use explicit rules for this, human experience is enough. (This may not apply to my grandparents)\n\n\n2.1.3 Drawbacks\nThe world is complex, messy, and in a state of constant change.\n\nComplexity makes building a rule-based system nearly impossible\nConstant change means that market trends fluctuate, attackers learn to circumvent simple spam-detection algorithms, and new types of diseases emerge. Organisations relying on rules or human intuition must constantly update their approach\nBuilding intuition is a costly and time-consuming process tied to specific individuals\nHuman performance is inconsistent, we all have bad days\n\nMachine Learning systems are not perfect either, but address many of these drawbacks.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Defining Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/defining-prediction.html#what-is-machine-learning",
    "href": "chapters/defining-prediction.html#what-is-machine-learning",
    "title": "2  Defining Prediction",
    "section": "2.2 What is Machine Learning?",
    "text": "2.2 What is Machine Learning?\nNow, how is Machine Learning (ML) different?\nMachine Learning can be defined from its terms:\n\nMachine: a computerised system, not human\nLearning: a system that adapts to data, not a rule-based method\n\nAs its name indicates, ML algorithms learn to predict either continuous values or class labels from historical data. As an example, a Machine Learning algorithm or model would learn the relationship between the features of a property and its price, or the dimensions of a tumour and its type (“malignant” or “benign”). Exactly how this learning happens is the purpose of this book, but let’s not get ahead of ourselves.\n\n2.2.1 Machine Learning Models\nThroughout this book, Machine Learning “model” and “algorithm” will be used interchangeably. A model can be defined by what it does. In the context of predictive Machine Learning, it takes an input and outputs a prediction.\n\\[\n\\text{Input} \\longrightarrow \\text{Model} \\longrightarrow \\text{Prediction}\n\\]\nAdapting this framework to the tumour diagnosis example, the model takes tumour measurements as features and outputs a diagnosis:\n\\[\n\\text{Tumour Measurements} \\longrightarrow \\text{Model} \\longrightarrow \\text{Diagnosis}\n\\]\nFor property pricing, the input is the characteristics of the property and the output a price prediction.\n\\[\n\\text{Property Characteristics} \\longrightarrow \\text{Model} \\longrightarrow \\text{Price Prediction}\n\\]\nIn English, models and algorithms have slightly different meanings. The Cambridge Dictionary defines a model as:\n\n“model, noun: a simple representation of a system or process, especially one that can be used in calculations or predictions of what might happen”\n(Cambridge Dictionary 2024b)\n\nOn the other hand, an algorithm is defined as:\n\n“algorithm, noun: a set of […] instructions or rules that, especially if given to a computer, will help to calculate an answer to a problem”\n(Cambridge Dictionary 2024a)\n\nCombining these two definitions, a Machine Learning model is a system adapting to data to generate predictions. It learns the relationship between an input and an output.\n\n\n2.2.2 What makes a good model?\nThe job of a Machine Learning model is to make the most accurate predictions, to be as close to reality as possible. \\[\n\\text{Input} \\longrightarrow \\text{Model} \\longrightarrow \\text{Prediction}\n\\]\nPredictions must be as close as possible to the ground truth, which is the true label or output. For example, the ground truth for a tumor is its actual diagnosis. The ground truth for a property is its final sale price. The ground truth for an email is whether it is actually spam or not.\n\n\nDefining Truth\n\nI love definitions, but defining truth is often disappointing. In this book, anything that is true is confirmed by the reality around us; i.e., empirical observation. This choice is made out of convenience. If you want to have some fun, I would recommend opening a few dictionaries and reading the definitions of the word “truth”.\nIn the Cambridge Dictionary, “truth” is defined as:\n\n“the quality of being true”\n(Cambridge Dictionary 2024d)\n\nHoping to find an answer there, I looked up the definition of “true”:\n\n“right and not wrong; correct”\n(Cambridge Dictionary 2024c)\n\nAs you can see, with limited success. This links back to the circularity of words and dictionaries. We only define words with more words.\n\nMeasuring this degree of closeness to ground truth and building accurate models are topics that will be discussed in the next sections.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Defining Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/defining-prediction.html#final-thoughts",
    "href": "chapters/defining-prediction.html#final-thoughts",
    "title": "2  Defining Prediction",
    "section": "2.3 Final Thoughts",
    "text": "2.3 Final Thoughts\nMachine Learning models are everywhere. At their essence, these are just adaptive prediction systems. They learn relationships between input and output from historical data.\n\\[\n\\text{Input} \\longrightarrow \\text{Model} \\longrightarrow \\text{Prediction}\n\\]\nHow these models adapt and learn is the topic of this book.\nYet, these prediction models are only one part of the field of Machine Learning. The following chapter will explore two other types of Machine Learning: Unsupervised Learning and Reinforcement Learning.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Defining Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/defining-prediction.html#solutions",
    "href": "chapters/defining-prediction.html#solutions",
    "title": "2  Defining Prediction",
    "section": "2.4 Solutions",
    "text": "2.4 Solutions\n\nSolution 2.1. Exercise 2.1\nSome ideas:\n\nClassification: fraud detection, object detection, credit approval\nRegression: energy demand prediction, sales prediction, stock price prediction\n\n\n\n\n\n\nCambridge Dictionary. 2024a. “Algorithm.” Cambridge Dictionary. 2024. https://dictionary.cambridge.org/dictionary/english/algorithm.\n\n\n———. 2024b. “Model.” Cambridge Dictionary. 2024. https://dictionary.cambridge.org/dictionary/english/model.\n\n\n———. 2024c. “True.” Cambridge Dictionary. 2024. https://dictionary.cambridge.org/dictionary/english/true.\n\n\n———. 2024d. “Truth.” Cambridge Dictionary. 2024. https://dictionary.cambridge.org/dictionary/english/truth.\n\n\nMerriam-Webster Dictionary. 2024. “Intuition.” Merriam-Webster. 2024. https://www.merriam-webster.com/dictionary/intuition.\n\n\nParrish, Shane. 2016. “Daniel Kahneman and Herbert Simon on Intuition.” Farnam Street. 2016. https://fs.blog/daniel-kahneman-on-intuition/.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Defining Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/types-ml.html",
    "href": "chapters/types-ml.html",
    "title": "3  Other Types of Machine Learning",
    "section": "",
    "text": "3.0.1 Reinforcement Learning\nGenerating predictions by learning from input/output pairs only refers to a part of the discipline of Machine Learning, also known as supervised learning.\nIt is called “supervised” because the algorithm learns from labelled examples. As an example, a model learns to predict the diagnosis of a suspicious mass from its measurements.\nSupervised learning will be the main focus of this book. This section will briefly explore other types of Machine Learning.\nSometimes, these labels do not exist. Let’s imagine an archaeologist who stumbles upon a pile of bones. They want to understand if all the bones belong to the same species. One possible approach would be to measure all of these bones. In a simplified case, there are two measurements, length and width.\nPlotting all these bone measurements on a chart, two clusters emerge:\nThe exact species are still unknown, but the bones seem to belong to two different clusters. You have gained this information through the use of unsupervised learning, learning patterns or underlying structures of the data.\nAnother type of Machine Learning is reinforcement learning (RL), in which the Machine Learning algorithm does not generate predictions, but selects actions to maximise a certain reward within an environment. As an example, the objective of a RL algorithm could be to win at chess. At each move:",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Other Types of Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/types-ml.html#final-thoughts",
    "href": "chapters/types-ml.html#final-thoughts",
    "title": "3  Other Types of Machine Learning",
    "section": "3.1 Final Thoughts",
    "text": "3.1 Final Thoughts\nThere are three main types of Machine Learning:\n\nSupervised learning: generate predictions based on input/output pairs (previous chapter)\nUnsupervised learning: discover patterns and underlying structure in data\nReinforcement learning: build models that select actions to maximise a reward while observing an environment\n\nMachine Learning is a very exciting field. But how is this related to Artificial Intelligence and Data Science? This will be covered by the next chapter.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Other Types of Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/types-ml.html#solutions",
    "href": "chapters/types-ml.html#solutions",
    "title": "3  Other Types of Machine Learning",
    "section": "3.2 Solutions",
    "text": "3.2 Solutions\n\nSolution 3.1. Exercise 3.1\nData to collect:\n\nPurchase history: frequency of purchase, quantity/amount purchased\nBrowsing behaviour: activity in the last month\nDemographic data: gender, age\n\nWhat to do with it:\n\nPlot the data and observe trends or clusters\nUse clustering algorithms to identify these clusters in higher dimensions (out of this book’s scope)\n\n\n\nSolution 3.2. Exercise 3.2\nAutonomous vacuum cleaners can be powered by reinforcement learning systems. Give examples of their:\n\nactions: move forward/back/left/right, change power, start/stop, dock to recharge\nenvironment: room layout, location of dirt, battery level\nreward/objective: maximise dust collected, reduce cleaning time, avoid collisions\n\n\n\nSolution 3.3. Exercise 3.3\nOther RL example: self-driving cars\n\nactions: accelerate, brake, steer, signal, change lanes\nenvironment: road, traffic, pedestrians, weather conditions\nreward/objective: reach destination safely\n\nMany other examples work here.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Other Types of Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/ai-ml-ds.html",
    "href": "chapters/ai-ml-ds.html",
    "title": "4  Machine Learning, Artificial Intelligence and Data Science",
    "section": "",
    "text": "4.1 Artificial Intelligence\nThe hype is real. Words like Artificial Intelligence (AI), Machine Learning (ML) and Data Science (DS) are frequently thrown around. This short chapter will explore the relations between these different concepts.\nArtificial Intelligence is the design of computational (i.e., non-human) systems that can perform tasks typically requiring human intelligence (see Russell and Norvig 2021, 1–4). These tasks include learning, reasoning, problem-solving and planning, among others. Anything that we commonly associate with our own cognitive capabilities. AI is concerned both with the understanding of intelligence and the building of intelligent systems.\nThe reason the new Generative AI models like ChatGPT or Claude feel so “AI” is that they replicate aspects of human intelligence. They can generate helpful answers to user queries, in a human-like way.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning, Artificial Intelligence and Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/ai-ml-ds.html#machine-learning",
    "href": "chapters/ai-ml-ds.html#machine-learning",
    "title": "4  Machine Learning, Artificial Intelligence and Data Science",
    "section": "4.2 Machine Learning",
    "text": "4.2 Machine Learning\nMachine Learning (ML) is a subset of Artificial Intelligence focused on designing algorithms that learn from data. These Machine Learning algorithms, also called “models”, can generalise the rules learnt on training data to new, unseen data points. As opposed to rule-based systems, ML algorithms are not explicitly programmed; they learn rules from the data. The next chapter will explore Machine Learning in more detail.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning, Artificial Intelligence and Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/ai-ml-ds.html#deep-learning",
    "href": "chapters/ai-ml-ds.html#deep-learning",
    "title": "4  Machine Learning, Artificial Intelligence and Data Science",
    "section": "4.3 Deep Learning",
    "text": "4.3 Deep Learning\nDeep Learning is a subset of Machine Learning concerned with the development of multi-layer Neural Networks to perform tasks like classification and regression. These Neural Networks are (roughly) inspired by the workings of the human brain. Since the 1990s they have pushed the state of the art in Machine Learning research and are the backbone of the current AI revolution. As these models are more complex, they will not be covered in this book.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning, Artificial Intelligence and Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/ai-ml-ds.html#data-science",
    "href": "chapters/ai-ml-ds.html#data-science",
    "title": "4  Machine Learning, Artificial Intelligence and Data Science",
    "section": "4.4 Data Science",
    "text": "4.4 Data Science\nMachine Learning is closely related to Data Science. For a few years (my first years on the job market), Data Scientist was the sexiest job of the 21st century (Davenport and Patil 2012).\nData Science is the extraction of generalisable knowledge from data. This is not knowledge at a given point in time, but generalisable knowledge. Knowledge that can be used on unseen data to predict the future.\nWhereas Data Analysis is the study of what happened, Data Science focusses on predicting future trends. Looking at the case of property pricing, Data Analysts would create reports and analyse what happened. On the other hand, Data Scientists would build models to predict the price of new properties.\nBeyond the hype, Data Science is a field concerned with extracting and extrapolating knowledge from data (Dhar 2013).\n\nExtract: to obtain something from something else, to get information from data.\nExtrapolate: to predict by projecting past experience or known data (Merriam-Webster Dictionary 2024).\n\nTo summarise, Data Science uses data to extract useful knowledge from data to inform the future.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning, Artificial Intelligence and Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/ai-ml-ds.html#final-thoughts",
    "href": "chapters/ai-ml-ds.html#final-thoughts",
    "title": "4  Machine Learning, Artificial Intelligence and Data Science",
    "section": "4.5 Final Thoughts",
    "text": "4.5 Final Thoughts\nThis chapter defined some important concepts:\n\nArtificial Intelligence (AI): design of intelligent systems, emulation of human cognition\nMachine Learning (ML): algorithms that learn from data\nDeep Learning: branch of ML focused on Neural Network modelling\nData Science (DS): extraction of generalisable knowledge from data\n\nNow that these definitions are out of the way, the next chapters will show how Machine Learning models work. The next chapter will explore the role of data in Machine Learning.\n\n\n\n\nDavenport, Thomas H., and D. J. Patil. 2012. “Data Scientist: The Sexiest Job of the 21st Century.” Harvard Business Review 90 (10): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\nDhar, Vasant. 2013. “Data Science and Prediction.” Communications of the ACM 56 (12): 64–73. https://doi.org/10.1145/2500499.\n\n\nMerriam-Webster Dictionary. 2024. “Extrapolate.” Merriam-Webster. 2024. https://www.merriam-webster.com/dictionary/extrapolate.\n\n\nRussell, Stuart J., and Peter Norvig. 2021. Artificial Intelligence: A Modern Approach. 4th ed. Hoboken: Pearson.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning, Artificial Intelligence and Data Science</span>"
    ]
  },
  {
    "objectID": "chapters/data-space.html",
    "href": "chapters/data-space.html",
    "title": "5  Data and Space",
    "section": "",
    "text": "5.1 The Anatomy of a Table\nPrediction models need information stored as input/output pairs. To predict the diagnosis of a tumour based on its characteristics, these models require many examples of tumour characteristics and tumour diagnosis pairs.\nThis collection is called a dataset, or a data sample. Data refers to any information being recorded and stored. From its Latin origin, it means “what is given”.\nFrom the point of view of the practitioner, data is rarely given, it is earned with sweat and tears. A sacrifice worth making, as a prediction model is only as good as its training data.\nThe simplest form of data is tabular data. The first known examples of data tables are 4000 years old (!) dating back to the Old Babylonian period. Then, these clay tablets were used for accounting and trade.\nTables are made of columns and rows.\nRows generally represent a real-world entity, with columns describing the attributes of this entity. Here, each row is a property, and each column describes an attribute of this property.\nAs shown in this table, the values stored in tables can have different data types:\nFor prediction purposes, the label to predict is generally one of the many columns describing various entities. In the case of the example above, it is the “Sale Price (€)” column.\nIn most organisations today, tabular data is stored in relational databases, also called Relational Database Management Systems. It is a mouthful, frequently abbreviated as RDBMS, still a mouthful.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data and Space</span>"
    ]
  },
  {
    "objectID": "chapters/data-space.html#the-anatomy-of-a-table",
    "href": "chapters/data-space.html#the-anatomy-of-a-table",
    "title": "5  Data and Space",
    "section": "",
    "text": "Basic elements of a table\n\n\n\n\n\nnumeric: integer or continuous values\ncategorical: text or category\nboolean: true or false\ndatetime: representing a point in time",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data and Space</span>"
    ]
  },
  {
    "objectID": "chapters/data-space.html#data-is-anything-stored",
    "href": "chapters/data-space.html#data-is-anything-stored",
    "title": "5  Data and Space",
    "section": "5.2 Data is Anything Stored",
    "text": "5.2 Data is Anything Stored\nData is any piece of information that is recorded. The text that I am typing now is data. The signal sent from my laptop keyboard strokes, to the CPU, and over a network to Google Docs… All of this is data.\nTo put some structure to this, the following are the main data modalities:\n\nImages: generally represented as grids of pixel (Picture Elements) values\nAudio: sound wave, amplitude of a signal over time\nVideo: a combination of frames and one or more audio tracks\nText: sequence of characters, each encoded as bits\nBinary files: any file stored with bits, sequence of 0s and 1s\n\nThe above data types are ultimately stored as binary data on a computer. Some, like text files, are human-readable, while others—such as images, audio, and video—are often stored in binary file formats that are not directly readable by humans.\nThis book will focus on building an intuition for Machine Learning using tabular data only.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data and Space</span>"
    ]
  },
  {
    "objectID": "chapters/data-space.html#from-data-to-space",
    "href": "chapters/data-space.html#from-data-to-space",
    "title": "5  Data and Space",
    "section": "5.3 From Data to Space",
    "text": "5.3 From Data to Space\nGetting back to tabular data, these first sections will focus on numerical columns only. Categorical and datetime features will be explored in the later sections. Spoiler alert: they will all be converted to numbers.\nLooking at the table below, each tumour can be represented by a list of its characteristics, a list of numbers.\n\n\n\nDiagnosis\nPerimeter Mean (µm)\nArea Mean (µm²)\nTexture Mean\n\n\n\n\nBenign\n80\n700\n17.5\n\n\nMalignant\n110\n1200\n23.0\n\n\n\nIn Mathematics, a list of numbers can represent a point in space. In the example above, each tumour is associated with three numbers. We can use these numbers to represent each tumour as a point in a three-dimensional space.\n\n\nNote: But what is space?\n\nThis is a fascinating question. The short answer is: a set with a degree of structure. A set can be thought of as a collection. But any set is not necessarily a space, as spaces require structure.\nAs an example, I can see objects on my desk: a glass of water and my notebook. These objects lie somewhere in a three dimensional space, with a certain position with regards to my computer. Using these positions, I could compute the distance between the different objects in space around me. But there, we are already getting carried away.\n\nBelow are two charts plotting suspicious mass observations in two and three dimensions:\n\n\n\nPlotting observations in two dimension\n\n\n\n\n\nPlotting observations in three dimensions\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import load_breast_cancer\n\n# Use breast cancer dataset for realistic texture means\ndata = load_breast_cancer()\nbenign_texture = data.data[data.target == 1][:, 1]  # texture_mean for benign\nmalignant_texture = data.data[data.target == 0][:, 1]  # texture_mean for malignant\n\nbenign_center = [80, 700]\nmalignant_center = [110, 1200]\n\nn_samples = 70\n\nX_benign, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=1)\nX_malignant, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=2)\n\nbenign_std = [10, 120]\nmalignant_std = [12, 300]\n\nX_benign = X_benign * benign_std + benign_center\nX_malignant = X_malignant * malignant_std + malignant_center\n\n# Sample realistic texture means\nnp.random.seed(42)\nbenign_texture_sample = np.random.choice(benign_texture, n_samples)\nmalignant_texture_sample = np.random.choice(malignant_texture, n_samples)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X_benign[:,0], X_benign[:,1], marker='o', color='blue', label='Benign')\nplt.scatter(X_malignant[:,0], X_malignant[:,1], marker='x', color='red', label='Malignant')\nplt.xlabel('Perimeter Mean (µm)', fontsize=16)\nplt.ylabel('Area Mean (µm²)', fontsize=16)\nplt.title('Tumours: Perimeter Mean vs Area Mean', fontsize=18)\nplt.legend(fontsize=12)\nplt.grid(True)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()\n\n# 3D plot\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_benign[:,0], X_benign[:,1], benign_texture_sample, marker='o', color='blue', label='Benign')\nax.scatter(X_malignant[:,0], X_malignant[:,1], malignant_texture_sample, marker='x', color='red', label='Malignant')\nax.set_xlabel('Perimeter Mean (µm)', fontsize=16)\nax.set_ylabel('Area Mean (µm²)', fontsize=16)\nax.set_zlabel('Texture Mean', fontsize=16)\nax.set_title('Tumours in 3D Feature Space', fontsize=18)\nax.legend(fontsize=12)\nax.tick_params(axis='x', labelsize=14)\nax.tick_params(axis='y', labelsize=14)\nax.tick_params(axis='z', labelsize=14)\nplt.show()\n\nThis text is printed on a flat surface. And yet, using perspective and transparency, one can create the illusion of a third dimension. If you think that this 3D chart is barely legible, I agree with you.\nWhat is true in a two- and three-dimensional space also applies to any number of dimensions. As the number of dimensions increases, plotting data on paper gets more difficult.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data and Space</span>"
    ]
  },
  {
    "objectID": "chapters/data-space.html#final-thoughts",
    "href": "chapters/data-space.html#final-thoughts",
    "title": "5  Data and Space",
    "section": "5.4 Final Thoughts",
    "text": "5.4 Final Thoughts\nBut why do we bother with space and with these lists of numbers? Representing entities and observations in this way allows us to use mathematical tricks to make predictions. The nature of these tools will be studied in the next sections, starting with distance, and answering the question: how similar are two observations?\n\n\n\n\nCDLI contributors. 2025. “Terms of Use.” https://cdli.earth/terms-of-use. https://cdli.earth/terms-of-use.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data and Space</span>"
    ]
  },
  {
    "objectID": "chapters/distance.html",
    "href": "chapters/distance.html",
    "title": "6  Distance and Similarity",
    "section": "",
    "text": "6.1 Starting with Subtraction\nNow that points exist in space, we could use the distance between them to make inferences. Inference refers to using information we have, to make guesses about the information we do not have.\nAs an example, we could infer that similar tumours, or tumours with similar characteristics, have the same diagnosis. But how could we measure the similarity between two observations?\nThis is where distance comes in.\nLooking at the plot below, how would you classify the tumour labelled by a question mark?\nHumans looking at this chart would intuitively compare the diagnosis of nearby observations with the unknown observation, and base their guess on neighbours.\nThis type of inference is something we do every day. As an example, I have seen many rabbits. If I see a small animal with long ears pointing up, I will classify this as a rabbit, as it is similar to my previous observations of rabbits.\nTo understand the concept of distance, we can start with one-dimensional space. This notion may seem strange, as we generally associate space with either two or three dimensions.\nLet’s consider a set of three flats in Berlin, labelled A, B and C, with the following surface areas:\nIn this example, the surface area represents the single dimension of this space. We can plot the different flats on this dimension:\nIn this space, what is the difference between A and C? How would you compute it?\nA simple subtraction would be a good start:\n\\[\n\\text{surface}_C - \\text{surface}_A = 55 - 22 = 33\n\\]\nNow, calculate the distance between B and C. You should get:\n\\[\n\\text{surface}_C - \\text{surface}_B = 55 - 35 = 20\n\\]\nIn line with our intuition, we see that the distance between B and C is shorter than the distance between A and C. The difference in their surface area is smaller.\nMoving forward, we will note the distance between A and C (or any other two points) as \\(d(A, C)\\).\nCan you see an issue with using subtraction as a distance calculation?\nOne of the main issues is that \\(d(A, C) \\neq d(C, A)\\). From the above, we know \\(d(A, C) = 33\\). Calculating \\(d(C, A)\\), we get:\n\\[\n\\text{surface}_A - \\text{surface}_C = 22 - 55 = -33\n\\]\nThis is unfortunate, as the distance between two points should not have a direction. It should be the same regardless of the starting point.\nHow could we solve this?",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance and Similarity</span>"
    ]
  },
  {
    "objectID": "chapters/distance.html#starting-with-subtraction",
    "href": "chapters/distance.html#starting-with-subtraction",
    "title": "6  Distance and Similarity",
    "section": "",
    "text": "Flat\nSurface Area (m²)\n\n\n\n\nA\n22\n\n\nB\n35\n\n\nC\n55\n\n\n\n\n\n\n\nDistance in one dimension\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nareas = np.array([22, 35, 55])\nlabels = np.array(['A', 'B', 'C'])\n\nsort_idx = np.argsort(areas)\nareas_sorted = areas[sort_idx]\nlabels_sorted = labels[sort_idx]\n\nplt.figure(figsize=(10, 2.5))\nplt.axhline(0, color='grey', linewidth=1, zorder=1)\nplt.scatter(areas_sorted, np.zeros_like(areas_sorted), s=200, color='royalblue', zorder=2)\n\nplt.yticks([])\nplt.xlabel('Surface Area (m²)', fontsize=16)\nplt.title('Flats on the Number Line', fontsize=18)\nplt.grid(True, axis='x', linestyle='--', alpha=0.5)\n\nxtick_vals = np.arange(20, 61, 10)\nplt.xticks(xtick_vals, xtick_vals, fontsize=14)\n\nfor x, label in zip(areas_sorted, labels_sorted):\n    plt.annotate(label, (x, 0), xytext=(0, 20), textcoords='offset points',\n                 ha='center', va='bottom', fontsize=16, fontweight='bold', color='royalblue')\n\nplt.xlim(15, 60)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance and Similarity</span>"
    ]
  },
  {
    "objectID": "chapters/distance.html#alternatives-to-subtraction",
    "href": "chapters/distance.html#alternatives-to-subtraction",
    "title": "6  Distance and Similarity",
    "section": "6.2 Alternatives to Subtraction",
    "text": "6.2 Alternatives to Subtraction\nThere are two mathematical tricks we could use to tackle this challenge:\n\nAbsolute Value\nSquared Difference\n\n\n6.2.1 Absolute Value\nYou can think of the absolute value of a number as removing any negative sign. More rigorously, the absolute value is the magnitude of a number, not its sign. The absolute value of number \\(x\\) is noted \\(|x|\\).\nFor example: \\(|2| = |-2| = 2\\)\nFor mathematically-inclined readers (others can close their eyes for two lines), the absolute value function is defined as:\n\\[\n|x| =\n\\begin{cases}\nx & \\text{if } x \\geq 0 \\\\\n-x & \\text{if } x &lt; 0\n\\end{cases}\n\\]\nPlotting the absolute value of all numbers between \\(-5\\) and \\(5\\), we get the following triangular shape:\n\n\n\nAbsolute value function\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-5, 5, 100)\ny = np.abs(x)\nplt.figure(figsize=(8, 5))\nplt.plot(x, y, color='royalblue', linewidth=3)\nplt.title('Absolute Value Function', fontsize=18)\nplt.xlabel('x', fontsize=16)\nplt.ylabel('|x|', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\nOne of the main advantages of the absolute value function as a distance metric is the following:\n\\[\n|x-y| = |y-x|\n\\]\nThe absolute value of the difference between two numbers is the same regardless of their order. Going back to our example:\n\\[\\begin{aligned}\n|\\text{surface}_C - \\text{surface}_A| &= |55 - 22| = |33| = 33 \\\\\n|\\text{surface}_A - \\text{surface}_C| &= |22 - 55| = |-33| = 33\n\\end{aligned}\n\\]\n\nExercise 6.1 Compute the absolute value distance between B and C, and between C and B. Show that both are equal to 20.\n\n\n\n6.2.2 Squared Difference\nAnother way to make sure the distance between two points is the same is to square the difference. A number squared, noted \\(x^2\\), is a number multiplied by itself: \\(x \\cdot x\\)\nThis operation has the same property as the absolute value:\n\\[\n(x-y)^2 = (y-x)^2\n\\]\nRevisiting the example above:\n\\[\\begin{aligned}\n(\\text{surface}_C - \\text{surface}_A)^2 &= (55 - 22)^2 = 33^2 = 1089 \\\\\n(\\text{surface}_A - \\text{surface}_C)^2 &= (22 - 55)^2 = (-33)^2 = 1089\n\\end{aligned}\n\\]\nPlotting the square of all numbers in the range \\(-5\\) to \\(5\\), we notice that it has a parabolic shape:\n\n\n\nSquare function\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-5, 5, 100)\ny = x**2\nplt.figure(figsize=(8, 5))\nplt.plot(x, y, color='royalblue', linewidth=3)\nplt.title('Square Function', fontsize=18)\nplt.xlabel('x', fontsize=16)\nplt.ylabel('$x^2$', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\nThis function increases faster as the input number grows.\nOne potential issue with squared difference is that this number can grow very fast, and is sometimes hard to interpret. Considering the example above, it seems strange that the distance between \\(22\\) and \\(55\\) would be \\(1089\\).\nTo make squared differences more interpretable, it is common to use the square root of the squared difference. The square root (noted \\(\\sqrt{\\phantom{x}}\\)) is the number which, when multiplied by itself, gives the original number. For example, \\(\\sqrt{9} = 3\\) because \\(3 \\cdot 3 = 9\\)\nGoing back to the example, the square root difference between A and C would be:\n\\[\n\\sqrt{(\\text{surface}_C - \\text{surface}_A)^2} = \\sqrt{33^2} = \\sqrt{1089} = 33\n\\]\n\nExercise 6.2 Compute the squared difference between B and C, and between C and B. Show that both are equal to 400.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance and Similarity</span>"
    ]
  },
  {
    "objectID": "chapters/distance.html#two-dimensions",
    "href": "chapters/distance.html#two-dimensions",
    "title": "6  Distance and Similarity",
    "section": "6.3 Two Dimensions",
    "text": "6.3 Two Dimensions\nThe methods above can accurately measure the distance between points in one dimension. But how to measure the distance between points in two dimensions?\nWhat would be the distance between the points A and B shown on the picture below?\n\n\n\nTwo points in space\n\n\nHint: The Pythagorean theorem may be useful.\n\n\n\nThinking of distance as the hypotenuse of a right-angle triangle\n\n\nTo find the distance between point A \\((1,1)\\) and point B \\((5,4)\\) using the Pythagorean theorem, we can consider these points as two vertices of a right-angled triangle.\nThe horizontal distance (\\(\\Delta x\\)) between the points is: \\[\n\\Delta x = x_2 - x_1 = 5 - 1 = 4\n\\] The vertical distance (\\(\\Delta y\\)) between the points is: \\[\n\\Delta y = y_2 - y_1 = 4 - 1 = 3\n\\] According to the Pythagorean theorem, the square of the hypotenuse (which is the distance \\(d\\) between points A and B) is equal to the sum of the squares of the other two sides (\\(\\Delta x\\) and \\(\\Delta y\\)): \\[\nd^2 = (\\Delta x)^2 + (\\Delta y)^2\n\\] Substituting the values: \\[\\begin{aligned}\nd^2 &= (4)^2 + (3)^2 \\\\\nd^2 &= 16 + 9 \\\\\nd^2 &= 25 \\\\\n\\end{aligned}\n\\] To find \\(d\\), we take the square root of both sides: \\[\\begin{aligned}\nd &= \\sqrt{25} \\\\\nd &= 5\n\\end{aligned}\n\\]\nSo, the distance between point A \\((1,1)\\) and point B \\((5,4)\\) is \\(5\\).\nYou may notice a striking similarity between the Pythagorean theorem and the square root of the squared distance defined in the previous section.\nThe distance function derived from the Pythagorean theorem is called the Euclidean distance. Let us compare the Euclidean distance in one and two dimensions:\n\nIn one dimension: \\[\nd(A, B) = \\sqrt{(x_2 - x_1)^2}\n\\]\nIn two dimensions: \\[\nd(A, B) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n\\]\n\n\nExercise 6.3 Calculate the Euclidean Distance between point A and B defined in this table:\n\n\n\nPoint\n\\(x_1\\)\n\\(x_2\\)\n\n\n\n\nA\n2\n4\n\n\nB\n5\n1\n\n\n\nHint: It may be helpful to plot these two points.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance and Similarity</span>"
    ]
  },
  {
    "objectID": "chapters/distance.html#to-infinity-and-beyond",
    "href": "chapters/distance.html#to-infinity-and-beyond",
    "title": "6  Distance and Similarity",
    "section": "6.4 To Infinity and Beyond",
    "text": "6.4 To Infinity and Beyond\nAs shown in the last chapter, data represents points in space in many dimensions. It is not uncommon to have datasets with hundreds of columns. How to measure distance in such a high-dimensional space?\nThe Euclidean Distance could be used for two, three or any \\(n\\) number of dimensions. It can be noted in the following way: \\[\nd(A, B) = \\sqrt{(b_1 - a_1)^2 + (b_2 - a_2)^2 + (b_3 - a_3)^2 + \\cdots + (b_n - a_n)^2}\n\\]\n\nExercise 6.4 Compute the Euclidean Distance between the points A and B in 5 dimensions (you can use a calculator):\n\n\n\n\n\n\n\n\n\n\n\nPoint\nDimension 1\nDimension 2\nDimension 3\nDimension 4\nDimension 5\n\n\n\n\nA\n2\n4\n1\n3\n7\n\n\nB\n5\n1\n6\n2\n9\n\n\n\n\n\n6.4.1 Scary Sigma\n\n6.4.1.1 Some Context\nA more concise notation of the Euclidean distance uses the \\(\\Sigma\\) (pronounced “sigma”) summation operator. This is a scary symbol, though its meaning is relatively simple.\n\n\n\nSigma explained\n\n\nAs an example, \\(\\sum_{i=1}^{n} i\\) represents the sum of all integers from \\(1\\) to \\(n\\):\n\\[\n\\sum_{i=1}^{n} i = 1 + 2 + 3 + \\cdots + n\n\\]\nIn this expression:\n\n\\(i=1\\) (at the bottom): Starting value of our counter\n\\(n\\) (at the top): Ending value of our counter\n\\(i\\) (after the sigma): The expression to sum for each value of the counter\n\nTo make this more concrete, the sum of all integers from 1 to 4 can be written: \\[\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4\n\\]\nThe following expression is the sum of all integers from 1 to 3, divided by 2:\n\\[\n\\sum_{i=1}^{3} \\frac{i}{2} = \\frac{1}{2} + \\frac{2}{2} + \\frac{3}{2}\n\\]\n\nExercise 6.5 Calculate the following summations:\n\n\\(\\sum_{i=1}^{4} (i+2)\\)\n\\(\\sum_{i=1}^{3} \\frac{i}{3}\\)\n\\(\\sum_{i=1}^{5} \\frac{1}{i}\\)\n\n\nThe \\(\\Sigma\\) operator is very useful when dealing with collections of numbers and dimensions; something that is very common in Machine Learning.\n\n\n6.4.1.2 Sigma and the Euclidean Distance\nUsing the \\(\\Sigma\\) notation, how to represent the Euclidean Distance in a more concise format?\nIn dot notation for \\(n\\) dimensions:\n\\[\nd(A, B) = \\sqrt{(b_1 - a_1)^2 + (b_2 - a_2)^2 + \\cdots + (b_n - a_n)^2}\n\\]\nIn sigma notation, with \\(n\\) the number of dimensions:\n\\[\nd(A, B) = \\sqrt{\\sum_{i=1}^{n} (b_i - a_i)^2}\n\\]",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance and Similarity</span>"
    ]
  },
  {
    "objectID": "chapters/distance.html#final-thoughts",
    "href": "chapters/distance.html#final-thoughts",
    "title": "6  Distance and Similarity",
    "section": "6.5 Final Thoughts",
    "text": "6.5 Final Thoughts\nThat is it! Using the above formula, you can compute the distance between any two points in a space of \\(n\\) dimensions. This will be very useful when building the first prediction model of this book, K-Nearest Neighbours.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance and Similarity</span>"
    ]
  },
  {
    "objectID": "chapters/distance.html#solutions",
    "href": "chapters/distance.html#solutions",
    "title": "6  Distance and Similarity",
    "section": "6.6 Solutions",
    "text": "6.6 Solutions\n\nSolution 6.1. Exercise 6.1\n\\[\\begin{aligned}\n|\\text{surface}_C - \\text{surface}_B| &= |55 - 35| = |20| = 20 \\\\\n|\\text{surface}_B - \\text{surface}_C| &= |35 - 55| = |-20| = 20\n\\end{aligned}\n\\]\n\n\nSolution 6.2. Exercise 6.2 \\[\\begin{aligned}\n(\\text{surface}_C - \\text{surface}_B)^2 &= (55 - 35)^2 = 20^2 = 400 \\\\\n(\\text{surface}_B - \\text{surface}_C)^2 &= (35 - 55)^2 = (-20)^2 = 400\n\\end{aligned}\n\\]\n\n\nSolution 6.3. Exercise 6.4 First, compute the squared difference for each of the five dimensions:\n\\[\\begin{aligned}\n(5-2)^2 &= 9 \\\\\n(1-4)^2 &= 9 \\\\\n(6-1)^2 &= 25 \\\\\n(2-3)^2 &= 1 \\\\\n(9-7)^2 &= 4 \\\\\n\\end{aligned}\n\\]\nSum:\n\\[\n9 + 9 + 25 + 1 + 4 = 48\n\\]\nTake the square root:\n\\[\nd(A, B) = \\sqrt{48} \\approx 6.93\n\\]\n\n\nSolution 6.4. Exercise 6.5\n\n\\(\\sum_{i=1}^{4} (i+2) = (1+2) + (2+2) + (3+2) + (4+2) = 3 + 4 + 5 + 6 = 18\\)\n\\(\\sum_{i=1}^{3} \\frac{i}{3} = \\frac{1}{3} + \\frac{2}{3} + \\frac{3}{3} = 2\\)\n\\(\\sum_{i=1}^{5} \\frac{1}{i} = 1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5} \\approx 1 + 0.5 + 0.333 + 0.25 + 0.2 = 2.283\\)",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance and Similarity</span>"
    ]
  },
  {
    "objectID": "chapters/neighbours.html",
    "href": "chapters/neighbours.html",
    "title": "7  Neighbours",
    "section": "",
    "text": "7.1 Intuition\nIt is all starting to come together. It is now time to build the first prediction model in this book. \\[\n\\text{Input} \\longrightarrow \\text{Model} \\longrightarrow \\text{Predictions}\n\\]\nA Machine Learning model learns the relationship between input/output pairs to generate predictions on new inputs. For the first time in this book, this chapter will explore how these models learn.\nUsing the example of tumour diagnosis:\nWe want to predict the diagnosis of the observation labelled with a question mark (\\(?\\)). How could this be done using distance?\nA good start would be to get the new observation’s closest neighbours. This can be done by calculating the distance between the new observation and the existing observations, and picking the ones with shortest distances.\nUsing your intuition, how would you classify this new observation?\nWhat did our intuition rely on to make this judgement? When I came up with a prediction, I looked at the observation’s neighbours and applied a majority vote.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neighbours</span>"
    ]
  },
  {
    "objectID": "chapters/neighbours.html#intuition",
    "href": "chapters/neighbours.html#intuition",
    "title": "7  Neighbours",
    "section": "",
    "text": "Tumour dataset: Perimeter Mean vs Area Mean",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neighbours</span>"
    ]
  },
  {
    "objectID": "chapters/neighbours.html#algorithm",
    "href": "chapters/neighbours.html#algorithm",
    "title": "7  Neighbours",
    "section": "7.2 Algorithm",
    "text": "7.2 Algorithm\nHow can we build this intuition into a program, an algorithm?\nThe goal is to craft a list of rules that could be executed by a computer. Such a list could look like this:\n\nFind the 5 closest neighbours of the observation\nCount the number of neighbours per class\nThe new observation is labelled with the class that has the highest number of neighbours\n\nIn Machine Learning, this model is referred to as K-Nearest Neighbours or KNN. \\(K\\) simply means any positive integer, referring to the number of neighbours considered (here 5).\nThis model acts as a map from feature values to a prediction. Using the algorithm described above, any tumour observation can be assigned to a diagnosis. This map can be visualised by applying this majority vote to every point and colouring it by its diagnosis:\n\n\n\nKNN prediction map\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_blobs\n\n\nbenign_center = [80, 700]\nmalignant_center = [110, 1200]\nn_samples = 70\nX_benign, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=1)\nX_malignant, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=2)\nbenign_std = [10, 120]\nmalignant_std = [12, 300]\nX_benign = X_benign * benign_std + benign_center\nX_malignant = X_malignant * malignant_std + malignant_center\n\n# Combine into features and target \nX = np.vstack([X_benign, X_malignant])\ny = np.hstack([np.zeros(len(X_benign)), np.ones(len(X_malignant))])\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Create custom colors with brighter hues\ncustom_cmap = ListedColormap(['#80C2FF', '#FF8080'])\n\n# Fit a KNN classifier with scaled data\nclf = KNeighborsClassifier(\n    n_neighbors=5,\n    weights='uniform',\n    algorithm='auto',\n    metric='euclidean'\n)\nclf.fit(X_scaled, y)\n\n# Create the decision boundary plot\nplt.figure(figsize=(8, 6))\n\n# Create a meshgrid for displaying the decision boundary\nh = 0.5  # step size in the mesh\nx_min, x_max = X[:, 0].min() - 5, X[:, 0].max() + 5\ny_min, y_max = X[:, 1].min() - 50, X[:, 1].max() + 50\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Scale the mesh points the same way the training data was scaled\nmesh_points = np.c_[xx.ravel(), yy.ravel()]\nmesh_points_scaled = scaler.transform(mesh_points)\n\n# Predict using the scaled mesh points\nZ = clf.predict(mesh_points_scaled)\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.contourf(xx, yy, Z, alpha=0.4, cmap=custom_cmap)\n\n# Plot the data points (using original unscaled coordinates for display)\nplt.scatter(X_benign[:,0], X_benign[:,1], marker='o', color='blue', label='Benign', \n            s=60, edgecolor='darkblue', alpha=0.9, linewidth=1)\nplt.scatter(X_malignant[:,0], X_malignant[:,1], marker='x', color='red', label='Malignant', \n            s=60, linewidth=2)\n\nplt.title('Nearest Neighbour Classification Boundary', fontsize=16)\nplt.xlabel('Perimeter Mean (µm)', fontsize=16)\nplt.ylabel('Area Mean (µm²)', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.legend(fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\"images/neighbours/classification_boundary.png\")\nplt.show()",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neighbours</span>"
    ]
  },
  {
    "objectID": "chapters/neighbours.html#adding-some-nuance",
    "href": "chapters/neighbours.html#adding-some-nuance",
    "title": "7  Neighbours",
    "section": "7.3 Adding some nuance",
    "text": "7.3 Adding some nuance\nThis is all very exciting. But let’s consider the following example from a patient’s perspective. Let’s imagine we have to predict the diagnosis of the following tumours:\n\n\n\nNeighbours with different class ratios\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_neighbours(num_malignant, ax, total_neighbours=5):\n    num_malignant = min(num_malignant, total_neighbours)\n    num_benign = total_neighbours - num_malignant\n    angles_malignant = np.linspace(0, np.pi, num_malignant + 1, endpoint=False)[:-1] if num_malignant &gt; 0 else []\n    angles_benign = np.linspace(np.pi, 2*np.pi, num_benign + 1, endpoint=False)[:-1] if num_benign &gt; 0 else []\n    radius = 0.5\n    \n    # Plot malignant neighbours\n    for i in range(num_malignant):\n        ax.plot(radius * np.cos(angles_malignant[i]), radius * np.sin(angles_malignant[i]), 'rx', markersize=10)\n    \n    # Plot benign neighbours\n    for i in range(num_benign):\n        ax.plot(radius * np.cos(angles_benign[i]), radius * np.sin(angles_benign[i]), 'bo', markersize=10)\n    \n    # Central unknown point\n    ax.plot(0, 0, 'g', markersize=18, marker=r'$\\mathbf{?}$') \n    \n    ax.set_title(f'{num_malignant} Malignant, {num_benign} Benign', fontsize=18)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlim([-1, 1])\n    ax.set_ylim([-1, 1])\n    ax.set_aspect('equal', adjustable='box')\n\n# Create a 2x3 subplot layout\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfig.suptitle('Classification based on Nearest Neighbours', fontsize=20, y=0.98)\n\n# Flatten the axes array for easier iteration\naxes = axes.flatten()\n\nfor i in range(6):\n    plot_neighbours(i, axes[i])\n\n# Add a legend to the first subplot\naxes[0].plot([], [], 'rx', markersize=10, label='Malignant')\naxes[0].plot([], [], 'bo', markersize=10, label='Benign')\naxes[0].plot([], [], ' ', markersize=18, marker=r'$\\mathbf{?}$', label='Unknown')\naxes[0].legend(loc='upper right', fontsize=12)\n\n\nplt.subplots_adjust(top=0.9)\nplt.show()\n\nThere, a simple majority vote would mean that the observation with 2 malignant neighbours could be classified as “benign”. As a patient, I would prefer to have a second opinion there.\nAs 2 out of the 5 neighbours of this observation are malignant, we are less certain that this tumour is benign. Could the model prediction reflect this uncertainty?\nOne way to do this would be, instead of taking the winner of a majority vote, we could estimate the probability of a tumour to be malignant.\n\n\nNote: Defining Probability\n\nThe probability assigns a degree of belief to an event, between 0 and 1. For example, the probability of rolling a 4 on a fair six-sided die is \\(1/6 \\approx 0.167\\). The probability of getting heads on a fair coin is \\(1/2 = 0.5\\).\n\nBefore computing the predicted probability of malignancy, we could start with intuition. If the 5 neighbours of the observation are malignant, the model should be 100% sure that the tumour is malignant. Conversely, if none of its 5 neighbours is malignant, the model would assign a probability of 0% to malignancy.\nSo far so good. But what about 3 and 2, or 1 and 4? By calculating the average, we could come up with a predicted probability.\nIf 3 neighbours are malignant and 2 are benign, we could compute the probability of malignancy with the expression:\n\\[\n\\text{Probability} = \\frac{3}{3+2} = \\frac{3}{5} = 0.6 = 60\\%\n\\]\n\nExercise 7.1 Calculate the probability of malignancy if four out of five neighbours are malignant. Show that it is \\(80\\%\\)\n\nBy using this approach, the model can output more nuanced predictions that reflect the uncertainty in the training data.\nThis approach can be used to revise the map visualised in the previous section. Instead of being coloured by the predicted label, it is coloured by the predicted probability of malignancy (\\(1\\) malignant, \\(0\\) benign).\n\n\n\nKNN probability map\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_blobs\n\nbenign_center = [80, 700]\nmalignant_center = [110, 1200]\nn_samples = 70\nX_benign, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=1)\nX_malignant, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=2)\nbenign_std = [10, 120]\nmalignant_std = [12, 300]\nX_benign = X_benign * benign_std + benign_center\nX_malignant = X_malignant * malignant_std + malignant_center\n\n# Combine into features and target\nX = np.vstack([X_benign, X_malignant])\ny = np.hstack([np.zeros(len(X_benign)), np.ones(len(X_malignant))])\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Fit a KNN classifier with scaled data\nclf = KNeighborsClassifier(\n    n_neighbors=5,\n    weights='uniform',\n    algorithm='auto',\n    metric='euclidean'\n)\nclf.fit(X_scaled, y)\n\n# Create the decision boundary plot\nplt.figure(figsize=(10, 6))\n\n# Create a meshgrid for displaying the decision boundary\nh = 0.5  # step size in the mesh\nx_min, x_max = X[:, 0].min() - 5, X[:, 0].max() + 5\ny_min, y_max = X[:, 1].min() - 50, X[:, 1].max() + 50\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Scale the mesh points the same way the training data was scaled\nmesh_points = np.c_[xx.ravel(), yy.ravel()]\nmesh_points_scaled = scaler.transform(mesh_points)\n\n# Predict probabilities using the scaled mesh points\nZ_proba = clf.predict_proba(mesh_points_scaled)[:, 1]  # Probability of class 1 (malignant)\nZ_proba = Z_proba.reshape(xx.shape)\n\n# Create a discrete colormap with exactly 6 levels\n# Define the boundaries for the 6 levels\nbounds = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.01]  # Adding 1.01 to ensure 1.0 is included in the last bin\n# Define the colors for each level - blue to red gradient\ncolors = ['#0050FF', '#5080FF', '#80AAFF', '#FFB080', '#FF8050', '#FF5000']\n# Create a custom discrete colormap with these colors and boundaries\ncmap = ListedColormap(colors)\nnorm = BoundaryNorm(bounds, cmap.N)\n\n# Plot the decision boundary with discrete probability levels\ncontour = plt.contourf(xx, yy, Z_proba, levels=bounds, cmap=cmap, norm=norm, alpha=0.7)\n\n# Plot the data points (using original unscaled coordinates for display)\nplt.scatter(X_benign[:,0], X_benign[:,1], marker='o', color='blue', label='Benign',\n           s=60, edgecolor='darkblue', alpha=0.9, linewidth=1)\nplt.scatter(X_malignant[:,0], X_malignant[:,1], marker='x', color='red', label='Malignant',\n           s=60, linewidth=2)\n\n# Add a colorbar with discrete ticks\ncbar = plt.colorbar(contour, ticks=[0.1, 0.3, 0.5, 0.7, 0.9])  # Position ticks in middle of each bin\ncbar.set_label('Probability of Malignant', fontsize=14)\n\n# Set custom labels on the colorbar\ncbar.ax.set_yticklabels(['0%', '20%', '40%', '60%', '80%'])\ncbar.ax.tick_params(labelsize=12)\n\n# Add a 100% label at the top\ncbar.ax.text(0.5, 1.02, '100%', transform=cbar.ax.transAxes, \n             ha='center', va='bottom', fontsize=12)\n\nplt.title('Nearest Neighbour Classification Boundary', fontsize=16)\nplt.xlabel('Perimeter Mean (µm)', fontsize=16)\nplt.ylabel('Area Mean (µm²)', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.legend(fontsize=12, loc='upper left')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neighbours</span>"
    ]
  },
  {
    "objectID": "chapters/neighbours.html#from-classification-to-regression",
    "href": "chapters/neighbours.html#from-classification-to-regression",
    "title": "7  Neighbours",
    "section": "7.4 From classification to regression",
    "text": "7.4 From classification to regression\nThe above section only showed the application of KNN to a classification problem, predicting the diagnosis of a suspicious mass.\nCould the same model be used for a regression problem (predicting a continuous quantity)?\nTo do so, let’s turn to the problem of property pricing. When selling a property, customers need to know how much their property is worth. This can be used as a basis to compute the listing price, price for which the property is first listed. It is also in the buyer’s interest to have a very good estimation of the value of a property before agreeing to the transaction.\nAs explained in the Defining Prediction chapter, this pricing can be done with:\n\nIntuition: From experience, real estate have an understanding of the property market in their area of specialisation. They could “know” how much a property would be worth\nRule-based Systems: Property analysts could build models that price properties based on their characteristics. A simple rule working surprisingly well is: average price per square meter * surface area of the property\n\nCan we use the Nearest Neighbour model to learn from historical data and generate new price predictions for unseen data? Another leading question. Yes.\nFor simplicity, let’s use the following features: number of rooms and distance from centre (\\(km\\)).\nThe training data looks like this:\n\n\n\nProperty pricing training data\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor, NearestNeighbors\n\nnp.random.seed(42)\n\nnum_samples = 50\nnumber_of_rooms = np.random.uniform(1, 5, num_samples)\ndistance_to_centre = np.random.uniform(1, 5, num_samples)\n\n# Price formula: base price + price per room - price per km from centre + noise\nprice = 300000 + 60000 * number_of_rooms - 40000 * distance_to_centre + np.random.normal(0, 25000, num_samples)\n\nX = np.vstack([number_of_rooms, distance_to_centre]).T\n\nnew_observation = np.array([[3, 2.5]])\n\nnbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(X)\ndistances, indices = nbrs.kneighbors(new_observation)\n\n# Print the neighbours' data for the table\nprint(\"The 5 closest neighbours are:\")\nfor i in range(len(indices.flatten())):\n    idx = indices.flatten()[i]\n    # Rounding for display purposes\n    print(\n        f\"Rooms: {round(X[idx, 0])}, \"\n        f\"Distance: {X[idx, 1]:.1f} km, \"\n        f\"Price: €{price[idx]:,.0f}\"\n    )\n\nplt.figure(figsize=(8, 6))\nsc = plt.scatter(number_of_rooms, distance_to_centre, c=price, cmap='plasma', s=80, edgecolor='k')\nplt.scatter(new_observation[0, 0], new_observation[0, 1], marker=r'$\\mathbf{?}$', color='lime', s=400, label='New Property')\n\nplt.scatter(X[indices, 0], X[indices, 1], facecolors='none', edgecolors='lime', s=200, linewidth=2, label='5 Closest Neighbours')\n\nplt.xlabel('Number of Rooms', fontsize=16)\nplt.ylabel('Distance to Centre (km)', fontsize=16)\nplt.title('Berlin Property Price Training Data', fontsize=18)\ncbar = plt.colorbar(sc)\ncbar.set_label('Price (€)', fontsize=14)\ncbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))\nplt.legend(fontsize=12)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.grid(True, linestyle='--', alpha=0.6)\n\nEach dot represents a property, coloured by its price. The question mark (\\(?\\)) is the new property we want to price.\nHow would you predict the new observation’s price? You could start by selecting its 5 closest neighbours. The five closest neighbours are highlighted on the chart and shown in the table below:\n\n\n\nNumber of Rooms\nDistance to Centre (km)\nPrice (k€)\n\n\n\n\n3\n2.7\n341\n\n\n3\n2.4\n401\n\n\n3\n2.9\n358\n\n\n4\n3.0\n377\n\n\n4\n2.3\n425\n\n\n\nNow, how could we generate a single prediction from this list?\nThe simplest approach would be to compute an average of all the neighbouring prices and use this as the prediction:\n\\[\\text{Predicted Price} = \\frac{451 + 467 + 457 + 468 + 436}{5} = \\frac{2279}{5} = 456\\]\nThat is it, we have our predictions! This process should remind you of probability predictions.\nUsing this method, we could also compute a map of property prices over the two features: number of rooms and distance from centre.\n\n\n\nKNN regression map\n\n\n\n\nFigure code\n\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X, price)\n\nxx, yy = np.meshgrid(np.linspace(1, 5, 100), np.linspace(1, 5, 100))\nZ = knn_reg.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z, cmap='plasma', alpha=0.8, levels=20)\n\nsc = plt.scatter(number_of_rooms, distance_to_centre, c=price, cmap='plasma', s=80, edgecolor='k')\nplt.scatter(new_observation[0, 0], new_observation[0, 1], marker=r'$\\mathbf{?}$', color='lime', s=400, label='New Property')\n\nplt.xlabel('Number of Rooms', fontsize=16)\nplt.ylabel('Distance to Centre (km)', fontsize=16)\nplt.title('KNN Regression Map (5 Neighbours)', fontsize=18)\ncbar = plt.colorbar(sc)\ncbar.set_label('Price (€)', fontsize=14)\ncbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))\n\nplt.legend(fontsize=12)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.grid(True, linestyle='--', alpha=0.6)\n\nThe map shows that there is a positive relationship between number of rooms and price, and the negative relationship between distance to centre and price. The most expensive properties are both central and have a high number of rooms. The (simple) model built allows us to map any given combination of number of rooms and distance to centre to a price.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neighbours</span>"
    ]
  },
  {
    "objectID": "chapters/neighbours.html#final-thoughts",
    "href": "chapters/neighbours.html#final-thoughts",
    "title": "7  Neighbours",
    "section": "7.5 Final Thoughts",
    "text": "7.5 Final Thoughts\nKNN is only the first of the Machine Learning models studied in this book. This model learns the relationship between input features and a target, using distance calculations and average over neighbours. Can you think of a way you could use KNN to predict something in your everyday life?\nWe have our first model, but how good is it? We will explore this in the next chapter.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neighbours</span>"
    ]
  },
  {
    "objectID": "chapters/neighbours.html#practice-exercise",
    "href": "chapters/neighbours.html#practice-exercise",
    "title": "7  Neighbours",
    "section": "7.6 Practice Exercise",
    "text": "7.6 Practice Exercise\n\nExercise 7.2 Suppose you are building a model to detect fraudulent transactions. You use two features, both measured on a 0–100 scale:\n\nTransaction Amount ($) (0–100)\nCustomer Age (years) (0–100)\n\nYou have the following 10 transactions in your training data:\n\n\n\nTransaction Amount\nCustomer Age\nFraudulent?\n\n\n\n\n95\n22\nYes\n\n\n90\n25\nYes\n\n\n92\n23\nYes\n\n\n97\n21\nYes\n\n\n93\n24\nYes\n\n\n94\n23\nNo\n\n\n20\n80\nNo\n\n\n25\n78\nNo\n\n\n18\n82\nNo\n\n\n23\n77\nNo\n\n\n\nA new transaction occurs with an amount of 93 and customer age 23.\nQuestion:\n1. Calculate the distance from each observation to the new transaction.\n2. Identify the 5 nearest neighbours and their labels.\n3. What is the predicted probability that the new transaction is fraudulent?",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neighbours</span>"
    ]
  },
  {
    "objectID": "chapters/neighbours.html#solutions",
    "href": "chapters/neighbours.html#solutions",
    "title": "7  Neighbours",
    "section": "7.7 Solutions",
    "text": "7.7 Solutions\n\nSolution 7.1. Exercise 7.1\n\\[\n\\text{Probability} = \\frac{4}{4+1} = \\frac{4}{5} = 0.8 = 80\\%\n\\]\n\n\nSolution 7.2. Exercise 7.2\nThe distance between two observations is calculated as:\n\\[\n\\text{Distance} = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n\\]\nwhere \\((x_1, y_1)\\) are the features of the training observation, and \\((x_2, y_2)\\) are the features of the new transaction (93, 23).\nLet’s compute the distance for each observation:\n\n\n\n\n\n\n\n\n\n\nID\nTransaction Amount\nCustomer Age\nFraudulent?\nDistance to (93,23)\n\n\n\n\n1\n95\n22\nYes\n\\(\\sqrt{(95-93)^2 + (22-23)^2} \\approx 2.24\\)\n\n\n2\n90\n25\nYes\n\\(\\sqrt{(90-93)^2 + (25-23)^2} = \\approx 3.61\\)\n\n\n3\n92\n23\nYes\n\\(\\sqrt{(92-93)^2 + (23-23)^2} = 1.00\\)\n\n\n4\n97\n21\nYes\n\\(\\sqrt{(97-93)^2 + (21-23)^2} = \\approx 4.47\\)\n\n\n5\n93\n24\nYes\n\\(\\sqrt{(93-93)^2 + (24-23)^2} = 1.00\\)\n\n\n6\n94\n23\nNo\n\\(\\sqrt{(94-93)^2 + (23-23)^2} = 1.00\\)\n\n\n7\n20\n80\nNo\n\\(\\sqrt{(20-93)^2 + (80-23)^2} \\approx 92.6\\)\n\n\n8\n25\n78\nNo\n\\(\\sqrt{(25-93)^2 + (78-23)^2} \\approx 87.46\\)\n\n\n9\n18\n82\nNo\n\\(\\sqrt{(18-93)^2 + (82-23)^2} \\approx 95.43\\)\n\n\n10\n23\n77\nNo\n\\(\\sqrt{(23-93)^2 + (77-23)^2} \\approx 88.41\\)\n\n\n\nSorted by distance (nearest first):\n\n\n\nID\nDistance\nFraudulent?\n\n\n\n\n3\n1.00\nYes\n\n\n5\n1.00\nYes\n\n\n6\n1.00\nNo\n\n\n1\n2.24\nYes\n\n\n2\n3.61\nYes\n\n\n\nThe 5 nearest neighbours are:\n\n4 fraudulent: ID 3, 5, 1, 2\n1 non-fraudulent: ID 6\n\nPredicted probability of fraud:\n\\[\nP(\\text{Fraud}) = \\frac{4}{5} = 0.8 = 80\\%\n\\]",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neighbours</span>"
    ]
  },
  {
    "objectID": "chapters/model-evaluation.html",
    "href": "chapters/model-evaluation.html",
    "title": "8  Model Evaluation",
    "section": "",
    "text": "8.1 Unseen Data\nWe have now built our first model, but how good is it really? To answer this question, let us go back to the main function of a Machine Learning model. It is to generate predictions for new observations that are as close to the truth as possible.\n\\[\n\\text{Input} \\longrightarrow \\text{Model} \\longrightarrow \\text{Prediction}\n\\]\nA good model makes predictions with low distance to the truth.\nTaking this into account, how can we determine which model best performs its function?\nThere are two important elements to consider:\nUnseen data is, by definition, unseen. We cannot have access to it, as the moment we see it it becomes seen. Does that mean that we cannot evaluate a model on unseen data? Do we just wait for new observations to come?",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/model-evaluation.html#unseen-data",
    "href": "chapters/model-evaluation.html#unseen-data",
    "title": "8  Model Evaluation",
    "section": "",
    "text": "8.1.1 Train-Test Split\nOne way to estimate a model’s performance on new data is to set aside a portion of the training data for testing. This will remain unseen to the model, and will not be used for training.\nAt the end of the training process, the trained model (like K-Nearest Neighbours) can be used to generate predictions on the test data. For this portion of unseen data, we have both model predictions and the true label.\nA good model will have predictions as close as possible to the true labels.\n\n\n8.1.2 Representativeness\n\n8.1.2.1 Simulating Prediction Conditions\nWe use a test set to estimate the performance of the model on unseen data.\nFor this estimation to make sense, the test set must be an accurate representation of what this unseen data will look like.\nTo make this more concrete, to evaluate a model trained to predict US property prices, it makes no sense to use Berlin flats as a test set. This is a bit of an extreme example, but illustrates the main idea.\nIn the training process, the model learns the relationship between input and output using observations on the training data.\n\\[\n\\text{Input} \\longrightarrow \\text{Model} \\longrightarrow \\text{Prediction}\n\\]\nIf the training data is not representative of the data that the model will generate predictions for, it cannot properly learn the relationship between input and output.\nIf the test data is not representative of the unseen data the model will be used to generate predictions on, the model performance on this dataset will not be a good estimation of future performance.\n\n\n8.1.2.2 Ensuring Representativeness\nBut how do we make sure that the training and test sets are consistent? A single dataset can contain a lot of variations. In the property pricing example, there can be many types of properties with very different characteristics (e.g., surface area or number of rooms). Some of them are easier to price than others. How do we make sure that the train and test set look alike?\nHere, the law of large numbers comes to the rescue. The law of large numbers states that if we randomly sample a large enough number of observations from a population (here, training data), the randomly selected sample will be representative of the original population.\nThis is the statistical foundation of polling. Before elections, polling agencies randomly sample a large group of people to get a representative sample of the population.\n\n\nSome issues with polling\n\nPolling suffers from selection bias. Some people will not reply to a call from a polling agency. Some groups are more likely to reply than others, which introduces a bias in the composition of the sample. As an example, retired individuals spend more time at home close to their phones and are more likely to pick up the phone.\nTo make sure that they obtain representative samples, these agencies have to use some sophisticated mathematical tricks to ensure representativeness. With mixed success.\n\nGoing back to Machine Learning, by randomly selecting a large number of observations from the training set, you can ensure that the test set is a representative sample of the training data.\nThis can be visualised with a simple example. Let’s imagine that the training data contains both flats and single houses. These types of properties generally have different surface areas. The following chart shows the distribution of surface areas of both flats and houses for the entire data:\n\n\n\nSurface area distribution for both Flats and Houses\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Simulate a population with two property types\nn_total = 1000\ntypes = np.random.choice(['Flat', 'House'], size=n_total, p=[0.7, 0.3])\nareas = np.where(types == 'Flat', np.random.normal(60, 10, n_total), np.random.normal(120, 20, n_total))\n\nfig, ax = plt.subplots(figsize=(7, 5))\n\nax.hist(areas[types == 'Flat'], bins=15, alpha=0.7, label='Flat')\nax.hist(areas[types == 'House'], bins=15, alpha=0.7, label='House')\nax.set_title('Whole dataset', fontsize=18)\nax.set_xlabel('Surface area (m$^2$)', fontsize=16)\nax.set_ylabel('Count', fontsize=16)\nax.tick_params(axis='both', labelsize=14)\nax.legend(fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\nBy randomly sampling 200 of the observations to create a test set, both the training and test sets have roughly similar distributions:\n\n\n\nSurface area distributions in the training and test sets\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nn_total = 1000\ntypes = np.random.choice(['Flat', 'House'], size=n_total, p=[0.7, 0.3])\nareas = np.where(types == 'Flat', np.random.normal(60, 10, n_total), np.random.normal(120, 20, n_total))\n\ntest_idx = np.random.choice(n_total, size=200, replace=False)\ntrain_idx = np.setdiff1d(np.arange(n_total), test_idx)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nfor ax, idx, title in zip(axes, [train_idx, test_idx], ['Train set', 'Test set']):\n    ax.hist(areas[idx][types[idx] == 'Flat'], bins=15, alpha=0.7, label='Flat')\n    ax.hist(areas[idx][types[idx] == 'House'], bins=15, alpha=0.7, label='House')\n    ax.set_title(title, fontsize=18)\n    ax.set_xlabel('Surface area (m$^2$)', fontsize=16)\n    ax.set_ylabel('Count', fontsize=16)\n    ax.tick_params(axis='both', labelsize=14)\n    ax.legend(fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\nThis is the law of large numbers in action! The larger the dataset and test set, the more similar the two distributions will look like.\n\n\n\n8.1.3 Information leakage\nRandomly selecting a portion of the training data sounds good. But can you see an issue with this method?\nLet us go back to the example of property prices. Property prices evolve over time. If I randomly select 20% of the past transactions as test set, the training set will see prices from the entire period. For instance, if the training data contains transactions from January 2023 to January 2025, the randomly selected test set will also contain transactions from January 2023 to January 2025.\nThis will give the model the opportunity to learn the price trend of the whole period and then predict past prices. A model trained until January 2025 predicting the price of a property sold in 2024 will benefit from future knowledge.\nThe performance of the model in pricing past transactions will not be a good estimation of how the model will predict future property prices. How would you estimate the performance of a model in predicting future prices?\nOne way to do so is to use a time-based train/test split. If the training data contains data from Jan 2023 to Jan 2025, you could keep the last two months of the data (Dec 2024 and Jan 2025) as test set and use the rest for training.\nThis way, you estimate model performance on future price prediction. This is exactly how the model will be used once released. After training from Jan 2023 to Jan 2025, the model will be used to predict prices for Feb 2025 and beyond.\n\n\n\nRandom split vs time-based split\n\n\n\n\n8.1.4 Wrapping up\nModel evaluation is a critical aspect of Machine Learning practice. Before using model predictions in the real world, it is necessary to assess the model’s accuracy; the quality of the model’s predictions.\nTo do so, a part of the training data should be kept as test set. This test set should be representative of the training data. You should avoid information leakage by making sure that the model will not have access to future information when predicting on the test set.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/model-evaluation.html#distance-to-the-truth",
    "href": "chapters/model-evaluation.html#distance-to-the-truth",
    "title": "8  Model Evaluation",
    "section": "8.2 Distance to the truth",
    "text": "8.2 Distance to the truth\nMeasuring the distance between the predictions and true labels depends on the type of problem. For classification tasks, predictions and ground truth will be class labels like “spam” or “malignant”. In regression problems, predictions and ground truth will be numbers.\nAs shown in a previous chapter, calculating distances is a fascinating topic, that will be further explored in the next two chapters.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/model-evaluation.html#final-thoughts",
    "href": "chapters/model-evaluation.html#final-thoughts",
    "title": "8  Model Evaluation",
    "section": "8.3 Final Thoughts",
    "text": "8.3 Final Thoughts\nTo evaluate the performance of a prediction model, this chapter reviewed the following method:\n\nSet aside a fraction of the training dataset as test set\nTrain the model on the train set, do not use the test set\nUse the trained model to generate predictions on the test set\nCompute the distance between the predictions and the labels of the test set\n\nExactly how to measure this distance will be the topic of the next two chapters.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-classification.html",
    "href": "chapters/evaluation-classification.html",
    "title": "9  Evaluating Classification Models",
    "section": "",
    "text": "9.1 Evaluating Distances\nThe previous chapter described the process of splitting a dataset between training and test sets to estimate a model’s performance on unseen data. This evaluation process has the following four steps:\nThe previous chapter left this last point open: how to determine whether the predictions of a model are close to the truth? This chapter will tackle this problem for classification models. The following chapter will explore regression models.\nAs a reminder, classification tasks are concerned with predicting category labels such as “spam” or “malignant”.\nImagine that we trained two models, Model A and Model B. We now want to determine which of the two is best.\nTo do so, we remove a test set from the training data, and generate predictions using both models. For these observations, we also have the ground truth, the true label of each.\nWe get the following results by generating predictions for the test set:\nWhich of the models is the most accurate?\nYou could start by counting the number of errors of each model:\nIn other words, Model A was correct three times out of five whereas Model B was correct four times out of five. Model B seems more correct on average.\nMore than just “three out of five”, we can say that Model A was correct 60% of the time. This metric is called the accuracy of a Machine Learning model.\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of observations correctly predicted}}{\\text{Total observation count}}\n\\]\nWhen you hear the words “Model Accuracy” in the media, this is it.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-classification.html#evaluating-distances",
    "href": "chapters/evaluation-classification.html#evaluating-distances",
    "title": "9  Evaluating Classification Models",
    "section": "",
    "text": "Observation\nModel A\nModel B\nTruth\n\n\n\n\n1\n\\(\\times\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n2\n\\(\\times\\)\n\\(\\circ\\)\n\\(\\times\\)\n\n\n3\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n4\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n5\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n\n\n\n\nModel A made two errors: observation 1 and 5\nModel B made one error: observation 2\n\n\n\n\nSample Size\n\nA sample of five observations is too low to draw conclusions about the performance of these two models. As a general rule, the more test samples the better. This allows us to get a better estimate of the performance of the model. As a rule of thumb, practitioners allocate between 5% and 20% of the training data as a test set.\nThere is always a trade-off between the amount of data given to the model for training, and the number of observations in the test set. Data set aside for testing reduces the size of the training data available to the model which can result in lower model performance.\nThe later sections of this book will cover some methods designed to address this challenge.\n\n\n\n\nExercise 9.1 Calculate the accuracy of Model B.\n\n\n\n9.1.1 Beyond Accuracy: Recall and Precision\nBut is accuracy sufficient? To answer this question, we need to ask another question: Are all errors the same? Do they have the same consequences on the world?\nTo do so, let us move beyond simple \\(\\times\\) and \\(\\circ\\) and into the world of tumour diagnosis. There, a benign mass misdiagnosed as a malignant tumour would generate stress and inconvenience. A malignant tumour misdiagnosed as benign could have fatal consequences.\nLet us now consider the two following models, with \\(\\circ\\) representing a benign mass and \\(\\times\\) representing a malignant tumour:\n\n\n\nObservation\nModel A\nModel B\nTruth\n\n\n\n\n1\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n2\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n3\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\circ\\)\n\n\n4\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\circ\\)\n\n\n5\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n6\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n7\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n8\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n\n\nExercise 9.2 Show that model A has a higher accuracy than model B.\n\nEven though model A has higher accuracy than model B, it misclassified one malignant tumour as benign (Observation 5).\nOn the other hand, model B classified two benign masses as malignant (Observations 3 and 4) but caught all the malignant tumours.\nThis goes to show that Accuracy is only a part of the picture. How can we move from the description above to actual metrics?\n\n9.1.1.1 Useful Vocabulary\nBefore going into error metrics, it is important to introduce some vocabulary.\nIn binary classification, the model learns to assign observations into two categories, such as “benign” and “malignant” in the case of tumour diagnosis, or “spam” and “non-spam” for email filtering.\nMathematically, these two labels are represented as \\(1\\) and \\(0\\). Generally, the class that the model was built to detect is assigned \\(1\\) and the other \\(0\\). In tumour diagnosis, the malignant label is generally assigned the number \\(1\\) as these are the cases the model was designed for. Similarly, in email filtering, the label “spam” is assigned the number \\(1\\), as the model aims at identifying spam messages to filter them out of the inbox.\n\nExercise 9.3 If you were building a fraud detection model for an online payments company, which labels would you predict for? Which one would be assigned to \\(1\\) and \\(0\\)?\n\nIn the example of tumour diagnosis, a malignant tumour correctly classified as “malignant” is called a True Positive. On the other hand, a malignant tumour misclassified as a benign mass is a False Negative.\nHere, the words “positive” or “negative” are not associated with any value judgement. They are simply another way to say \\(1\\) or \\(0\\). Thinking about medical examples may make more sense here. When a medical test is positive, it means that the targeted substance is present. The same applies to spam detection. A positive spam detection means classifying an email as “spam”.\nGoing back to jargon, a True Positive is when the variable of interest is correctly detected (e.g., “spam” or “malignant”). A False Negative is when the variable of interest goes undetected. As an example, a malignant tumour is misdiagnosed as “benign”, or a spam email landing into the inbox.\nBased on these, what would be a False Positive and a True Negative? Think about it in terms of tumours and spam emails before reading on.\n\nFalse Positive: model wrongly predicts the presence of the variable of interest, e.g., a legitimate email predicted as “spam”\nTrue Negative: model correctly predicts the absence of the variable of interest, e.g., a benign mass is correctly classified as a benign mass\n\nThese can be summarised in the following table:\n\n\n\n\n\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive\nFalse Negative\n\n\nActual Negative\nFalse Positive\nTrue Negative\n\n\n\nThis is also called the Confusion Matrix.\nTo make this more concrete, this table could be adapted to the tumour diagnosis example:\n\n\n\n\n\n\n\n\n\nPredicted Malignant\nPredicted Benign\n\n\n\n\nActual Malignant\nMalignant tumour correctly classified as “malignant”\nMalignant tumour incorrectly classified as “benign”\n\n\nActual Benign\nBenign mass incorrectly classified as “malignant”\nBenign mass correctly classified as “benign”\n\n\n\nOr for spam filtering:\n\n\n\n\n\n\n\n\n\nPredicted Spam\nPredicted Not Spam\n\n\n\n\nActual Spam\nSpam correctly classified as “spam”\nSpam incorrectly classified as “not spam”\n\n\nActual Not Spam\nLegitimate email incorrectly classified as “spam”\nLegitimate email correctly classified as “not spam”\n\n\n\nTo test your understanding, try building a Confusion Matrix for a payment fraud detection model.\n\n\n\n\n\n\n\n\n\nPredicted Fraudulent\nPredicted Legitimate\n\n\n\n\nActual Fraudulent\nFraudulent transaction correctly classified as “fraudulent”\nFraudulent transaction incorrectly classified as “legitimate”\n\n\nActual Legitimate\nLegitimate transaction incorrectly classified as “fraudulent”\nLegitimate transaction correctly classified as “legitimate”\n\n\n\nNow that we clearly understand the language of True/False Negative/Positive, let’s get back to measuring the performance of a Machine Learning model.\n\n\n9.1.1.2 Recall\nIn the example of tumour diagnosis, we would like a model that would catch all malignant tumours. This is because False Negatives, i.e. misdiagnosing a malignant tumour as benign, can have fatal consequences. We want to compare the performances of Model A and B:\n\n\n\nObservation\nModel A\nModel B\nTruth\n\n\n\n\n1\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n2\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n3\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\circ\\)\n\n\n4\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\circ\\)\n\n\n5\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n6\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n7\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n8\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n\nRecall is the metric that answers the question: out of all the positive cases, how many did the model catch?\nRephrasing this for the tumour diagnosis example: out of all the malignant tumours, how many did the model catch?\nThis is calculated as follows:\n\\[\n\\text{Recall} = \\frac{\\text{Positive examples caught by Model}}{\\text{All positive examples}}\n\\]\nRephrasing this in term of True/False Positive, we get:\n\\[\n\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\nCalculating Recall for model B:\n\\[\n\\text{Recall}_{\\text{Model B}} = \\frac{4}{4 + 0} = 100\\%\n\\]\n\nExercise 9.4 Calculate the recall of Model A, and prove that it is \\(75%\\).\n\nIn the case of disease diagnosis, Recall is a critical metric as missing positive cases can have dire consequences on a patient’s life.\n\n\n9.1.1.3 Precision\nIn other scenarios, when the cost of a False Positive is high, we care for the Precision of the model. In other words, we want to avoid False Positives.\nPrecision answers the question: Out of all the observations predicted as positive, how many were True Positives, i.e., actually positive?\nBuilding a spam detection algorithm, the objective is to classify incoming emails as either “spam” or “non-spam”. Every email classified as “spam” would be filtered out of the inbox. In line with the previous section, the positive case would be “spam”, as it is the case that would require action; filtering email out of the inbox.\nIn spam filtering, False Positives can have serious negative consequences. Once, a recruiter’s email ended up in my spam folder. Without her reminder, I would have never worked at my current company because of a spam filtering error.\nFor this reason, the most important metric here is Precision: out of the messages classified as spam, how many were actually spam messages?\nThis can be computed as follows:\n\\[\n\\text{Precision} = \\frac{\\text{Number of emails correctly classified as spam}}{\\text{Total number of emails classified as spam}}\n\\]\nRephrasing this expression in Confusion Matrix jargon:\n\\[\n\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Positive}}\n\\]\nLooking at the example below:\n\n\n\nObservation\nModel A\nModel B\nTruth\n\n\n\n\n1\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n2\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n3\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\circ\\)\n\n\n4\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\circ\\)\n\n\n5\n\\(\\circ\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n6\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n7\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n8\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n\nthe precision of Model A is:\n\\[\n\\text{Precision}_\\text{Model A} = \\frac{3}{3 + 0} = 100\\%\n\\]\n\nExercise 9.5 Calculate the Precision of Model B.\n\n\n\n9.1.1.4 Revisiting Accuracy\nAccuracy, the first error metric explored in this chapter, can also be calculated with Confusion Matrix terms.\nAs a reminder, accuracy is calculated as follows: \\[\n\\text{Accuracy} = \\frac{\\text{Number of observations correctly predicted}}{\\text{Total observation count}}\n\\]\nUsing the language of True/False Positive/Negative, it can be computed with the following formula: \\[\n\\text{Accuracy} = \\frac{\\text{True Positive} + \\text{True Negative}}{\\text{Observation Count}} =\n{}\\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{TN} + \\text{FN}}\n\\]\nThis section described Accuracy, Recall, Precision and the Confusion Matrix. It is now to apply them to model selection; to choose the best performing model for a given task.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-classification.html#practical-model-selection",
    "href": "chapters/evaluation-classification.html#practical-model-selection",
    "title": "9  Evaluating Classification Models",
    "section": "9.2 Practical Model Selection",
    "text": "9.2 Practical Model Selection\nAfter having built two different Machine Learning tumour diagnosis models (A and B), you get the following Confusion Matrices:\nModel A\n\n\n\n\nPredicted Malignant\nPredicted Benign\n\n\n\n\nActual Malignant\n40\n10\n\n\nActual Benign\n10\n40\n\n\n\nModel B\n\n\n\n\nPredicted Malignant\nPredicted Benign\n\n\n\n\nActual Malignant\n45\n5\n\n\nActual Benign\n5\n45\n\n\n\nWhich model you pick?\nIf you picked B, that is correct. Why did you choose it?\n\nExercise 9.6 If you have not done so already, compute the Accuracy, Precision and Recall of both models.\n\nMaking this decision more complex, which of the following two models would you pick?\nModel A\n\n\n\n\nPredicted Malignant\nPredicted Benign\n\n\n\n\nActual Malignant\n48\n2\n\n\nActual Benign\n18\n32\n\n\n\nModel B\n\n\n\n\nPredicted Malignant\nPredicted Benign\n\n\n\n\nActual Malignant\n50\n0\n\n\nActual Benign\n20\n30\n\n\n\nIf you picked B, that is correct again. Why did you choose model B? In this case, model B has the same Accuracy and the highest Recall. In the case of tumour diagnosis, this is probably the most important metric to look at.\nWould you pick a different model for spam detection? Probably, as Precision becomes more important then. You do not want legitimate emails to end up in your spam folder.\n\nExercise 9.7 Calculate Recall and Precision for model A and B",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-classification.html#probabilities-and-model-evaluation",
    "href": "chapters/evaluation-classification.html#probabilities-and-model-evaluation",
    "title": "9  Evaluating Classification Models",
    "section": "9.3 Probabilities and Model Evaluation",
    "text": "9.3 Probabilities and Model Evaluation\nFor the sake of simplicity, this chapter has only considered binary predictions: either malignant or benign, either spam or non-spam.\nAs we have seen in the KNN chapter, classification models can also output predicted probabilities. Instead of simply predicting an observation as “malignant” or “benign”, the model can output a predicted probability of malignancy.\nTo convert these probabilities to a binary label, a threshold of 0.5 is generally used. Any predicted probability beyond this threshold (here 0.5) would be classified as “malignant”, otherwise, it would be classified as “benign”. For example, a predicted probability of 48% would be classified as “benign”, while a predicted probability of 51% as “malignant”.\nThe threshold can be any number between 0 and 1. The lower the threshold, the higher the number of Positives. In the example of tumour diagnosis, this would mean a higher number of observations predicted as “malignant”. On the other hand, a higher threshold would lead to fewer positives. In the example of spam detection, this would lead to fewer emails classified as “spam”.\nLet us show this with an example:\n\n\n\n\n\n\n\n\n\n\n\nObservation\nPredicted Probability\nThreshold 0.3\nThreshold 0.5\nThreshold 0.7\nGround Truth\n\n\n\n\n1\n0.6\n\\(\\times\\)\n\\(\\times\\)\n\\(\\circ\\)\n\\(\\times\\)\n\n\n2\n0.75\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n3\n0.2\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n4\n0.8\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\\(\\times\\)\n\n\n5\n0.4\n\\(\\times\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n6\n0.1\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\\(\\circ\\)\n\n\n\nWhich translates to the following Confusion Matrices:\nThreshold 0.3\n\n\n\n\nPredicted \\(\\times\\)\nPredicted \\(\\circ\\)\n\n\n\n\nActual \\(\\times\\)\n3\n0\n\n\nActual \\(\\circ\\)\n1\n2\n\n\n\nThreshold 0.5\n\n\n\n\nPredicted \\(\\times\\)\nPredicted \\(\\circ\\)\n\n\n\n\nActual \\(\\times\\)\n3\n0\n\n\nActual \\(\\circ\\)\n0\n3\n\n\n\nThreshold 0.7\n\n\n\n\nPredicted \\(\\times\\)\nPredicted \\(\\circ\\)\n\n\n\n\nActual \\(\\times\\)\n2\n1\n\n\nActual \\(\\circ\\)\n0\n3\n\n\n\nThe following line chart shows the evolution of the different metrics described in this chapter as the prediction threshold increases:\n\n\n\nVisualising recall and precision as the threshold increases\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nthresholds = [0.3, 0.5, 0.7]\nrecall = [1.0, 1.0, 0.67]\nprecision = [0.75, 1.0, 1.0]\naccuracy = [0.83, 1, 0.83]\n\nplt.figure(figsize=(8,5))\nplt.plot(thresholds, recall, marker='o', label='Recall')\nplt.plot(thresholds, precision, marker='o', label='Precision')\nplt.plot(thresholds, accuracy, marker='o', label='Accuracy')\nplt.xlabel('Threshold', fontsize=16)\nplt.ylabel('Score', fontsize=16)\nplt.title('Effect of Threshold on Metrics', fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.legend(fontsize=14)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nWe see that decreasing the prediction threshold to 0.3 has the following effects on the three metrics we analysed:\n\nIncrease in Recall: with more observations predicted as “malignant”, recall can only increase or stay constant\nDecrease in Precision: with more observations predicted as “malignant” despite a low predicted probability, the risk of misclassifying observations as “malignant” increases\n\n\nExercise 9.8 Describe the effect of increasing the threshold on Precision, Recall and Accuracy.\n\nFor tumour diagnosis, a lower threshold may make more sense. Going back to the Nearest Neighbour example, from a patient’s perspective, if the observation has two malignant neighbours and three benign ones, for a predicted probability of \\(2/5 = 40\\%\\), I would still want further checks.\nFor spam filtering, a higher threshold could be preferred, to reduce the risk of a legitimate email being filtered out. You could filter out results only when they have predicted probability of 0.8 or 0.9.\nThis example illustrates the Precision/Recall trade-off. Setting a higher threshold will increase Precision and reduce Recall. Setting a lower threshold will reduce Precision and increase Recall.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-classification.html#final-thoughts",
    "href": "chapters/evaluation-classification.html#final-thoughts",
    "title": "9  Evaluating Classification Models",
    "section": "9.4 Final Thoughts",
    "text": "9.4 Final Thoughts\nThis section has shown how to evaluate a binary classification model. There are three steps to this process:\n\nSet aside a share of the training data as test set\nUsing the trained model, generate predictions on this test set\nCalculate the distance to the truth of the predictions generated using performance metrics such as Accuracy, Recall, and Precision\n\nIt is important to remember that there is no optimal performance metric. The best metric for a problem depends on the consequences of model error on the real world.\nAfter this description of classification model evaluation, the next section will explore the evaluation of regression models.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-classification.html#solutions",
    "href": "chapters/evaluation-classification.html#solutions",
    "title": "9  Evaluating Classification Models",
    "section": "9.5 Solutions",
    "text": "9.5 Solutions\n\nSolution 9.1. Exercise 9.1\nAccuracy of Model B \\(= \\frac{4}{5} = 80\\%\\)\n\n\nSolution 9.2. Exercise 9.2\nModel A: Correct on observations 1, 2, 3, 4, 6, 7, 8 (7 out of 8). Incorrect on 5.\nModel B: Correct on 1, 2, 5, 6, 7, 8 (6 out of 8). Incorrect on 3 and 4.\nAccuracy of Model A \\(= \\frac{7}{8} = 87.5\\%\\)\nAccuracy of Model B \\(= \\frac{6}{8} = 75\\%\\)\n\n\nSolution 9.3. Exercise 9.3 Labels:\n\nfraudulent transaction \\(1\\)\nnon fraudulent or legitimate transaction \\(0\\)\n\n\n\nSolution 9.4. Exercise 9.4\nModel A: Out of 4 malignant tumours (observations 5, 6, 7, 8), Model A caught 3 (6, 7, 8), True Positives. Missed 5, a False Negative.\n\\(\\text{Recall}_{\\text{Model A}} = \\frac{3}{3 + 1} = 75\\%\\)\n\n\nSolution 9.5. Exercise 9.5\n\\[\n\\text{Precision}_\\text{Model B} = \\frac{4}{4 + 2} = \\frac{4}{6} \\approx 67\\%\n\\]\n\n\nSolution 9.6. Exercise 9.6\nFor Model A:\n\nTP = 40\nTN = 40\nFP = 10\nFN = 10\n\n\\(\\text{Accuracy} = \\frac{40 + 40}{100} = 80\\%\\)\n\\(\\text{Precision} = \\frac{40}{40 + 10} = \\frac{40}{50} = 80\\%\\)\n\\(\\text{Recall} = \\frac{40}{40 + 10} = \\frac{40}{50} = 80\\%\\)\nFor Model B:\n\nTP = 45\n\nTN = 45\n\nFP = 5\n\nFN = 5\n\n\\(\\text{Accuracy} = \\frac{45 + 45}{100} = 90\\%\\)\n\\(\\text{Precision} = \\frac{45}{45 + 5} = \\frac{45}{50} = 90\\%\\)\n\\(\\text{Recall} = \\frac{45}{45 + 5} = \\frac{45}{50} = 90\\%\\)\n\n\nSolution 9.7. Exercise 9.7\nFor Model A:\n\nTP = 48\nTN = 32\nFP = 18\nFN = 2\n\n\\(\\text{Precision} = \\frac{48}{48 + 18} = \\frac{48}{66} \\approx 73\\%\\)\n\\(\\text{Recall} = \\frac{48}{48 + 2} = \\frac{48}{50} = 96\\%\\)\nFor Model B:\n\nTP = 50\n\nTN = 30\n\nFP = 20\n\nFN = 0\n\n\\(\\text{Precision} = \\frac{50}{50 + 20} = \\frac{50}{70} \\approx 71\\%\\)\n\\(\\text{Recall} = \\frac{50}{50 + 0} = \\frac{50}{50} = 100\\%\\)\n\n\nSolution 9.8. Exercise 9.8\nIncreasing the threshold generally:\n\nIncreases Precision: fewer observations are classified as positive, so those that are classified as positive are more likely to be true positives\nDecreases Recall: more actual positives are missed, as the model becomes more conservative",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-regression.html",
    "href": "chapters/evaluation-regression.html",
    "title": "10  Evaluating Regression Models",
    "section": "",
    "text": "10.1 Evaluating Distances\nThe previous chapter described how to evaluate classification models. The main approach was simple:\nThe exact same approach can be applied to regression models. The only difference with the previous chapter is the computation of the distance between predictions and true labels.\nAs a reminder, a regression task is the prediction of any continuous quantity, such as the temperature tomorrow, the price of a property or a crop yield.\nThe general idea is still the same: \\[\n\\text{Input} \\longrightarrow \\text{Model} \\longrightarrow \\text{Prediction}\n\\]\nLet’s start with a simple example of a model predicting the price of a property:\nHow would you measure the error of this model? One way to do this is to measure the distance between each prediction and the ground truth using the distance functions explored in the Distance chapter.\nThe error of each prediction can be computed with the following formula:\n\\[\n\\text{Prediction Error} = \\text{Prediction} - \\text{Ground Truth}\n\\]\nThe error in the prediction for property 1 is:\n\\[\n\\text{Prediction Error}_1 = 140 - 120 = 20\n\\]\nThe model over-predicted the price of the property by 20.\nNow, how can we aggregate these errors to come up with an evaluation of the model? One idea would be to compute the average error (for \\(n\\) observations):\n\\[\n\\text{Average Error} = \\frac{\\text{Error}_1 + \\text{Error}_2 + \\cdots + \\text{Error}_n}{n}\n\\]\nComputing the average error of the example model, we get:\n\\[\n\\text{Average Error} = \\frac{20 + 10 + (-30)}{3} = \\frac{0}{3} = 0\n\\]\nUsing the \\(\\Sigma\\) operator described in the Distance chapter, this notation can be made more compact:\n\\[\n\\text{Average Error} = \\frac{1}{n} \\sum_{i=1}^{n} (\\text{Prediction}_i - \\text{Truth}_i)\n\\]\nIf you are not familiar with the \\(\\Sigma\\) operator, please refer to the Distance chapter in which this concept is clearly explained.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-regression.html#evaluating-distances",
    "href": "chapters/evaluation-regression.html#evaluating-distances",
    "title": "10  Evaluating Regression Models",
    "section": "",
    "text": "Property\nPredicted Price\nActual Price\n\n\n\n\n1\n140\n120\n\n\n2\n110\n100\n\n\n3\n100\n130\n\n\n\n\n\n\n\n\n\n\nExercise 10.1 Show that the prediction errors for property 2 and 3 are \\(10\\) and \\(-30\\) respectively.\n\n\nSolution 10.1. You should get the following table:\n\n\n\nProperty\nPredicted Price\nActual Price\nPrediction Error\n\n\n\n\n1\n140\n120\n20\n\n\n2\n110\n100\n10\n\n\n3\n100\n130\n-30",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-regression.html#beyond-subtraction",
    "href": "chapters/evaluation-regression.html#beyond-subtraction",
    "title": "10  Evaluating Regression Models",
    "section": "10.2 Beyond Subtraction",
    "text": "10.2 Beyond Subtraction\nDo you notice something strange? The average model error is \\(0\\), which would describe a perfect model. Yet, we do see that this model is not perfect, it makes errors, it does not perfectly predict the actual price of any property listed above.\nThe issue with averaging errors is that they cancel out. The numerator of the fraction becomes \\(0\\).\nRemembering the Distance chapter, could we use another distance function to avoid this problem?\nThere are two main methods we could use:\n\nAveraging the absolute values of the errors\nAveraging the squared errors\n\n\n10.2.1 Mean Absolute Error\nThe absolute value of a number \\(x\\) is noted \\(|x|\\). It is the magnitude of the number, regardless of its sign. As an example \\(|2| = |-2| = 2\\). When we average the absolute values of the error, we can compute the Mean Absolute Error (MAE):\n\\[\n\\text{Mean Absolute Error} = \\frac{| \\text{Error}_1 | + | \\text{Error}_2 | + \\cdots + | \\text{Error}_n |}{n}\n\\]\nUsing the \\(\\Sigma\\) notation to make this more compact:\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | \\text{Prediction}_i - \\text{Truth}_i |\n\\]\nComputing the Mean Absolute Error for the example data:\n\n\n\nProperty\nPredicted Price\nActual Price\nPrediction Error\n\n\n\n\n1\n140\n120\n20\n\n\n2\n110\n100\n10\n\n\n3\n100\n130\n-30\n\n\n\nWe get:\n\\[\n\\text{MAE} = \\frac{|20| + |10| + |-30|}{3} = \\frac{20 + 10 + 30}{3} = \\frac{60}{3} = 20\n\\]\nThis metric is much better than the average error as it gives us an idea of the average distance between individual predictions and the ground truth. Looking at the example data, the model is on average \\(20\\) away from the ground truth.\n\n\n10.2.2 Mean Squared Error\nAnother way to do so is to compute the average of the squared errors. This metric is called the Mean Squared Error (MSE):\n\\[\n\\text{Mean Squared Error} = \\frac{\\text{Error}_1^2 + \\text{Error}_2^2 + \\cdots + \\text{Error}_n^2}{n}\n\\]\nOr, in \\(\\Sigma\\) notation:\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\text{Prediction}_i - \\text{Truth}_i)^2\n\\]\nThis method has the advantage of turning every error into a positive number before averaging. This way, errors do not cancel out.\nComputing the Mean Squared Error for the example data, we get:\n\\[\n\\text{MSE} = \\frac{20^2 + 10^2 + (-30)^2}{3} = \\frac{400 + 100 + 900}{3} = \\frac{1400}{3} \\approx 466.67\n\\]\nDo you notice something strange? The resulting metric is much larger than expected, beyond the scale of the original errors.\nOne way to make this metric more interpretable is to take the square root of this number:\n\\[\n\\sqrt{466.67} \\approx 21.6\n\\]\nThis is called the Root Mean Squared Error (RMSE), another commonly used regression model performance metric. It is computed as follows:\n\\[\n\\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (\\text{Prediction}_i - \\text{Truth}_i)^2 } = \\sqrt{\\text{MSE}}\n\\]\n\n\n10.2.3 Average Error is still useful\nDoes that mean that we should never use average Error? Not exactly.\nThe Average Error is still a useful metric to see if a prediction model has a bias; i.e., if a model consistently over- or under-predicts. If the average error of the model is not close to \\(0\\), it means that there is systematic over- or under-prediction.\nThis can be illustrated with the following example:\n\n\n\nProperty\nPredicted Price\nActual Price\n\n\n\n\n1\n150\n120\n\n\n2\n130\n100\n\n\n3\n140\n130\n\n\n\n\nExercise 10.2 Calculate the Average Error of this model and determine whether the model over- or under-predicts.\n\n\nSolution 10.2. \\[\\begin{aligned}\n\\text{Prediction Error}_1 &= 150 - 120 = 30 \\\\\n\\text{Prediction Error}_2 &= 130 - 100 = 30 \\\\\n\\text{Prediction Error}_3 &= 140 - 130 = 10 \\\\\n\\text{Average Error} &= \\frac{30 + 30 + 10}{3} = \\frac{70}{3} \\approx 23.33\n\\end{aligned}\n\\]\nSince the average error is positive, the model over-predicts.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-regression.html#practice-exercise",
    "href": "chapters/evaluation-regression.html#practice-exercise",
    "title": "10  Evaluating Regression Models",
    "section": "10.3 Practice Exercise",
    "text": "10.3 Practice Exercise\nLooking at these two pricing models, which one would you pick?\n\n\n\nProperty\nModel A Prediction\nModel B Prediction\nTruth\n\n\n\n\n1\n150\n140\n120\n\n\n2\n110\n100\n100\n\n\n3\n100\n140\n140\n\n\n\n\nExercise 10.3 Show that Model B generates predictions that are closer to the truth than Model A, using the error metrics shown above.\n\n\nSolution 10.3. Model A:\n\nErrors: \\(150-120=30\\), \\(110-100=10\\), \\(100-140=-40\\)\nAverage Error: \\(\\frac{30 + 10 + (-40)}{3} = 0\\)\nMAE: \\(\\frac{|30| + |10| + |-40|}{3} = \\frac{30 + 10 + 40}{3} = 26.67\\)\nMSE: \\(\\frac{30^2 + 10^2 + (-40)^2}{3} = \\frac{900 + 100 + 1600}{3} = \\frac{2600}{3} \\approx 866.67\\)\nRMSE: \\(\\sqrt{866.67} \\approx 29.43\\)\n\nModel B:\n\nErrors: \\(140-120=20\\), \\(100-100=0\\), \\(140-140=0\\)\nAverage Error: \\(\\frac{20 + 0 + 0}{3} = 6.67\\)\nMAE: \\(\\frac{|20| + |0| + |0|}{3} = \\frac{20}{3} \\approx 6.67\\)\nMSE: \\(\\frac{20^2 + 0^2 + 0^2}{3} = \\frac{400}{3} \\approx 133.33\\)\nRMSE: \\(\\sqrt{133.33} \\approx 11.55\\)\n\nAll metrics show that Model B is closer to the truth.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-regression.html#choosing-between-metrics",
    "href": "chapters/evaluation-regression.html#choosing-between-metrics",
    "title": "10  Evaluating Regression Models",
    "section": "10.4 Choosing Between Metrics",
    "text": "10.4 Choosing Between Metrics\nThis chapter has introduced four performance metrics:\n\nAverage Error\nMean Absolute Error\nMean Squared Error\nRoot Mean Squared Error\n\nIn the example above, all metrics agreed; in other words, all metrics gave an advantage to Model B. This is not always the case.\n\nExercise 10.4 Compute the MSE and MAE of the two models below:\n\n\n\nProperty\nModel A Prediction\nModel B Prediction\nTruth\n\n\n\n\n1\n115\n110\n100\n\n\n2\n105\n110\n120\n\n\n3\n125\n130\n140\n\n\n4\n100\n90\n150\n\n\n\n\n\nSolution 10.4. Model A:\n\nErrors: \\(115-100=15\\), \\(105-120=-15\\), \\(125-140=-15\\), \\(100-150=-50\\)\nMAE: \\(\\frac{|15| + |-15| + |-15| + |-50|}{4} = \\frac{15 + 15 + 15 + 50}{4} = \\frac{95}{4} = 23.75\\)\nMSE: \\(\\frac{15^2 + (-15)^2 + (-15)^2 + (-50)^2}{4} = \\frac{225 + 225 + 225 + 2500}{4} = \\frac{3175}{4} = 793.75\\)\n\nModel B:\n\nErrors: \\(110-100=10\\), \\(110-120=-10\\), \\(130-140=-10\\), \\(90-150=-60\\)\nMAE: \\(\\frac{|10| + |-10| + |-10| + |-60|}{4} = \\frac{10 + 10 + 10 + 60}{4} = \\frac{90}{4} = 22.5\\)\nMSE: \\(\\frac{10^2 + (-10)^2 + (-10)^2 + (-60)^2}{4} = \\frac{100 + 100 + 100 + 3600}{4} = \\frac{3900}{4} = 975\\)\n\n\nAs you can see, Model A has a higher MAE than Model B, and Model B has a higher MSE than model A. How can that be? To understand the reason why, we need to understand the difference between the absolute value and the squared value of a number. These two functions are visualised below:\n\n\n\nAbsolute value and square functions\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 200)\nabs_x = np.abs(x)\nsq_x = x**2\n\nplt.figure(figsize=(7,5))\nplt.plot(x, abs_x, label='Absolute Value $|x|$', linewidth=2)\nplt.plot(x, sq_x, label='Square $x^2$', linewidth=2)\nplt.title('Comparison of $|x|$ and $x^2$', fontsize=18)\nplt.xlabel('$x$', fontsize=16)\nplt.ylabel('Value', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.legend(fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nYou may see that the square function increases faster as numbers grow larger or more negative, whereas the growth rate of the absolute value function stays constant. This means that the Mean Squared Error will strongly penalise extreme errors. Remembering the previous example:\n\n\n\nProperty\nModel A Prediction\nModel B Prediction\nTruth\n\n\n\n\n1\n115\n110\n100\n\n\n2\n105\n110\n120\n\n\n3\n125\n130\n140\n\n\n4\n100\n90\n150\n\n\n\nModel B has a very high prediction error on property 4 (\\(-60\\)), which results in a higher MSE value than Model A, despite having a lower Mean Absolute Error. What to do in these cases?\nFirst, this does not happen often. It was a bit of work to build an example that would show this edge case.\nPractical considerations aside, there is no one-size-fits-all metric. The best metric is the one that best measures the consequences of an error in the real world.\nTaking the example of property pricing, large errors can have a strong negative impact:\n\nOver-pricing may lead to financial losses as the pricing company may not be able to sell the property\nUnder-pricing may reduce the number of deals a real estate company can make\n\nGiven these considerations, the Mean Squared Error is a better choice.\nThere are also mathematical reasons why the Mean Squared Error and Root Mean Squared Errors are sometimes preferred over the Mean Absolute Error. One of them is explained in the note below.\n\n\nMean Square Error and Differentiability\n\nThe Mean Absolute Error (MAE) function is not differentiable at \\(0\\), because the absolute value function \\(|x|\\) has a “sharp corner” at \\(x=0\\).\nThis is an issue for many machine learning algorithms, which rely on differentiability for optimisation (such as gradient descent). In contrast, the Mean Squared Error (MSE) is differentiable everywhere, making it easier to use for training models.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-regression.html#final-thoughts",
    "href": "chapters/evaluation-regression.html#final-thoughts",
    "title": "10  Evaluating Regression Models",
    "section": "10.5 Final Thoughts",
    "text": "10.5 Final Thoughts\nThis section described regression model evaluation. It is very similar to the approach used for classification models described in the previous chapter:\n\nSet aside a share of the training data as a test set\nUsing the trained model, generate predictions on this test set\nCalculate the distance to the truth of the predictions generated using performance metrics such as Average Error, MAE, MSE or RMSE\n\nThe main difference is the distance calculation used.\nThis is it for model evaluation. The next chapter will introduce this book’s second Machine Learning model architecture: Decision Trees.",
    "crumbs": [
      "Model Evaluation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Evaluating Regression Models</span>"
    ]
  },
  {
    "objectID": "chapters/trees-intuition.html",
    "href": "chapters/trees-intuition.html",
    "title": "11  Decision Trees",
    "section": "",
    "text": "11.1 Multiple Splits\nIt is now time to introduce a second model architecture. How would you predict the class of the unknown observation without using distance functions or nearest neighbours?\nOne possibility would be to split the data at \\(x_1 = 2\\). Any observation with \\(x_1\\) greater than \\(2\\) would be classified as \\(\\circ\\); the rest would be classified as \\(\\times\\).\nFor a more complex case, how would you predict the class of the unknown observation with this training data? How would you split the \\(\\times\\) from the \\(\\circ\\)?\nIn this example, there is no single line that can perfectly split the two groups. In Computer Science, this is also called the XOR problem.\nInstead of splitting the data once, what if you could split the data multiple times? An example is shown below:",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-intuition.html#multiple-splits",
    "href": "chapters/trees-intuition.html#multiple-splits",
    "title": "11  Decision Trees",
    "section": "",
    "text": "A more complex example problem\n\n\n\n\nFigure code\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nax.set_xticks(np.arange(0, 4, 1))\nax.set_yticks(np.arange(0, 4, 1))\nax.grid(True, linestyle='-', color='lightgrey', linewidth=0.8)\n\nax.set_xlim(0, 3.5)\nax.set_ylim(0, 3.5)\n\nax.set_xlabel('$x_1$', fontsize=16)\nax.set_ylabel('$x_2$', rotation=0, ha='right', fontsize=16)\n\nax.tick_params(axis='both', which='major', labelsize=14)\n\nnp.random.seed(42)\nx1_region1 = np.random.uniform(0.2, 1.8, 10)\nx2_region1 = np.random.uniform(0.2, 1.8, 10)\nax.scatter(x1_region1, x2_region1, marker='o', color='blue', s=100, facecolors='none', edgecolors='blue', linewidths=2, label='Class O')\n\nx1_region2 = np.random.uniform(2.1, 3.2, 10)\nx2_region2 = np.random.uniform(0.2, 0.9, 10)\nax.scatter(x1_region2, x2_region2, marker='x', color='red', s=100, linewidths=3, label='Class X')\n\nx1_region3 = np.random.uniform(0.2, 1.8, 10)\nx2_region3 = np.random.uniform(2.1, 3.2, 10)\nax.scatter(x1_region3, x2_region3, marker='x', color='red', s=100, linewidths=3)\n\nx1_region4 = np.random.uniform(2.1, 3.2, 10)\nx2_region4 = np.random.uniform(1.1, 3.2, 10)\nax.scatter(x1_region4, x2_region4, marker='o', color='blue', s=100, facecolors='none', edgecolors='blue', linewidths=2)\n\nax.set_title('More Complex Problem', fontsize=18)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.savefig(\"images/trees/complex_problem.png\")\nplt.show()\n\n\n\n\nXOR Problem\n\nThe XOR (Exclusive Or) Problem refers to a foundational Machine Learning and Computer Science problem in which two classes cannot be separated by any single straight line. It demonstrates the weaknesses of simple linear models.\n\n\n\nXOR Problem: Can you find a single line separating the x from the o?\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.set_xticks([0, 1])\nax.set_yticks([0, 1])\nax.set_xlim(-0.2, 1.2)\nax.set_ylim(-0.2, 1.2)\nax.grid(True, linestyle='-', color='lightgrey', linewidth=0.8)\n\n# XOR points\npoints = np.array([[0,0],[0,1],[1,0],[1,1]])\nlabels = [0,1,1,0]\nfor pt, lbl in zip(points, labels):\n    if lbl == 0:\n        ax.scatter(pt[0], pt[1], marker='o', color='blue', s=120, edgecolor='blue', facecolors='none', linewidth=2)\n    else:\n        ax.scatter(pt[0], pt[1], marker='x', color='red', s=120, linewidths=3)\nax.set_xlabel('$x_1$', fontsize=16)\nax.set_ylabel('$x_2$', fontsize=16)\nax.set_title('XOR Problem', fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.savefig(\"images/trees/xor_problem.png\")\nplt.show()\n\nThe XOR is a logical operation that takes two Boolean (True/False or 1/0) values as inputs and returns 1 (or True) if they are different and 0 or False otherwise. You can see the truth table for this operation below.\nXOR Truth Table\n\n\n\nInput A\nInput B\nOutput (A XOR B)\n\n\n\n\nFalse\nFalse\nFalse\n\n\nFalse\nTrue\nTrue\n\n\nTrue\nFalse\nTrue\n\n\nTrue\nTrue\nFalse\n\n\n\nMore information on the XOR problem at (“Exclusive Or,” n.d.)\n\n\n\n\n\nSplitting the data multiple times\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(42)\nx1_region1 = np.random.uniform(0.2, 1.8, 10)\nx2_region1 = np.random.uniform(0.2, 1.8, 10)\nx1_region2 = np.random.uniform(2.1, 3.2, 10)\nx2_region2 = np.random.uniform(0.2, 0.9, 10)\nx1_region3 = np.random.uniform(0.2, 1.8, 10)\nx2_region3 = np.random.uniform(2.1, 3.2, 10)\nx1_region4 = np.random.uniform(2.1, 3.2, 10)\nx2_region4 = np.random.uniform(1.1, 3.2, 10)\n\nfig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\nsplits = [\n    {'lines': [([2, 2], [0, 3.5])]},\n    {'lines': [([2, 2], [0, 3.5]), ([2, 3.5], [1, 1])]},\n    {'lines': [([2, 2], [0, 3.5]), ([2, 3.5], [1, 1]), ([0, 2], [2, 2])]}\n]\ntitles = ['First Split', 'Second Split', 'Third Split']\n\nfor i, ax in enumerate(axs):\n    ax.set_xticks(np.arange(0, 4, 1))\n    ax.set_yticks(np.arange(0, 4, 1))\n    ax.set_xlim(0, 3.5)\n    ax.set_ylim(0, 3.5)\n    ax.grid(True, linestyle='-', color='lightgrey', linewidth=0.8)\n    ax.scatter(x1_region1, x2_region1, marker='o', color='blue', s=100, facecolors='none', edgecolors='blue', linewidths=2)\n    ax.scatter(x1_region2, x2_region2, marker='x', color='red', s=100, linewidths=3)\n    ax.scatter(x1_region3, x2_region3, marker='x', color='red', s=100, linewidths=3)\n    ax.scatter(x1_region4, x2_region4, marker='o', color='blue', s=100, facecolors='none', edgecolors='blue', linewidths=2)\n    for line in splits[i]['lines']:\n        ax.plot(line[0], line[1], color='black', linestyle='-', linewidth=2)\n    ax.set_title(titles[i], fontsize=18)\n    ax.set_xlabel('$x_1$', fontsize=16)\n    if i == 0:\n        ax.set_ylabel('$x_2$', fontsize=16)\n    ax.tick_params(axis='both', which='major', labelsize=14)\nplt.tight_layout()\nplt.savefig(\"images/trees/three_splits.png\")\nplt.show()",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-intuition.html#from-splits-to-predictions",
    "href": "chapters/trees-intuition.html#from-splits-to-predictions",
    "title": "11  Decision Trees",
    "section": "11.2 From Splits to Predictions",
    "text": "11.2 From Splits to Predictions\nUsing these three splits, how could you classify a new observation?\nOne way to do so would be to go through the splits we did one by one:\n\nIf \\(x_1 \\geq 2\\):\n\nIf \\(x_2 \\geq 1\\): assign \\(\\circ\\)\nElse: assign \\(\\times\\)\n\nElse:\n\nIf \\(x_2 \\geq 2\\): assign \\(\\times\\)\nElse: assign \\(\\circ\\)\n\n\nThe observation is then assigned the label that is the majority in the resulting partition.\n\nExercise 11.1 Using the above list, generate predictions for the following observations:\n\nObservation 1: \\(x_1 = 3,x_2 = 3\\)\nObservation 2: \\(x_1 = 1,x_2 = 3\\)\nObservation 3: \\(x_1 = 1,x_2 = 1\\)",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-intuition.html#from-partition-to-trees",
    "href": "chapters/trees-intuition.html#from-partition-to-trees",
    "title": "11  Decision Trees",
    "section": "11.3 From Partition to Trees",
    "text": "11.3 From Partition to Trees\nThe above list may be difficult to follow. This type of logic could be more elegantly represented as a tree (also called a decision tree):\n\n\n\nVisualising splits as a Decision Tree\n\n\n\n\nFigure code\n\nfrom graphviz import Digraph\ndot = Digraph()\ndot.attr(rankdir='TB', fontsize='16', fontname='Helvetica') # Top to bottom, Helvetica font\n# Decision nodes (ovals)\ndot.attr('node', shape='ellipse', fontname='Helvetica')\ndot.node('A', 'x₁ ≥ 2?')\ndot.node('B', 'x₂ ≥ 1?')\ndot.node('C', 'x₂ ≥ 2?')\n# Leaf nodes (circles)\ndot.attr('node', shape='circle', style='filled', fillcolor=\"#D46363\", fontname='Helvetica')\ndot.node('E', 'x')\ndot.node('F', 'x')\ndot.attr('node', shape='circle', style='filled', fillcolor=\"#5B90C4\", fontname='Helvetica')\ndot.node('D', 'o')\ndot.node('G', 'o')\n\nwith dot.subgraph() as s:\n    s.attr(rank='same')\n    s.node('D')\n    s.node('E') \n    s.node('F') \n    s.node('G') \n    s.edge('D', 'E', style='invis')\n    s.edge('E', 'F', style='invis')\n    s.edge('F', 'G', style='invis')\n\n# Edges\ndot.edge('A', 'B', 'Yes')\ndot.edge('A', 'C', 'No')\ndot.edge('B', 'D', 'Yes')\ndot.edge('C', 'F', 'Yes')\ndot.edge('B', 'E', 'No')\ndot.edge('C', 'G', 'No')\n\n# Render the graph\ndot.render('images/trees/tree_example', format='png', cleanup=True)\n\n\nExercise 11.2 Generate predictions for the following observations by going down the Decision Tree above:\n\nObservation 1: \\(x_1 = 3,x_2 = 3\\)\nObservation 2: \\(x_1 = 1,x_2 = 3\\)\nObservation 3: \\(x_1 = 1,x_2 = 1\\)\n\n\nTo better understand the relationship between trees and data splits, the tree and the partition can be visualised next to one another:\n\n\n\nDecision Tree as a partition of space\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom graphviz import Digraph\n\n# 1. Create and render the Graphviz tree with leaf numbers BELOW the node\ndot = Digraph()\ndot.attr(rankdir='TB', fontsize='16', fontname='Helvetica')\ndot.attr('node', shape='ellipse', fontname='Helvetica')\ndot.node('A', 'x₁ ≥ 2?')\ndot.node('B', 'x₂ ≥ 1?')\ndot.node('C', 'x₂ ≥ 2?')\n\n# Leaves: label is just the class, number is a separate node below\ndot.attr('node', shape='circle', style='filled', fillcolor=\"#D46363\", fontname='Helvetica')\ndot.node('E', 'x')\ndot.node('F', 'x')\ndot.attr('node', shape='circle', style='filled', fillcolor=\"#5B90C4\", fontname='Helvetica')\ndot.node('D', 'o')\ndot.node('G', 'o')\n\n# Add invisible nodes for numbers below each leaf\ndot.attr('node', shape='plaintext', fontname='Helvetica', fontsize='24', fillcolor='white')\ndot.node('E_num', '2')\ndot.node('F_num', '3')\ndot.node('D_num', '1')\ndot.node('G_num', '4')\n\n# Position numbers below leaves using invisible edges\ndot.edge('E', 'E_num', style='invis', weight='100')\ndot.edge('F', 'F_num', style='invis', weight='100')\ndot.edge('D', 'D_num', style='invis', weight='100')\ndot.edge('G', 'G_num', style='invis', weight='100')\n\nwith dot.subgraph() as s:\n    s.attr(rank='same')\n    s.node('D')\n    s.node('E') \n    s.node('F') \n    s.node('G') \n    s.edge('D', 'E', style='invis')\n    s.edge('E', 'F', style='invis')\n    s.edge('F', 'G', style='invis')\n\nwith dot.subgraph() as s:\n    s.attr(rank='same')\n    s.node('D_num')\n    s.node('E_num')\n    s.node('F_num')\n    s.node('G_num')\n    s.edge('D_num', 'E_num', style='invis')\n    s.edge('E_num', 'F_num', style='invis')\n    s.edge('F_num', 'G_num', style='invis')\n\ndot.edge('A', 'B', 'Yes')\ndot.edge('A', 'C', 'No')\ndot.edge('B', 'D', 'Yes')\ndot.edge('B', 'E', 'No')\ndot.edge('C', 'F', 'Yes')\ndot.edge('C', 'G', 'No')\n\ntree_img_name = 'images/trees/tree_numbered'\ndot.render(tree_img_name, format='png', cleanup=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 8))\n\n# --- Tree plot ---\nax0 = axes[0]\nax0.axis('off')\ntree_img = Image.open(tree_img_name + \".png\")\nax0.imshow(tree_img)\nax0.set_title('Decision Tree', fontsize=18)\n\n# --- Partition plot ---\nax1 = axes[1]\nax1.set_xticks(np.arange(0, 4, 1))\nax1.set_yticks(np.arange(0, 4, 1))\nax1.grid(True, linestyle='-', color='lightgrey', linewidth=0.8)\nax1.set_xlim(0, 3.5)\nax1.set_ylim(0, 3.5)\nax1.set_xlabel('$x_1$', fontsize=16)\nax1.set_ylabel('$x_2$', rotation=0, ha='right', fontsize=16)\nax1.tick_params(axis='both', which='major', labelsize=14)\n\nnp.random.seed(42)\nx1_region1 = np.random.uniform(0.2, 1.8, 10)\nx2_region1 = np.random.uniform(0.2, 1.8, 10)\nax1.scatter(x1_region1, x2_region1, marker='o', color='blue', s=100, facecolors='none', edgecolors='blue', linewidths=2, label='Class O')\n\nx1_region2 = np.random.uniform(2.1, 3.2, 10)\nx2_region2 = np.random.uniform(0.2, 0.9, 10)\nax1.scatter(x1_region2, x2_region2, marker='x', color='red', s=100, linewidths=3, label='Class X')\n\nx1_region3 = np.random.uniform(0.2, 1.8, 10)\nx2_region3 = np.random.uniform(2.1, 3.2, 10)\nax1.scatter(x1_region3, x2_region3, marker='x', color='red', s=100, linewidths=3)\n\nx1_region4 = np.random.uniform(2.1, 3.2, 10)\nx2_region4 = np.random.uniform(1.1, 3.2, 10)\nax1.scatter(x1_region4, x2_region4, marker='o', color='blue', s=100, facecolors='none', edgecolors='blue', linewidths=2)\n\n# Partition lines\nax1.plot([2, 2], [0, 3.5], color='black', linestyle='-', linewidth=2)\nax1.plot([0, 2], [2, 2], color='black', linestyle='-', linewidth=2)\nax1.plot([2, 3.5], [1, 1], color='black', linestyle='-', linewidth=2)\n\nax1.set_title('Partition of Input Space', fontsize=18)\nax1.set_aspect('equal', adjustable='box')\n\n# Overlay region numbers (matching tree leaves)\nax1.text(1, 2.7, '3', fontsize=90, color='red', alpha=0.15, ha='center', va='center', weight='bold')\nax1.text(1, 1, '4', fontsize=90, color='blue', alpha=0.15, ha='center', va='center', weight='bold')\nax1.text(2.7, 0.5, '2', fontsize=90, color='red', alpha=0.15, ha='center', va='center', weight='bold')\nax1.text(2.7, 2.2, '1', fontsize=90, color='blue', alpha=0.15, ha='center', va='center', weight='bold')\n\nplt.tight_layout()\nplt.savefig(\"images/trees/tree_numbered_partition.png\")\nplt.show()\n\nEach split or comparison creates a linear separation in the feature space, represented by a black line in the chart above.\nIn Computer Science, trees are a type of graph. Graphs are collections of nodes and edges.\n\n\n\nGraphs are collections of nodes and edges connecting them\n\n\nEdges can be directed or undirected. Directed edges can only be traversed in one direction, usually represented as arrows. Edges, i.e., connections between nodes, can form cycles. These cycles are paths that start and end with the same node.\n\n\n\nEdges can be directed\n\n\n\nExercise 11.3 The diagram above highlights the cycle A→B→D→A. Can you find another cycle?\n\nTrees are Directed Acyclic Graphs (DAGs). This is a bit of a mouthful, let’s take these terms one by one:\n\nDirected: They flow from the root to the leaves\nAcyclic: They do not contain any cycles; there is only a single path from the root to each node\n\nBelow is an example of a tree with seven nodes:\n\n\n\nA simple tree with seven nodes\n\n\nTrees, like all graphs, are composed of nodes and edges. In addition to the usual graph jargon, there are a few tree-specific concepts. To understand them, consider the example above:\n\nParent and child nodes: both connected by an edge (nodes B and D)\nRoot node: The topmost node of a tree (node A)\nLeaf node: A node with no children (node G)\n\n\nExercise 11.4 Find another example of:\n\nLeaf node\nParent node",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-intuition.html#from-trees-to-maps",
    "href": "chapters/trees-intuition.html#from-trees-to-maps",
    "title": "11  Decision Trees",
    "section": "11.4 From Trees to Maps",
    "text": "11.4 From Trees to Maps\nJust like the KNN model, Decision Trees build a map from the input features to the target variable. Using a Decision Tree to generate predictions for all possible feature values, we get the following map:\n\n\n\nBuilding a map with Decision Trees\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# Generate meshgrid\nxx, yy = np.meshgrid(np.linspace(0, 3.5, 200), np.linspace(0, 3.5, 200))\nZ = np.zeros_like(xx, dtype=int)\n\n# Apply the tree rules\nfor i in range(xx.shape[0]):\n    for j in range(xx.shape[1]):\n        x1, x2 = xx[i, j], yy[i, j]\n        if x1 &gt;= 2:\n            if x2 &gt;= 1:\n                Z[i, j] = 0  # circle\n            else:\n                Z[i, j] = 1  # x\n        else:\n            if x2 &gt;= 2:\n                Z[i, j] = 1  # x\n            else:\n                Z[i, j] = 0  # circle\n\ncmap = ListedColormap(['#80C2FF', '#FF8080'])\nplt.figure(figsize=(8, 8))\nplt.contourf(xx, yy, Z, cmap=cmap, alpha=0.4)\n\n# Overlay the data points\nnp.random.seed(42)\nx1_region1 = np.random.uniform(0.2, 1.8, 10)\nx2_region1 = np.random.uniform(0.2, 1.8, 10)\nx1_region2 = np.random.uniform(2.1, 3.2, 10)\nx2_region2 = np.random.uniform(0.2, 0.9, 10)\nx1_region3 = np.random.uniform(0.2, 1.8, 10)\nx2_region3 = np.random.uniform(2.1, 3.2, 10)\nx1_region4 = np.random.uniform(2.1, 3.2, 10)\nx2_region4 = np.random.uniform(1.1, 3.2, 10)\n\nplt.scatter(x1_region1, x2_region1, marker='o', color='blue', s=100, facecolors='none', edgecolors='blue', linewidths=2)\nplt.scatter(x1_region2, x2_region2, marker='x', color='red', s=100, linewidths=3)\nplt.scatter(x1_region3, x2_region3, marker='x', color='red', s=100, linewidths=3)\nplt.scatter(x1_region4, x2_region4, marker='o', color='blue', s=100, facecolors='none', edgecolors='blue', linewidths=2)\n\nplt.plot([2, 2], [0, 3.5], color='black', linestyle='-', linewidth=2)\nplt.plot([0, 2], [2, 2], color='black', linestyle='-', linewidth=2)\nplt.plot([2, 3.5], [1, 1], color='black', linestyle='-', linewidth=2)\n\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16)\nplt.title('Decision Tree Partition Map', fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.savefig(\"images/trees/decision_tree_partition_map.png\")\nplt.show()",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-intuition.html#final-thoughts",
    "href": "chapters/trees-intuition.html#final-thoughts",
    "title": "11  Decision Trees",
    "section": "11.5 Final Thoughts",
    "text": "11.5 Final Thoughts\nThat is it, we have built our first Decision Tree! We can use this model to predict any new observation based on its two features and position on the map above.\nThis is only a simple introduction to Decision Trees. The next chapters will explore the inner workings of this model family, starting with the evaluation of data splits. What makes a good split of a dataset?",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-intuition.html#solutions",
    "href": "chapters/trees-intuition.html#solutions",
    "title": "11  Decision Trees",
    "section": "11.6 Solutions",
    "text": "11.6 Solutions\n\nSolution 11.1. Exercise 11.1\n\nObservation 1: \\(\\circ\\)\nObservation 2: \\(\\times\\)\nObservation 3: \\(\\circ\\)\n\n\n\nSolution 11.2. Exercise 11.2\n\nObservation 1: \\(\\circ\\)\nObservation 2: \\(\\times\\)\nObservation 3: \\(\\circ\\)\n\n\n\nSolution 11.3. Exercise 11.3 There are many possible cycles, here are two examples:\n\nC→D→C\nC→B→D→C\n\n\n\nExercise 11.5 Solution. Exercise 11.5\n\nLeaf node: D, E, F, G\nParent node: A, B, C\n\n\n\n\n\n\n“Exclusive Or.” n.d. https://en.wikipedia.org/wiki/Exclusive_or.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/splitting-maths.html",
    "href": "chapters/splitting-maths.html",
    "title": "12  Evaluating Splits",
    "section": "",
    "text": "12.1 Best Split First\nIn most problems, the different groups cannot be separated. Going back to the tumour diagnosis example, the training data looks messy:\nHow could we build a tree from there?\nWe could start by finding the best split, the split that separates the data best. The split will create two groups:\nFor each of the new groups, we can find the best splits, apply them, and create two new groups:\nWe can keep splitting until the subgroups are “pure”; i.e., there are only observations belonging to the same class.\nIn practice, other constraints are applied. In general it makes sense to limit the depth of a tree to make the size of the resulting model manageable. These simple models generally have stronger generalisation abilities.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Splits</span>"
    ]
  },
  {
    "objectID": "chapters/splitting-maths.html#best-split-first",
    "href": "chapters/splitting-maths.html#best-split-first",
    "title": "12  Evaluating Splits",
    "section": "",
    "text": "First split\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import DecisionTreeClassifier\n\nbenign_center = [80, 700]\nmalignant_center = [110, 1200]\nn_samples = 70\n\nX_benign, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=1)\nX_malignant, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=2)\n\nbenign_std = [10, 120]\nmalignant_std = [12, 300]\n\nX_benign = X_benign * benign_std + benign_center\nX_malignant = X_malignant * malignant_std + malignant_center\n\n# Stack data and create labels\nX = np.vstack([X_benign, X_malignant])\ny = np.array([0]*n_samples + [1]*n_samples)  # 0: Benign, 1: Malignant\n\n# Fit a decision tree with depth=1 (stump)\ntree = DecisionTreeClassifier(max_depth=1, random_state=0)\ntree.fit(X, y)\n\n# Get split info\nfeature_names = ['Perimeter Mean (µm)', 'Area Mean (µm²)']\nsplit_feature = tree.tree_.feature[0] \nsplit_threshold = tree.tree_.threshold[0]\nprint(f\"First split: {feature_names[split_feature]} &lt; {split_threshold:.2f}\")\n\n# Plot\nplt.figure(figsize=(8,6))\nplt.scatter(X_benign[:,0], X_benign[:,1], marker='o', color='blue', label='Benign')\nplt.scatter(X_malignant[:,0], X_malignant[:,1], marker='x', color='red', label='Malignant')\n\n# Plot the split\nif split_feature == 0:\n    plt.axvline(split_threshold, color='green', linestyle='--', linewidth=2, label=f'Split')\nelif split_feature == 1:\n    plt.axhline(split_threshold, color='green', linestyle='--', linewidth=2, label=f'Split')\n\nplt.xlabel('Perimeter Mean (µm)', fontsize=16)\nplt.ylabel('Area Mean (µm²)', fontsize=16)\nplt.title('Tumours: Perimeter Mean vs Area Mean', fontsize=18)\nplt.legend(fontsize=12)\nplt.grid(True)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.savefig(\"images/trees/example_first_split.png\")\nplt.show()\n\n\n\n\n\nAll splits\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import DecisionTreeClassifier\n\nbenign_center = [80, 700]\nmalignant_center = [110, 1200]\nn_samples = 70\n\nX_benign, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=1)\nX_malignant, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=2)\n\nbenign_std = [10, 120]\nmalignant_std = [12, 300]\n\nX_benign = X_benign * benign_std + benign_center\nX_malignant = X_malignant * malignant_std + malignant_center\n\n# Stack data and create labels\nX = np.vstack([X_benign, X_malignant])\ny = np.array([0]*n_samples + [1]*n_samples)  # 0: Benign, 1: Malignant\n\n# Fit a decision tree (no max_depth limit)\ntree = DecisionTreeClassifier(random_state=0,max_depth=2)\ntree.fit(X, y)\n\nfeature_names = ['Perimeter Mean (µm)', 'Area Mean (µm²)']\n\n# Helper function to recursively collect all splits\ndef collect_splits(tree, node_id=0, path=[], splits=[]):\n    feature = tree.tree_.feature[node_id]\n    threshold = tree.tree_.threshold[node_id]\n    if feature &gt;= 0:\n        # Save the split with its path (list of (feature, threshold, direction))\n        splits.append((path.copy(), feature, threshold))\n        # Left child: feature &lt;= threshold\n        collect_splits(tree, tree.tree_.children_left[node_id], path + [(feature, threshold, 'left')], splits)\n        # Right child: feature &gt; threshold\n        collect_splits(tree, tree.tree_.children_right[node_id], path + [(feature, threshold, 'right')], splits)\n    return splits\n\nsplits = collect_splits(tree, 0, [], [])\n\n# For plotting, get the data limits\nx0min, x0max = X[:,0].min() - 5, X[:,0].max() + 5\nx1min, x1max = X[:,1].min() - 50, X[:,1].max() + 50\n\nn_splits = len(splits)\nfig, axes = plt.subplots(1, n_splits, figsize=(6*n_splits, 6), sharex=True, sharey=True)\n\nif n_splits == 1:\n    axes = [axes]  # Make iterable\n\nfor i in range(n_splits):\n    ax = axes[i]\n    ax.scatter(X_benign[:,0], X_benign[:,1], marker='o', color='blue', label='Benign' if i==0 else \"\")\n    ax.scatter(X_malignant[:,0], X_malignant[:,1], marker='x', color='red', label='Malignant' if i==0 else \"\")\n\n    # Draw all previous splits as black lines, in their respective regions\n    for j in range(i):\n        path, feat, thresh = splits[j]\n        xlims = [x0min, x0max]\n        ylims = [x1min, x1max]\n        for (pf, pt, dirn) in path:\n            if pf == 0:\n                if dirn == 'left':\n                    xlims[1] = min(xlims[1], pt)\n                else:\n                    xlims[0] = max(xlims[0], pt)\n            else:\n                if dirn == 'left':\n                    ylims[1] = min(ylims[1], pt)\n                else:\n                    ylims[0] = max(ylims[0], pt)\n        if feat == 0:\n            ax.plot([thresh, thresh], ylims, color='black', linewidth=2)\n        else:\n            ax.plot(xlims, [thresh, thresh], color='black', linewidth=2)\n\n    # Draw current split as green dashed line, in its region\n    path, feat, thresh = splits[i]\n    xlims = [x0min, x0max]\n    ylims = [x1min, x1max]\n    for (pf, pt, dirn) in path:\n        if pf == 0:\n            if dirn == 'left':\n                xlims[1] = min(xlims[1], pt)\n            else:\n                xlims[0] = max(xlims[0], pt)\n        else:\n            if dirn == 'left':\n                ylims[1] = min(ylims[1], pt)\n            else:\n                ylims[0] = max(ylims[0], pt)\n    if feat == 0:\n        ax.plot([thresh, thresh], ylims, color='green', linestyle='--', linewidth=2,\n                label=f'Split {i+1}: {feature_names[feat]} = {thresh:.1f}')\n    else:\n        ax.plot(xlims, [thresh, thresh], color='green', linestyle='--', linewidth=2,\n                label=f'Split {i+1}: {feature_names[feat]} = {thresh:.1f}')\n\n    ax.set_xlabel('Perimeter Mean (µm)', fontsize=16)\n    if i == 0:\n        ax.set_ylabel('Area Mean (µm²)', fontsize=16)\n    ax.set_title(f'Split {i+1}', fontsize=18)\n    ax.grid(True)\n    ax.tick_params(axis='both', which='major', labelsize=14)\n    ax.legend(fontsize=12, loc='upper left')\n\nplt.tight_layout()\nplt.savefig(\"images/trees/example_three_splits.png\")\nplt.show()\n\n\n\n\n\nWhat is tree depth?\n\nThe depth of a tree is the distance from the root node, here the first split, to the leaf node, here the last node, responsible for assigning the prediction.\nWhat is the depth of the tree built in the previous examples?\n\n\n\nExample tree\n\n\nIt is two, as there are two edges (connections) on the path from the root node (top) to the leaf node (bottom).",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Splits</span>"
    ]
  },
  {
    "objectID": "chapters/splitting-maths.html#gini-impurity-coefficient",
    "href": "chapters/splitting-maths.html#gini-impurity-coefficient",
    "title": "12  Evaluating Splits",
    "section": "12.2 Gini Impurity Coefficient",
    "text": "12.2 Gini Impurity Coefficient\nHow would you measure the quality of a split? Or the purity of the resulting groups?\nImagine that Split 1 creates the following two groups:\n\n\n\nGroup\nMalignant\nBenign\n\n\n\n\nA\n9\n1\n\n\nB\n2\n8\n\n\n\nAnd that Split 2 creates the following two groups:\n\n\n\nGroup\nMalignant\nBenign\n\n\n\n\nC\n6\n4\n\n\nD\n5\n5\n\n\n\nIntuitively, we know that Split 1 has done better, as the resulting subgroups are more homogeneous. Group A contains mostly malignant observations and Group B more benign ones. On the other hand, Group C and D are both mixed. But how to quantify this intuition?\nThis is exactly what the Gini Impurity Coefficient measures.\nThe Gini Coefficient measures “impurity” through the probability of randomly picking two items of different classes within the same group.\nBut why would this probability measure impurity? If this probability is low, it means that we are not likely to pick items of different classes. In other words, we are likely to pick items from the same class. This probability will be 0 if all the items of a group are of a single class.\nOn the other hand, when this probability is high, we are likely to pick items of a different class. This will happen when the group is mixed. It will be 1 in the extreme case in which the group contains only a single item from each class.\nLet’s illustrate this with Group A in the table below:\n\n\n\nGroup\nMalignant\nBenign\n\n\n\n\nA\n9\n1\n\n\n\nIf we randomly picked two items in this group (with replacement) what would be the probability of picking items from two different classes (malignant and benign)?\nTry thinking about it before reading on.\nTo make this simpler, what is the probability to pick first a malignant tumour, then a benign tumour?\nThe probability of picking a malignant and benign tumour are the following: \\[\nP(\\text{malignant}) = \\frac{9}{9+1} = 0.9\n\\] \\[\nP(\\text{benign}) = \\frac{1}{9+1} = 0.1\n\\]\nAs the two picks are random and with replacement, the probability of picking one, then the other is:\n\\[\nP(\\text{malignant then benign}) = P(\\text{malignant}) \\cdot P(\\text{benign}) = 0.9 \\cdot 0.1 = 0.09\n\\]\nNow, what would be the probability of picking a benign observation then a malignant one (reversing the order)? We have already computed the probabilities above, we just need to reverse the order: \\[\nP(\\text{benign then malignant}) = P(\\text{benign}) \\cdot P(\\text{malignant}) = 0.1 \\cdot 0.9 = 0.09\n\\]\nWhich, by symmetry, gives the same results. As this sampling is independent, the probability of picking malignant then benign is the same as benign then malignant. To get the probability of these two events we just have to sum them:\n\\[\n\\begin{aligned}\nP(\\text{picking items of different class}) &= P(\\text{malignant then benign}) + P(\\text{benign then malignant}) \\\\\n&= 0.09 + 0.09 = 0.18\n\\end{aligned}\n\\]\nThat is it, we have calculated the Gini Impurity Coefficient for Group A.\n\nExercise 12.1 Compute the Gini Impurity Coefficient for Group B and show that it is 0.32.\n\n\n\nGroup\nMalignant\nBenign\n\n\n\n\nB\n2\n8\n\n\n\n\nThe Gini Impurity Coefficient of Group B is slightly higher than Group A, as it is slightly more mixed.\n\n12.2.1 Evaluating Splits\nNow, let’s get back to quantifying the quality of a split. To do so, we will compute the average Gini Coefficient of each split.\nSplit 1:\n\n\n\nGroup\nMalignant\nBenign\n\n\n\n\nA\n9\n1\n\n\nB\n2\n8\n\n\n\nSplit 2:\n\n\n\nGroup\nMalignant\nBenign\n\n\n\n\nC\n6\n4\n\n\nD\n5\n5\n\n\n\nAs covered in the previous section, we know the Gini Coefficient of both Group A and B: 0.18 and 0.32 respectively.\nThe average Gini Coefficient for Split 1, Group A and B, can be computed as follows:\n\\[\n\\frac{n_a}{n_a + n_b} \\cdot \\text{Gini}_{a} + \\frac{n_b}{n_a + n_b} \\cdot \\text{Gini}_{b}\n\\]\n\\[\n\\frac{10}{10+10} \\cdot 0.18 + \\frac{10}{10+10} \\cdot 0.32 = 0.25\n\\]\n\nExercise 12.2 Compute the Gini Coefficients for both groups (C and D) in Split 2, and prove that they are equal to: 0.48 and 0.5 respectively.\n\n\n\nGroup\nMalignant\nBenign\n\n\n\n\nC\n6\n4\n\n\nD\n5\n5\n\n\n\n\nNow, computing the average Gini Impurity Coefficient for Split 2, we get:\n\\[\n\\frac{n_c}{n_c + n_d} \\cdot \\text{Gini}_{c} + \\frac{n_d}{n_c + n_d} \\cdot \\text{Gini}_{d}\n\\]\nWe get:\n\\[\n\\frac{10}{10+10} \\cdot 0.48 + \\frac{10}{10+10} \\cdot 0.5 = 0.49\n\\]\nConcluding this section, Split 1 has an average Gini Impurity Coefficient of 0.25 compared to 0.49 for Split 2. It is the split that separates the data best.\nNow that we have a way to measure the quality of a split, we can simply try splitting the data at all feature values for all features, and pick the best one.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Splits</span>"
    ]
  },
  {
    "objectID": "chapters/splitting-maths.html#trying-different-splits",
    "href": "chapters/splitting-maths.html#trying-different-splits",
    "title": "12  Evaluating Splits",
    "section": "12.3 Trying Different Splits",
    "text": "12.3 Trying Different Splits\nTo find the best data split, the Decision Tree learning algorithm evaluates each possible splitting feature and value, and picks the one that has the lowest Gini Impurity Coefficient.\nThe example above only shows two different splits. This process can be repeated for all features and all splitting values. At each trial, the Gini Impurity Coefficient is recorded. The algorithm then selects the split achieving the lowest Gini Impurity Coefficient. This can be represented visually:\n\n\n\nTrying all combinations and picking the first split achieving the minimum Gini Impurity Coefficient\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# --- Data generation ---\nnp.random.seed(0)\nx1_plus = np.random.uniform(0.5, 1.8, 10)\nx2_plus = np.random.uniform(1.0, 3.5, 10)\nx1_circle = np.random.uniform(2.2, 3.8, 12)\nx2_circle = np.random.uniform(1.5, 3.8, 12)\n\nX_plus = np.column_stack([x1_plus, x2_plus])\nX_circle = np.column_stack([x1_circle, x2_circle])\nX = np.vstack([X_plus, X_circle])\ny = np.array([1]*len(X_plus) + [0]*len(X_circle))  # 1: x, 0: o\n\n# --- Gini impurity for a split ---\ndef gini_impurity(y_left, y_right):\n    def gini(y):\n        if len(y) == 0:\n            return 0\n        p = np.mean(y)\n        return 2 * p * (1 - p)\n    n = len(y_left) + len(y_right)\n    return (len(y_left) * gini(y_left) + len(y_right) * gini(y_right)) / n\n\n# --- Gini vs x1 threshold ---\nfidx = 0  # x1\nfmin, fmax = X[:, fidx].min(), X[:, fidx].max()\nthresholds = np.linspace(fmin, fmax, 300)\nginis = []\nfor t in thresholds:\n    left = y[X[:, fidx] &lt;= t]\n    right = y[X[:, fidx] &gt; t]\n    gini = gini_impurity(left, right)\n    ginis.append(gini)\nginis = np.array(ginis)\nmin_idx = np.argmin(ginis)\nmin_thresh = thresholds[min_idx]\nmin_gini = ginis[min_idx]\n\n# --- Plot ---\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\n\n# Top: Data scatter\nax1.set_xticks(np.arange(0, 5, 1))\nax1.set_yticks(np.arange(0, 5, 1))\nax1.grid(True, linestyle='-', color='lightgrey', linewidth=0.8)\nax1.set_xlim(0, 4.5)\nax1.set_ylim(0, 4.5)\nax1.set_xlabel('$x_1$', fontsize=16)\nax1.set_ylabel('$x_2$', rotation=0, ha='right', fontsize=16)\nax1.tick_params(axis='both', which='major', labelsize=14)\n\nax1.scatter(x1_plus, x2_plus, marker='x', color='red', s=120, linewidths=3, label='Class x')\nax1.scatter(x1_circle, x2_circle, marker='o', color='blue', s=100, facecolors='none', edgecolors='blue', linewidths=2, label='Class O')\n\n# Green split line at best threshold\nax1.axvline(min_thresh, color='green', linestyle='--', lw=2, label=f'Split at $x_1$={min_thresh:.2f}')\n\nax1.set_title('Decision Trees: Splitting Data', fontsize=18, pad=20)\nplt.gca().set_aspect('equal', adjustable='box')\n\n# To avoid duplicate legend\nhandles, labels = ax1.get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nax1.legend(by_label.values(), by_label.keys(), fontsize=12)\n\n# Bottom: Gini vs x1\nax2.plot(thresholds, ginis, color='blue', lw=2)\nax2.scatter([min_thresh], [min_gini], color='red', s=80, zorder=5, label='Minimum Gini')\nax2.axvline(min_thresh, color='green', linestyle='--', lw=2, label=f'Split at $x_1$={min_thresh:.2f}')\nax2.set_xlabel('$x_1$', fontsize=16)\nax2.set_ylabel('Gini impurity', fontsize=16)\nax2.set_title('Gini vs. $x_1$ Split', fontsize=18)\nax2.grid(True)\nax2.tick_params(axis='both', which='major', labelsize=14)\n\n\nplt.tight_layout()\nplt.savefig(\"images/trees/split_and_gini_vs_x1.png\")\nplt.show()\n\nFrom the value 1.76, the split perfectly separates the two data classes, achieving a Gini Impurity Coefficient of 0.\nBut what happens when data is not fully separable? For these cases, the Gini Impurity Coefficient will not reach 0. Still, the split that achieves the lowers Gini Impurity Coefficient will be selected.\nThis splitting trial and error can be visualised with the tumour diagnosis dataset:\n.\nWe first try splitting the data using the Perimeter Mean of the cell nuclei. The following line chart plots the Gini Impurity criterion for different splitting values over this feature.\n\n\n\nTrying different split values for Perimeter Mean\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import DecisionTreeClassifier\n\n# --- Data generation ---\nbenign_center = [80, 700]\nmalignant_center = [110, 1200]\nn_samples = 70\n\nX_benign, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=1)\nX_malignant, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=2)\n\nbenign_std = [10, 120]\nmalignant_std = [12, 300]\n\nX_benign = X_benign * benign_std + benign_center\nX_malignant = X_malignant * malignant_std + malignant_center\n\nX = np.vstack([X_benign, X_malignant])\ny = np.array([0]*n_samples + [1]*n_samples)  # 0: Benign, 1: Malignant\n\nfeature_names = ['Perimeter Mean (µm)', 'Area Mean (µm²)']\n\n# --- Fit tree ---\ntree = DecisionTreeClassifier(random_state=0, max_depth=2)\ntree.fit(X, y)\n\n# --- Gini impurity for a split ---\ndef gini_impurity(y_left, y_right):\n    def gini(y):\n        if len(y) == 0:\n            return 0\n        p = np.mean(y)\n        return 2 * p * (1 - p)\n    n = len(y_left) + len(y_right)\n    return (len(y_left) * gini(y_left) + len(y_right) * gini(y_right)) / n\n\n# --- Gini vs Perimeter Mean ---\nfidx = 0  # Perimeter Mean\nfmin, fmax = X[:, fidx].min(), X[:, fidx].max()\nthresholds = np.linspace(fmin, fmax, 300)\nginis = []\nfor t in thresholds:\n    left = y[X[:, fidx] &lt;= t]\n    right = y[X[:, fidx] &gt; t]\n    gini = gini_impurity(left, right)\n    ginis.append(gini)\nginis = np.array(ginis)\nmin_idx = np.argmin(ginis)\nmin_thresh = thresholds[min_idx]\nmin_gini = ginis[min_idx]\n\n# Actual split(s) used by the tree for Perimeter Mean\nsplits = []\ntree_ = tree.tree_\nfor node in range(tree_.node_count):\n    if tree_.feature[node] == fidx:\n        splits.append(tree_.threshold[node])\n\n# --- Plot ---\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n\n# Top: Data scatter\nax1.scatter(X_benign[:,0], X_benign[:,1], marker='o', color='blue', label='Benign')\nax1.scatter(X_malignant[:,0], X_malignant[:,1], marker='x', color='red', label='Malignant')\nax1.set_ylabel('Area Mean (µm²)', fontsize=16)\nax1.set_title('Tumours: Perimeter Mean vs Area Mean', fontsize=18)\nax1.legend(fontsize=12)\nax1.grid(True)\nax1.tick_params(axis='both', which='major', labelsize=14)\n# Green split line\nax1.axvline(min_thresh, color='green', linestyle='--', lw=2, label=f'Split at {min_thresh:.1f}')\n# To avoid duplicate legend\nhandles, labels = ax1.get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nax1.legend(by_label.values(), by_label.keys(), fontsize=12)\n\n# Bottom: Gini vs Perimeter Mean\nax2.plot(thresholds, ginis, color='blue', lw=2)\nax2.scatter([min_thresh], [min_gini], color='red', s=80, zorder=5, label='Minimum Gini')\nax2.axvline(min_thresh, color='green', linestyle='--', lw=2, label=f'Split at {min_thresh:.1f}')\nax2.set_xlabel('Perimeter Mean (µm)', fontsize=16)\nax2.set_ylabel('Gini impurity', fontsize=16)\nax2.set_title('Gini vs. Perimeter Mean', fontsize=18)\nax2.grid(True)\nax2.tick_params(axis='both', which='major', labelsize=14)\n# To avoid duplicate legend\nhandles, labels = ax2.get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nax2.legend(by_label.values(), by_label.keys(), fontsize=12, loc='upper right')\n\nplt.tight_layout(h_pad=2)\nplt.savefig(\"images/trees/scatter_and_gini_vs_perimeter.png\")\nplt.show()\n\nThis Gini Impurity Coefficient reaches a minimum of approximately 15.2 at a split value of about 96.6.\nThe same could be done with the Mean Area of the cell nuclei:\n\n\n\nTrying different split values for Perimeter Mean\n\n\n\n\nFigure code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.tree import DecisionTreeClassifier\n\n# --- Data generation ---\nbenign_center = [80, 700]\nmalignant_center = [110, 1200]\nn_samples = 70\n\nX_benign, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=1)\nX_malignant, _ = make_blobs(n_samples=n_samples, centers=[(0, 0)], cluster_std=1, random_state=2)\n\nbenign_std = [10, 120]\nmalignant_std = [12, 300]\n\nX_benign = X_benign * benign_std + benign_center\nX_malignant = X_malignant * malignant_std + malignant_center\n\nX = np.vstack([X_benign, X_malignant])\ny = np.array([0]*n_samples + [1]*n_samples)  # 0: Benign, 1: Malignant\n\nfeature_names = ['Perimeter Mean (µm)', 'Area Mean (µm²)']\n\n# --- Fit tree ---\ntree = DecisionTreeClassifier(random_state=0, max_depth=2)\ntree.fit(X, y)\n\n# --- Gini impurity for a split ---\ndef gini_impurity(y_left, y_right):\n    def gini(y):\n        if len(y) == 0:\n            return 0\n        p = np.mean(y)\n        return 2 * p * (1 - p)\n    n = len(y_left) + len(y_right)\n    return (len(y_left) * gini(y_left) + len(y_right) * gini(y_right)) / n\n\n# --- Gini vs Perimeter Mean ---\nfidx = 0  # Perimeter Mean\nfmin, fmax = X[:, fidx].min(), X[:, fidx].max()\nthresholds = np.linspace(fmin, fmax, 300)\nginis = []\nfor t in thresholds:\n    left = y[X[:, fidx] &lt;= t]\n    right = y[X[:, fidx] &gt; t]\n    gini = gini_impurity(left, right)\n    ginis.append(gini)\nginis = np.array(ginis)\nmin_idx = np.argmin(ginis)\nmin_thresh = thresholds[min_idx]\nmin_gini = ginis[min_idx]\n\n# Actual split(s) used by the tree for Perimeter Mean\nsplits = []\ntree_ = tree.tree_\nfor node in range(tree_.node_count):\n    if tree_.feature[node] == fidx:\n        splits.append(tree_.threshold[node])\n\n# --- Plot ---\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n\n# Top: Data scatter\nax1.scatter(X_benign[:,0], X_benign[:,1], marker='o', color='blue', label='Benign')\nax1.scatter(X_malignant[:,0], X_malignant[:,1], marker='x', color='red', label='Malignant')\nax1.set_ylabel('Area Mean (µm²)', fontsize=16)\nax1.set_title('Tumours: Perimeter Mean vs Area Mean', fontsize=18)\nax1.legend(fontsize=12)\nax1.grid(True)\nax1.tick_params(axis='both', which='major', labelsize=14)\n# Green split line\nax1.axvline(min_thresh, color='green', linestyle='--', lw=2, label=f'Split at {min_thresh:.1f}')\n# To avoid duplicate legend\nhandles, labels = ax1.get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nax1.legend(by_label.values(), by_label.keys(), fontsize=12)\n\n# Bottom: Gini vs Perimeter Mean\nax2.plot(thresholds, ginis, color='blue', lw=2)\nax2.scatter([min_thresh], [min_gini], color='red', s=80, zorder=5, label='Minimum Gini')\nax2.axvline(min_thresh, color='green', linestyle='--', lw=2, label=f'Split at {min_thresh:.1f}')\nax2.set_xlabel('Perimeter Mean (µm)', fontsize=16)\nax2.set_ylabel('Gini impurity', fontsize=16)\nax2.set_title('Gini vs. Perimeter Mean', fontsize=18)\nax2.grid(True)\nax2.tick_params(axis='both', which='major', labelsize=14)\n# To avoid duplicate legend\nhandles, labels = ax2.get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nax2.legend(by_label.values(), by_label.keys(), fontsize=12, loc='upper right')\n\nplt.tight_layout(h_pad=2)\nplt.savefig(\"images/trees/scatter_and_gini_vs_perimeter.png\")\nplt.show()\n\nThe Gini Impurity Coefficient reaches a minimum of approximately 16.5 at a split value of about 885.7.\nThe best split is obtained by splitting the data based on Perimeter Mean at 96.6. This process is repeated every time a group is split.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Splits</span>"
    ]
  },
  {
    "objectID": "chapters/splitting-maths.html#final-thoughts",
    "href": "chapters/splitting-maths.html#final-thoughts",
    "title": "12  Evaluating Splits",
    "section": "12.4 Final Thoughts",
    "text": "12.4 Final Thoughts\nThe Gini Impurity Coefficient is the measure used to evaluate the quality of a split in Decision Tree learning. A good split partitions the data into two homogeneous groups.\nTo train a Decision Tree model, the algorithm finds the best data splits, using the Gini Impurity Coefficient as evaluation criterion.\nThe next chapter will explore how this splitting process is applied recursively.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Splits</span>"
    ]
  },
  {
    "objectID": "chapters/splitting-maths.html#solutions",
    "href": "chapters/splitting-maths.html#solutions",
    "title": "12  Evaluating Splits",
    "section": "12.5 Solutions",
    "text": "12.5 Solutions\n\nSolution 12.1. Exercise 12.1\nFirst, compute the probabilities:\n\\[\nP(\\text{malignant}) = \\frac{2}{2+8} = 0.2\n\\] \\[\nP(\\text{benign}) = \\frac{8}{2+8} = 0.8\n\\]\nNow, the probability of picking first a malignant tumour, then a benign tumour:\n\\[\nP(\\text{malignant then benign}) = P(\\text{malignant}) \\cdot P(\\text{benign}) = 0.2 \\cdot 0.8 = 0.16\n\\]\nThe probability of picking first a benign tumour, then a malignant tumour:\n\\[\nP(\\text{benign then malignant}) = P(\\text{benign}) \\cdot P(\\text{malignant}) = 0.8 \\cdot 0.2 = 0.16\n\\]\nSum the two to get the probability of picking items of different classes:\n\\[\\begin{aligned}\nP(\\text{picking items of different class}) &= P(\\text{malignant then benign}) + P(\\text{benign then malignant}) \\\\\n&= 0.16 + 0.16 = 0.32\n\\end{aligned}\n\\]\nThe Gini Impurity Coefficient is therefore \\(0.32\\). This is in line with our intuition as this number should be higher than for Group A, as Group B is more mixed.\n\n\nSolution 12.2. Exercise 12.2\nGroup C\n\n\n\nGroup\nMalignant\nBenign\n\n\n\n\nC\n6\n4\n\n\n\n\\[\nP(\\text{malignant}) = \\frac{6}{6+4} = 0.6\n\\] \\[\nP(\\text{benign}) = \\frac{4}{6+4} = 0.4\n\\]\nNow, the probability of picking first a malignant tumour, then a benign tumour:\n\\[\nP(\\text{malignant then benign}) = P(\\text{malignant}) \\cdot P(\\text{benign}) = 0.6 \\cdot 0.4 = 0.24\n\\]\nThe probability of picking first a benign tumour, then a malignant tumour:\n\\[\nP(\\text{benign then malignant}) = P(\\text{benign}) \\cdot P(\\text{malignant}) = 0.4 \\cdot 0.6 = 0.24\n\\]\nSum the two to get the probability of picking items of different classes:\n\\[\n\\begin{aligned}\nP(\\text{picking items of different class}) &= P(\\text{malignant then benign}) + P(\\text{benign then malignant}) \\\\\n&= 0.24 + 0.24 = 0.48\n\\end{aligned}\n\\]\nThe Gini Impurity Coefficient for Group C is therefore \\(0.48\\).\nGroup D\n\n\n\nGroup\nMalignant\nBenign\n\n\n\n\nD\n5\n5\n\n\n\n\\[\nP(\\text{malignant}) = \\frac{5}{5+5} = 0.5\n\\] \\[\nP(\\text{benign}) = \\frac{5}{5+5} = 0.5\n\\]\nNow, the probability of picking first a malignant tumour, then a benign tumour:\n\\[\nP(\\text{malignant then benign}) = P(\\text{malignant}) \\cdot P(\\text{benign}) = 0.5 \\cdot 0.5 = 0.25\n\\]\nThe probability of picking first a benign tumour, then a malignant tumour:\n\\[\nP(\\text{benign then malignant}) = P(\\text{benign}) \\cdot P(\\text{malignant}) = 0.5 \\cdot 0.5 = 0.25\n\\]\nSum the two to get the probability of picking items of different classes:\n\\[\n\\begin{aligned}\nP(\\text{picking items of different class}) &= P(\\text{malignant then benign}) + P(\\text{benign then malignant}) \\\\\n&= 0.25 + 0.25 = 0.5\n\\end{aligned}\n\\]\nThe Gini Impurity Coefficient for Group D is therefore \\(0.5\\).",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Splits</span>"
    ]
  },
  {
    "objectID": "chapters/recursion.html",
    "href": "chapters/recursion.html",
    "title": "13  Splitting Recursively",
    "section": "",
    "text": "13.1 Functions\nThe Decision Tree learning algorithm is a recursive algorithm.\nRecursion is a fascinating idea that builds on the concept of function. In programming, functions are reusable pieces of code that execute a piece of code\nAs an example, the following function adds two numbers and returns the result:\nThis is a simple program represented in pseudocode, a language between natural language (like English) and code.\nGoing through this function line by line:\nIn python code, this function definition would look like this:\nOnce defined, this function can be called over and over again, without having to copy-paste any code. Functions are a core building block of programming.\nTo call this function in python, you could write adder(2,3) and 5 would be the result.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Splitting Recursively</span>"
    ]
  },
  {
    "objectID": "chapters/recursion.html#functions",
    "href": "chapters/recursion.html#functions",
    "title": "13  Splitting Recursively",
    "section": "",
    "text": "Adder(a, b):\n    result = a + b\n    return result\n\n\n\nWhat is pseudocode?\n\nPseudocode is a language in between natural language and code.\nNatural languages are languages like English, French or Chinese that we use to communicate with one another. It is both:\n\nambiguous: the same word can mean two or more meanings\ncontext-dependent: the meaning of a word depends on its context\n\nCode is an unambiguous and context-free language. We use it to define programs to be run by computers.\nPseudocode is closer to natural language and allows us to communicate programs and ideas.\n\n\n\nDefines a new function called Adder, that takes two input parameters a and b\nSums the two parameters and stores the result in the result variable\nReturns the result variable to the user\n\n\ndef adder(a,b):\n    result = a + b\n    return result",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Splitting Recursively</span>"
    ]
  },
  {
    "objectID": "chapters/recursion.html#recursion",
    "href": "chapters/recursion.html#recursion",
    "title": "13  Splitting Recursively",
    "section": "13.2 Recursion",
    "text": "13.2 Recursion\nIn Computer Science, recursion is when a function calls itself, directly or indirectly, to solve a problem by breaking it down into smaller subproblems.\nThis is the technical definition, this chapter will make this concept more concrete.\nLet’s take the Fibonacci sequence as an example:\n0, 1, 1, 2, 3, 5, 8, …\nIn the Fibonacci sequence, the \\(n^{th}\\) term is the sum of the previous two, with the first two terms of the series being 0 and 1.\nTo calculate the 3rd term, we add the first two numbers together: 0 + 1 = 1 To calculate the 4th, we add the second and the third number: 1 + 1 = 2\nWe can continue this process:\n\n5th: 1 + 2 = 3\n6th: 2 + 3 = 5\n\n\nExercise 13.1 Compute the 7th and 8th term of the Fibonacci sequence.\n\nTo make this process more general, we can compute the \\(n^{th}\\) Fibonacci term, \\(Fibonacci(n)\\), with the following expression:\n\\[\nFibonacci(n) = Fibonacci(n-1) + Fibonacci(n-2)\n\\]\nThis could be confusing, as a function is used in its own definition. Yet, it simply means that each term is the sum of the two previous ones in the series.\nThis expression could also be turned into the following program, which would compute the \\(n^{th}\\) Fibonacci term:\nFibonacci(n):\n    if n == 0: return 0\n    if n == 1: return 1\n    return Fibonacci(n-1) + Fibonacci(n-2)\nIf you have never seen pseudocode before, this may look intimidating. It simply means the following (numbers in the list follow the line number in the code snippet):\n\nDefines a new function called Fibonacci, this function takes one parameter called n\nIf that parameter is 0, the function returns 0\nIf that parameter is 1, the function returns 1\nOtherwise, the function should return the sum of the two previous Fibonacci terms, represented as Fibonacci(n-1) and Fibonacci(n-2)\n\nThe last line is recursion in action: a function calling itself! To get Fibonacci(3) we need to compute Fibonacci(2) and Fibonacci(1). To get Fibonacci(2), we need Fibonacci(1) and Fibonacci(0). This is exactly what recursion is about.\nNote: In Computer Science, lists start at index 0. So the first term is index 0, the second is index 1 etc…\nIf you find this confusing, you are not alone. Let’s compute an example with Fibonacci(3), which will return the 4th term of the Fibonacci series (starting at index 0). The following will run the function line by line:\nFibonacci(3)\n3 is neither 0 nor 1\nCalling Fibonacci(2) and Fibonacci(1)\n    Fibonacci(2)\n    2 is neither 0 nor 1\n        Calling Fibonacci(1) and Fibonacci(0)\n            Fibonacci(1)\n            Return 1\n            Fibonacci(0)\n            Return 0\n    Return 1 + 0 = 1\n    Fibonacci(1)\n    Return 1\nReturn 1 + 1 = 2\nIn the above example, each indentation represents a level of recursion.\n\n13.2.1 Recursive Splitting\nThe splitting logic of a Decision Tree works in a similar way. It keeps splitting subgroups until the stopping condition is met. It could be defined in pseudocode as follows:\nSplit(Group):\n    If the group is “pure” do nothing\n    Find the best split feature and split value\n    Split the group into two: Subgroup A, and Subgroup B\n    Apply Split(SubgroupA) and Split(SubgroupB)\nIn the program defined above, line 2 defines the stopping criterion. In the case that the group contains only one class, the function terminates. Otherwise, it continues recursion with lines 3-5. It keeps splitting the data.\nLet’s apply this logic to the example data step by step:\nSplit(Dataset)  \nGroup contains two classes, proceed to splitting\nBest split is found at x1 = 2\nSplits Dataset into Groups A and B\nCalls Split(Group A) and Split(Group B)\n\n\n\nSplitting the dataset\n\n\n    Split(Group B)  \n    Proceed to splitting as group contains two classes  \n    Best split is found at x2 = 1\n    Splits B into Groups C and D\n    Calls Split(Group C) and Split(Group D)    \n    :::\n\n\n\nSplitting Group B\n\n\n        Split(Group C)\n        Does nothing as Group C contains only one class\n        Split(Group D)\n        Does nothing as Group D contains only one class\n    Split(Group A)  \n    Proceed to splitting as group contains two classes  \n    Best split is found at x2 = 2\n    Splits Group A into Groups E and F\n    Calls Split(Group E) and Split(Group F)\n\n\n\nSplitting Group A\n\n\n        Split(Group E)\n        Does nothing as Group E contains only one class\n        Split(Group F)\n        Does nothing as Group F contains only one class\nAlgorithm terminates\nIn Computer Science, it is not recursion if your head does not hurt thinking about it.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Splitting Recursively</span>"
    ]
  },
  {
    "objectID": "chapters/recursion.html#reviewing-the-decision-tree-learning-algorithm",
    "href": "chapters/recursion.html#reviewing-the-decision-tree-learning-algorithm",
    "title": "13  Splitting Recursively",
    "section": "13.3 Reviewing the Decision Tree Learning Algorithm",
    "text": "13.3 Reviewing the Decision Tree Learning Algorithm\nThe previous chapter has reviewed two foundational concepts of Decision Tree learning:\n\nRecursion: process calling itself\nGini Impurity Coefficient: a way to grade different data splits\n\nThe Decision Tree Learning algorithm can then be summarised as follows:\nSplit(Group)\n    If the group contains only one class or is too small, do nothing\n    Find the best split using the Gini Impurity criterion\n    Split the data into two subgroups\n    Apply the Split function to the two resulting subgroups\nThis function will build a tree, i.e., a collection of splits and leaf nodes, until there is no further way to split the data. That is it!\nTo predict for a new observation, they can be passed down the tree. The resulting prediction will be the label that constitutes the majority of the observations in the leaf.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Splitting Recursively</span>"
    ]
  },
  {
    "objectID": "chapters/recursion.html#final-thoughts",
    "href": "chapters/recursion.html#final-thoughts",
    "title": "13  Splitting Recursively",
    "section": "13.4 Final Thoughts",
    "text": "13.4 Final Thoughts\nThis chapter introduced the foundational concept of recursion. Recursion happens when a function calls itself. In the case of Decision Trees the splitting function first splits the dataset into two groups (see previous chapter), and then calls itself on each of the subgroups. This is done until no further groups can be split.\nThe next chapter will add some nuance and bring the entire Decision Tree algorithm together.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Splitting Recursively</span>"
    ]
  },
  {
    "objectID": "chapters/recursion.html#solutions",
    "href": "chapters/recursion.html#solutions",
    "title": "13  Splitting Recursively",
    "section": "13.5 Solutions",
    "text": "13.5 Solutions\n\nSolution 13.1. Exercise 13.1\n\n7th: \\(3 + 5 = 8\\)\n8th: \\(5 + 8 = 13\\)",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Splitting Recursively</span>"
    ]
  },
  {
    "objectID": "chapters/trees-proba-reg.html",
    "href": "chapters/trees-proba-reg.html",
    "title": "14  Probabilities and Regression with Decision Trees",
    "section": "",
    "text": "14.1 From labels to probabilities\nThe Decision Tree algorithm introduced in this section has two components:\nThe following chapter will explore how to use this algorithm to output probabilities instead of class labels, and extend this to regression with Decision Trees.\nThe Decision Tree algorithm shown above only outputs class labels as prediction (“malignant” or “benign”). In the simple version described above, an observation is assigned a prediction using a majority vote within the leaf. This is similar to the KNN model. How could the Decision Tree algorithm be used to output probabilities?\nLet’s think about this problem using a modified version of the example data:\nIn this example, leaf number 3 and 4 both contain observations from the two classes. Now, imagine that a new observation has \\(x_1=2.1\\) and \\(x_2=0.5\\), what prediction would we output?\nBased on the values of this observation’s feature, it will land in leaf number 3. In this leaf, the large majority of training observations are of class \\(\\times\\) (10 out of 11) with a single \\(\\circ\\) observation.\nUsing a similar approach to what was used in KNN, instead of applying a majority vote within a leaf, we could use the frequency of the class as a predicted probability.\nIn this example, the new observation will have a probability of \\(\\times\\) of:\n\\[\n\\frac{10}{11} \\approx 0.909\n\\]\nThat is it, nothing more complex.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probabilities and Regression with Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-proba-reg.html#from-labels-to-probabilities",
    "href": "chapters/trees-proba-reg.html#from-labels-to-probabilities",
    "title": "14  Probabilities and Regression with Decision Trees",
    "section": "",
    "text": "Non separable example data\n\n\n\n\n\n\n\n\nExercise 14.1 Compute the probability of \\(\\times\\) for an observation with features: \\(x_1 = 3\\) and \\(x_2 = 2\\). Show that it is approximately 0.17",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probabilities and Regression with Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-proba-reg.html#from-classification-to-regression",
    "href": "chapters/trees-proba-reg.html#from-classification-to-regression",
    "title": "14  Probabilities and Regression with Decision Trees",
    "section": "14.2 From classification to regression",
    "text": "14.2 From classification to regression\nSo far, we have focussed on classification problems, in which the objective is to assign a label (such as the malignancy of a tumour) to new observations. How can the same model be applied to a regression problem, i.e., to the prediction of a continuous variable, like the price of a property.\nThe core idea would remain the same: split the data recursively into subgroups to make them as “pure” as possible. This concept of “purity” or “homogeneity” is easy to define in a classification problem. A group that is “pure” is a group that contains a large majority of one class.\nIn regression problems, how could we define this concept of homogeneity? Think about it in terms of property prices. A homogeneous group would be a group that contains properties with similar prices. In such a group, each item would have little difference with the average price.\nNow, how to generate predictions of continuous values from these subgroups? In the classification case, we could just use majority vote or average for probability predictions. In line with what was shown with KNN, the same principle of average can be used for regression problems.\nThe predicted price of each new property would be the average of all the property prices in the same subgroup. This is similar to the logic of probability prediction described earlier.\nA split is good if the average in each leaf has a low error, if the average is an accurate prediction of the observations in this leaf.\nLet’s illustrate this with a simple example.\n\n14.2.0.1 Splitting Data\nImagine we are building a model predicting property prices based on Property Size (\\(m^2\\)) and Distance to Centre (\\(km\\)):\n\n\n\nProperties by Size and Distance coloured by Property Price\n\n\n\n\nFigure Code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate features\nsize = np.random.uniform(50, 150, 30)\ndistance = np.random.uniform(1, 5, 30)\n\n# Generate price with some relationship: higher size increases price, higher distance decreases price\nprice = 200000 + size * 2500 - distance * 15000 + np.random.normal(0, 15000, 30)\n\n# Plot\nplt.figure(figsize=(8,6))\nsc = plt.scatter(size, distance, c=price, cmap='viridis', s=100, edgecolor='k')\nplt.xlabel('Size (m²)', fontsize=14)\nplt.ylabel('Distance to Center (km)', fontsize=14)\nplt.title('Property Prices by Size and Distance', fontsize=16)\ncbar = plt.colorbar(sc)\ncbar.set_label('Price ($)', fontsize=12)\nplt.grid(True)\nplt.savefig(\"images/trees/property_prices.png\")\nplt.show()\n\nWe could take a subset of this data (prices now in k €):\n\n\n\nSize (m²)\nDistance (km)\nPrice (k €)\n\n\n\n\n60\n2.0\n320\n\n\n80\n1.5\n400\n\n\n120\n5.0\n350\n\n\n70\n3.0\n310\n\n\n150\n1.0\n600\n\n\n90\n4.0\n330\n\n\n\nSuppose the first split is on size &lt; 100. This divides the data into two groups:\n\nGroup A sizes (size &lt; 100): 60, 80, 70, 90\nGroup B sizes (size ≥ 100): 120, 150\n\nHow good is this split? To do this, we would compute the average of Group A and B, these will be our prediction for both groups.\nThe prediction for Group A is the average price in Group A:\n\\(\\text{Group A Mean} = \\frac{320 + 400 + 310 + 330}{4} = 340\\)\nThe prediction for Group B is the average price in Group B:\n\\(\\text{Group B Mean} = \\frac{350 + 600}{2} = 475\\)\nWe now need to measure how good of a prediction these averages are. How would you do it?\nIf you thought of the Mean Squared Error, well done! You can refer to the Model Evaluation chapter if this concept is still not clear enough.\nFor each group, we can compute the Mean Squared Error (MSE) of the prices:\n\nGroup A: \\[\n\\begin{aligned}\n\\text{MSE}_a &= \\frac{(320-340)^2 + (400-340)^2 + (310-340)^2 + (330-340)^2}{4} \\\\\n&= \\frac{400 + 3600 + 900 + 100}{4}  \\\\\n&= \\frac{5000}{4} = 1250\n\\end{aligned}\n\\]\nGroup B: \\[\n\\begin{aligned}\n\\text{MSE}_b &= \\frac{(350-475)^2 + (600-475)^2}{2} \\\\\n&= \\frac{(-125)^2 + (125)^2}{2} \\\\\n&= \\frac{15625 + 15625}{2} \\\\\n&= \\frac{31250}{2} = 15625\n\\end{aligned}\n\\]\n\nThis is a good start, but leaves us with two MSE numbers. How could we summarise these into one?\nOne idea is to compute the average MSE. It is a weighted average of the MSEs of the two groups (weighted by the number of observations in each group):\n\\[ \\begin{aligned}\n\\text{Weighted MSE} &= \\frac{n_a}{n_a + n_b} \\cdot \\text{MSE}_a + \\frac{n_b}{n_a + n_b} \\cdot \\text{MSE}_b \\\\\n&=\\frac{4}{6} \\cdot 1250 + \\frac{2}{6} \\cdot 15625 = 833.33 + 5208.33 = 6041.67\n\\end{aligned}\n\\]\n\n\n14.2.1 Trying Different Splits\nAt each splitting step, the algorithm would try different splitting features and values, and would pick the one that minimises average MSE.\nThis process can be visualised by showing the average MSE for each splitting value of the size feature:\n\n\n\nTrying different splitting values of the size feature\n\n\n\n\nFigure Code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nnp.random.seed(42)\nsize = np.random.uniform(50, 150, 30)\ndistance = np.random.uniform(1, 5, 30)\nprice = 200000 + size * 2500 - distance * 15000 + np.random.normal(0, 15000, 30)\nprice = price / 1000  # convert to k€\n\nX = np.column_stack([size, distance])\n\ndef avg_mse(split_value, X, price):\n    left_mask = X[:, 0] &lt; split_value\n    right_mask = ~left_mask\n    n = len(price)\n    n_left = np.sum(left_mask)\n    n_right = np.sum(right_mask)\n    if n_left == 0 or n_right == 0:\n        return np.nan\n    mean_left = price[left_mask].mean()\n    mean_right = price[right_mask].mean()\n    mse_left = np.mean((price[left_mask] - mean_left) ** 2)\n    mse_right = np.mean((price[right_mask] - mean_right) ** 2)\n    avg = (n_left / n) * mse_left + (n_right / n) * mse_right\n    return avg\n\nsplit_values = np.linspace(size.min() + 1, size.max() - 1, 200)\navg_mses = np.array([avg_mse(sv, X, price) for sv in split_values])\nbest_idx = np.nanargmin(avg_mses)\nbest_split = split_values[best_idx]\nbest_mse = avg_mses[best_idx]\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n\n# Top: Data scatter\nsc = ax1.scatter(size, distance, c=price, cmap='viridis', s=120, edgecolor='k')\nax1.set_ylabel('Distance to Center (km)', fontsize=16)\nax1.set_title('Property Prices by Size and Distance', fontsize=18)\nax1.axvline(best_split, color='green', linestyle='--', lw=2, label=f'Split at {best_split:.1f}')\nax1.legend(fontsize=12)\nax1.grid(True)\nax1.tick_params(axis='both', which='major', labelsize=14)\n\n# Bottom: MSE vs Size\nax2.plot(split_values, avg_mses, color='blue', lw=2)\nax2.scatter([best_split], [best_mse], color='red', s=80, zorder=5, label='Minimum MSE')\nax2.axvline(best_split, color='green', linestyle='--', lw=2, label=f'Split at {best_split:.1f}')\nax2.set_xlabel('Size (m²)', fontsize=16)\nax2.set_ylabel('Average MSE', fontsize=16)\nax2.set_title('Average MSE vs. Size Split', fontsize=18)\nax2.grid(True)\nax2.tick_params(axis='both', which='major', labelsize=14)\nax2.legend(fontsize=12, loc='upper right')\n\n# Add a colorbar that doesn't affect axis alignment\nfig.subplots_adjust(right=0.88)\ncbar_ax = fig.add_axes([0.90, 0.38, 0.025, 0.48]) \ncbar = fig.colorbar(sc, cax=cbar_ax)\ncbar.set_label('Price (k€)', fontsize=12)\n\nplt.tight_layout(rect=[0, 0, 0.88, 1])\nplt.savefig(\"images/trees/property_mse_vs_size.png\")\nplt.show()\n\nFollowing the same recursive process as the one shown in the classification case, we can build a Decision Tree that partitions the overall space into subgroups.\nThis stepwise learning process is shown below:\n\n\n\nThe Decision Tree learning algorithm selects the best split at each step\n\n\n\n\nFigure Code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\n\nnp.random.seed(42)\nn = 30\n\n# Generate two size bands, each with a narrow range\nsize = np.concatenate([\n    np.random.uniform(65, 95, n//2),   # left band\n    np.random.uniform(100, 125, n//2)  # right band\n])\n\n# Distance: spread out for both bands\ndistance = np.concatenate([\n    np.random.uniform(1, 5, n//2),\n    np.random.uniform(1, 5, n//2)\n])\n\n# Price: within each band, a strong step at different distance values\nprice = np.empty(n)\n# Left band: step at distance=3\nprice[:n//2] = np.where(distance[:n//2] &lt; 3, 350, 320) + np.random.normal(0, 10, n//2)\n# Right band: step at distance=2.2\nprice[n//2:] = np.where(distance[n//2:] &lt; 2.2, 480, 455) + np.random.normal(0, 10, n//2)\n\nX = np.column_stack([size, distance])\nfeature_names = ['Size (m²)', 'Distance to Center (km)']\n\n# Fit regression tree with depth=2\ntree = DecisionTreeRegressor(max_depth=2, random_state=0)\ntree.fit(X, price)\n\n# Helper: collect splits recursively\ndef collect_splits(tree, node_id=0, path=[], splits=[]):\n    feature = tree.tree_.feature[node_id]\n    threshold = tree.tree_.threshold[node_id]\n    if feature &gt;= 0:\n        splits.append((path.copy(), feature, threshold))\n        collect_splits(tree, tree.tree_.children_left[node_id], path + [(feature, threshold, 'left')], splits)\n        collect_splits(tree, tree.tree_.children_right[node_id], path + [(feature, threshold, 'right')], splits)\n    return splits\n\nsplits = collect_splits(tree, 0, [], [])\n\n# Print splits for verification\nfor i, (path, feat, thresh) in enumerate(splits):\n    print(f\"Split {i+1}: feature {feat} ({feature_names[feat]}), threshold {thresh:.2f}\")\n\n# Plot (reuse your plotting code here)\nx0min, x0max = size.min() - 5, size.max() + 5\nx1min, x1max = distance.min() - 0.5, distance.max() + 0.5\n\nn_splits = len(splits)\nfig, axes = plt.subplots(1, n_splits, figsize=(6*n_splits, 6), sharex=True, sharey=True)\n\nif n_splits == 1:\n    axes = [axes]\n\nfor i in range(n_splits):\n    ax = axes[i]\n    sc = ax.scatter(size, distance, c=price, cmap='viridis', s=100, edgecolor='k')\n    # Draw all previous splits as black lines, in their respective regions\n    for j in range(i):\n        path, feat, thresh = splits[j]\n        xlims = [x0min, x0max]\n        ylims = [x1min, x1max]\n        for (pf, pt, dirn) in path:\n            if pf == 0:\n                if dirn == 'left':\n                    xlims[1] = min(xlims[1], pt)\n                else:\n                    xlims[0] = max(xlims[0], pt)\n            else:\n                if dirn == 'left':\n                    ylims[1] = min(ylims[1], pt)\n                else:\n                    ylims[0] = max(ylims[0], pt)\n        if feat == 0:\n            ax.plot([thresh, thresh], ylims, color='black', linewidth=2)\n        else:\n            ax.plot(xlims, [thresh, thresh], color='black', linewidth=2)\n\n    # Draw current split as green dashed line, in its region\n    path, feat, thresh = splits[i]\n    xlims = [x0min, x0max]\n    ylims = [x1min, x1max]\n    for (pf, pt, dirn) in path:\n        if pf == 0:\n            if dirn == 'left':\n                xlims[1] = min(xlims[1], pt)\n            else:\n                xlims[0] = max(xlims[0], pt)\n        else:\n            if dirn == 'left':\n                ylims[1] = min(ylims[1], pt)\n            else:\n                ylims[0] = max(ylims[0], pt)\n    if feat == 0:\n        ax.plot([thresh, thresh], ylims, color='green', linestyle='--', linewidth=2,\n                label=f'Split {i+1}: {feature_names[feat]} = {thresh:.1f}')\n    else:\n        ax.plot(xlims, [thresh, thresh], color='green', linestyle='--', linewidth=2,\n                label=f'Split {i+1}: {feature_names[feat]} = {thresh:.2f}')\n    ax.set_xlabel('Size (m²)', fontsize=16)\n    if i == 0:\n        ax.set_ylabel('Distance to Center (km)', fontsize=16)\n    ax.set_title(f'Split {i+1}', fontsize=18)\n    ax.grid(True)\n    ax.tick_params(axis='both', which='major', labelsize=14)\n    ax.legend(fontsize=12, loc='upper left')\n\n# Add colorbar\nfig.subplots_adjust(right=0.92)\ncbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.7])\ncbar = fig.colorbar(sc, cax=cbar_ax)\ncbar.set_label('Price (k€)', fontsize=14)\n\nplt.tight_layout(rect=[0, 0, 0.92, 1])\nplt.savefig(\"images/trees/property_regression_splits_stepwise.png\")\nplt.show()\nAs described in this section, Decision Trees can be easily applied to regression tasks. The only significant change is the evaluation of split quality with the Mean Squared Error instead of the Gini Impurity Coefficient.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probabilities and Regression with Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-proba-reg.html#final-thoughts",
    "href": "chapters/trees-proba-reg.html#final-thoughts",
    "title": "14  Probabilities and Regression with Decision Trees",
    "section": "14.3 Final Thoughts",
    "text": "14.3 Final Thoughts\nThis chapter concludes our exploration of Decision Trees. Summarising once more the Decision Tree Learning algorithm:\n\nEvaluate splits using a homogeneity criterion: Gini or MSE\nSplit groups recursively\nOutput either class labels or probabilities using averaging\n\nTo test your knowledge, you can try the practice exercise below.\nLooking back, we now know how to train and evaluate two different Machine Learning models. This is already a lot. The next section will explore the last missing piece of the puzzle: Data Preprocessing, making data ready for modelling.",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probabilities and Regression with Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-proba-reg.html#practice-exercise",
    "href": "chapters/trees-proba-reg.html#practice-exercise",
    "title": "14  Probabilities and Regression with Decision Trees",
    "section": "14.4 Practice Exercise",
    "text": "14.4 Practice Exercise\n\nExercise 14.2 Suppose you are building a Decision Tree to detect fraudulent transactions. You use two features, both measured on a 0–100 scale:\n\nTransaction Amount ($) (0–100)\nCustomer Age (years) (0–100)\n\nYou have the following 10 transactions in your training data:\n\n\n\nTransaction Amount\nCustomer Age\nFraudulent?\n\n\n\n\n95\n22\nYes\n\n\n90\n25\nYes\n\n\n92\n23\nYes\n\n\n97\n21\nYes\n\n\n93\n24\nYes\n\n\n94\n23\nNo\n\n\n20\n80\nNo\n\n\n25\n78\nNo\n\n\n18\n82\nNo\n\n\n23\n77\nNo\n\n\n\nA new transaction occurs with an amount of 93 and customer age 23.\n\nBuild a Decision Tree with a single split (for simplicity) using the training data and finding the splits that minimise the Gini Impurity Coefficient?\nUsing the tree generated, what is the predicted probability of fraud of the new observation?\n\nSolution 14.2",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probabilities and Regression with Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/trees-proba-reg.html#solutions",
    "href": "chapters/trees-proba-reg.html#solutions",
    "title": "14  Probabilities and Regression with Decision Trees",
    "section": "14.5 Solutions",
    "text": "14.5 Solutions\n\nSolution 14.1. Exercise 14.1\n\\(\\frac{2}{12} = \\frac{1}{6} \\approx = 0.17\\)\n\n\nSolution 14.2. Exercise 14.2\n\nWe try splitting the data using different features and feature values. For each split, we compute the weighted Gini Impurity Coefficient. The resulting coefficients can be seen in the two tables below. For each table, the symbols \\(\\leq\\) and \\(&gt;\\) represent the two resulting subgroups:\n\n\n\\(\\leq\\): observations with feature value lower or equal to the splitting value\n\\(&gt;\\): observations with feature value greater than the splitting value\n\nSplits for Transaction Amount\n\n\n\n\n\n\n\n\n\n\n\nSplit at\n≤ Split Fraud\n≤ Split Non-Fraud\n&gt; Split Fraud\n&gt; Split Non-Fraud\nWeighted Gini\n\n\n\n\n19.00\n0\n1\n5\n4\n0.444\n\n\n21.50\n0\n2\n5\n3\n0.375\n\n\n24.00\n0\n3\n5\n2\n0.286\n\n\n57.50\n0\n4\n5\n1\n0.167\n\n\n91.00\n1\n4\n4\n1\n0.320\n\n\n92.50\n2\n4\n3\n1\n0.417\n\n\n93.50\n3\n4\n2\n1\n0.476\n\n\n94.50\n3\n5\n2\n0\n0.375\n\n\n96.00\n4\n5\n1\n0\n0.444\n\n\n\nSplits for Customer Age\n\n\n\n\n\n\n\n\n\n\n\nSplit at\n≤ Split Fraud\n≤ Split Non-Fraud\n&gt; Split Fraud\n&gt; Split Non-Fraud\nWeighted Gini\n\n\n\n\n21.50\n1\n0\n4\n5\n0.444\n\n\n22.50\n2\n0\n3\n5\n0.375\n\n\n23.00\n3\n1\n2\n4\n0.417\n\n\n23.50\n3\n1\n2\n4\n0.417\n\n\n24.50\n4\n1\n1\n4\n0.320\n\n\n51.00\n5\n1\n0\n4\n0.167\n\n\n77.50\n5\n2\n0\n3\n0.286\n\n\n79.00\n5\n3\n0\n2\n0.375\n\n\n81.00\n5\n4\n0\n1\n0.444\n\n\n\nBoth Transaction Amount of 57.5 and Customer Age of 51 achieve a Gini Coefficient of 0.167. Both of them would work. This solution will pick Customer Age = 51 as splitting value.\nThe resulting tree is:\n\n\n\nResulting tree splitting on Customer Age ≤ 51\n\n\n\nGenerating the prediction for observation: Customer Age: 23 and Transaction Amount: 93, the observation will fall into the left leaf, as \\(23 \\leq 51\\).\n\nThe predicted probability for this observation is then the average probability of Fraud in the leaf:\n\\[\nP(\\text{Fraud}) = \\frac{5}{5+1} \\approx 83\\%\n\\]",
    "crumbs": [
      "Decision Trees",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probabilities and Regression with Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/data-preprocessing.html",
    "href": "chapters/data-preprocessing.html",
    "title": "15  Data Preprocessing",
    "section": "",
    "text": "Now that we have a good idea of what prediction with Machine Learning looks like, let’s start implementing these methods to real-world examples.\nCan you, today, take data on something you want to predict and apply a KNN or a Decision Tree algorithm? Most likely not, because we are still missing a step.\nThis chapter will bridge the gap between the big ideas described in the previous chapters and real-world ML applications: data preprocessing.\nYou may have noticed that all the data used in this book was synthetic, i.e., generated by a quick Python script. There are several reasons for this:\n\nEnsuring that the data has the required properties\nCopyrights, legal and ethical issues\nConvenience\n\nThis generated data had some characteristics you may have noted:\n\nOnly numeric features, no categorical or date fields\nFeatures on the same scale (most of the time)\nNo missing data\n\nIn other words, in all of the examples, the data was represented as a clean table of data with numbers on the same scale.\nThis will not be the case for most of the datasets you will come across. It is a messy world out there. Some data will be missing, some data will not even be numbers.\nThe following chapters will explore ways to deal with these issues one by one. This last part of the book will allow you to apply Machine Learning models to any regression or classification problem you come across.\nNote: Preprocessing is a common source of information leakage between the training set and the test set. If the idea of train/test split is not clear, I would recommend reading through the Model Evaluation chapter once more. The concept of information leakage will be explored further in this section.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-variables.html",
    "href": "chapters/categorical-variables.html",
    "title": "16  Encoding Categorical Features",
    "section": "",
    "text": "16.1 Types of Categorical\nNot all data is numerical. Categorical features represent data groupings. As an example, a property in Berlin can have a given neighbourhood, e.g., “Neukölln”, “Kreuzberg” or “Charlottenburg”.\nAll of the Machine Learning models studied so far deal with observations as points in space. Numerical features naturally determine coordinates of points in space. Properties can be plotted by their surface area and distance to the centre of town without problems:\nWe could try building a model that ignores neighbourhood, and uses “Distance to the Centre” as a proxy. This is not a bad idea, but a lot of information is lost in the process. And what is the centre of Berlin anyway?\nHow would models handle these categorical variables like neighbourhoods? This is what this chapter is about.\nThere are two main types of categorical features:",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Encoding Categorical Features</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-variables.html#types-of-categorical",
    "href": "chapters/categorical-variables.html#types-of-categorical",
    "title": "16  Encoding Categorical Features",
    "section": "",
    "text": "Ordinal variables: these variables have an order. The energy efficiency rating of a property ranges from A (most efficient) to G (least efficient)\nNominal variables: these variables have no intrinsic order. No possible ranking can be made of the neighbourhoods (e.g., “Neukölln” or “Kreuzberg”) mentioned in the previous example",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Encoding Categorical Features</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-variables.html#ordinal-variables",
    "href": "chapters/categorical-variables.html#ordinal-variables",
    "title": "16  Encoding Categorical Features",
    "section": "16.2 Ordinal Variables",
    "text": "16.2 Ordinal Variables\nOrdinal variables can be converted to numbers relatively easily. As an example, you could map the energy rating from A to G to the numbers 1 to 7. This would create another dimension in the feature space of properties:\n\n\n\n\n\n\n\n\n\n\nProperty\nSurface Area (sq m)\nDistance to Centre (km)\nEnergy Rating\nEncoded Energy Rating\n\n\n\n\nA\n85\n4.2\nB\n2\n\n\nB\n120\n2.5\nA\n1\n\n\nC\n95\n6.1\nC\n3\n\n\nD\n70\n3.8\nD\n4\n\n\nE\n110\n1.2\nB\n2\n\n\nF\n60\n5.0\nF\n6\n\n\n\nBelow is a plot of the properties in three dimensions: surface area, distance to centre, and encoded energy class.\n\n\n\nProperties in 3D: Surface Area, Distance to Centre, Encoded Energy Class\n\n\n\n\nFigure code\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\nsurface_area = [85, 120, 95, 70, 110, 60]\ndistance_centre = [4.2, 2.5, 6.1, 3.8, 1.2, 5.0]\nenergy_rating = ['B', 'A', 'C', 'D', 'B', 'F']\nencoded_energy = [2, 1, 3, 4, 2, 6]\nlabels = ['A', 'B', 'C', 'D', 'E', 'F']\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(surface_area, distance_centre, encoded_energy, s=120, c='royalblue')\nfor i, label in enumerate(labels):\n    ax.text(surface_area[i]+1, distance_centre[i], encoded_energy[i], label, fontsize=16)\nax.set_xlabel('Surface Area (sq m)', fontsize=16)\nax.set_ylabel('Distance to Centre (km)', fontsize=16)\nax.set_zlabel('Encoded Energy Class', fontsize=16)\nax.set_title('Properties in 3D: Surface, Distance, Energy', fontsize=18)\nax.tick_params(axis='both', labelsize=14)\nplt.tight_layout()\nplt.savefig('figures/3d_surface_distance_energy.png')\nplt.show()\n\nIf the notion of space is not fully clear, refer to the chapter on data space.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Encoding Categorical Features</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-variables.html#nominal-variables",
    "href": "chapters/categorical-variables.html#nominal-variables",
    "title": "16  Encoding Categorical Features",
    "section": "16.3 Nominal Variables",
    "text": "16.3 Nominal Variables\nHandling nominal variables is trickier, as a direct conversion to numbers would not make sense. Why would “Neukölln” be 1 and “Kreuzberg” be 2? Or the other way around? We need a smarter solution. The following sections will describe two of them.\n\n16.3.1 One-Hot Encoding\nThe best way to understand One-Hot Encoding is to visualise it before explaining it. Imagine the following dataset:\n\n\n\nProperty\nNeighbourhood\nSurface Area (sq m)\nSell Price (K€)\n\n\n\n\nA\nNeukölln\n85\n420\n\n\nB\nKreuzberg\n120\n610\n\n\nC\nCharlottenburg\n95\n390\n\n\nD\nKreuzberg\n70\n370\n\n\nE\nCharlottenburg\n110\n700\n\n\nF\nNeukölln\n60\n310\n\n\n\nThe One-Hot Encoded dataset would look like this:\n\n\n\n\n\n\n\n\n\n\n\nProperty\nSurface Area (sq m)\nSell Price (K€)\nKreuzberg\nCharlottenburg\nNeukölln\n\n\n\n\nA\n85\n420\n0\n0\n1\n\n\nB\n120\n610\n1\n0\n0\n\n\nC\n95\n390\n0\n1\n0\n\n\nD\n70\n370\n1\n0\n0\n\n\nE\n110\n700\n0\n1\n0\n\n\nF\n60\n310\n0\n0\n1\n\n\n\nWhat happened there? The original neighbourhood column disappeared, and was replaced by three columns containing either 1 or 0.\nOne-Hot Encoding converts a categorical variable into a list of binary columns, indicating whether or not the property belongs to a category. The advantage of this method is that it assumes no order.\nA drawback of this approach is that it can create many columns. Imagine that you want to apply the same method to the street name. You may end up with thousands of columns. For mathematical reasons we will not go into, this can be an issue for Machine Learning algorithms. This is sometimes referred to as the curse of dimensionality. If you want to develop a quick intuition of why, just remember that a lot of strange things happen in a space with many dimensions.\n\n\nOne-Hot Encoding in practice\n\nIn practice, One-Hot Encoding converts a categorical variable with \\(N\\) distinct values into \\(N-1\\) binary columns. Why do we remove one?\nIn statistics and Machine Learning, many issues arise when the features of a dataset are perfectly correlated. To avoid this, One-Hot Encoding drops one of the binary columns.\nGoing back to the example with three neighbourhood values (Kreuzberg, Charlottenburg and Neukölln), the processed dataset would look like this:\n\n\n\n\n\n\n\n\n\n\nProperty\nSurface Area (sq m)\nSell Price (K€)\nKreuzberg\nCharlottenburg\n\n\n\n\nA\n85\n420\n0\n0\n\n\nB\n120\n610\n1\n0\n\n\nC\n95\n390\n0\n1\n\n\nD\n70\n370\n1\n0\n\n\nE\n110\n700\n0\n1\n\n\nF\n60\n310\n0\n0\n\n\n\nHaving both Kreuzberg and Charlottenburg as 0 would mean that the property is in Neukölln.\n\nOne-Hot Encoding solves the categorical variable problem for variables with few categories. How could we encode categorical variables like street name? Ignoring these variables with many distinct values is one possibility. However, in the example of property pricing, the street name could carry some relevant information. The next section will explore one idea.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Encoding Categorical Features</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-variables.html#target-encoding",
    "href": "chapters/categorical-variables.html#target-encoding",
    "title": "16  Encoding Categorical Features",
    "section": "16.4 Target Encoding",
    "text": "16.4 Target Encoding\nThinking about a property pricing model, how could we represent the street name of a property as a number?\nWe want to be able to capture the impact of the street name on the price. To do so, we could encode each value of the categorical variable (e.g., “Sonnenallee” or “Bergmannstr.”) as the average of the target variable for the observations in this category. In this case, each street name would be replaced by the average price on that street.\nThis is an abstract definition, let’s make it more concrete with an example:\n\n\n\n\n\n\n\n\n\nProperty\nSurface Area (sq m)\nStreet Name\nSell Price (K€)\n\n\n\n\nA\n85\nSonnenallee\n420\n\n\nB\n120\nBergmannstr.\n610\n\n\nC\n95\nKantstr.\n390\n\n\nD\n70\nBergmannstr.\n370\n\n\nE\n110\nUnter Den Linden\n700\n\n\nF\n60\nSonnenallee\n310\n\n\n\nTo encode the street name with Target Encoding, compute the average value of the target for each street name:\n\nBergmannstr.: \\(\\frac{610 + 370}{2} = 490\\)\nUnter Den Linden: \\(700\\) (only one property)\n\n\nExercise 16.2 Show that the average target value for Sonnenallee is 365.\n\nThe dataset with a target encoded street name feature now looks like this:\n\n\n\n\n\n\n\n\n\n\nProperty\nSurface Area (sq m)\nStreet Name\nSell Price (K€)\nEncoded Street Name\n\n\n\n\nA\n85\nSonnenallee\n420\n365\n\n\nB\n120\nBergmannstr.\n610\n490\n\n\nC\n95\nKantstr.\n390\n390\n\n\nD\n70\nBergmannstr.\n370\n490\n\n\nE\n110\nUnter Den Linden\n700\n700\n\n\nF\n60\nSonnenallee\n310\n365\n\n\n\nThis way, a lot of the price relevant information of the street name is preserved. The main advantage of this method is that it can deal with categorical variables with many values without creating too many dimensions.\nA possible drawback of this method is that if there are street names for which there is only one or a few properties, the average would only be the price of the single property on this street. There are smart ways to deal with these cases, though they are beyond the scope of this book.\n\n\nIf you are curious\n\nOne way to avoid the problem described above, you can decide on a threshold (e.g., 10), and encode any street name with fewer observations than 10 with the global average instead of the average for this street name.\n\nThis averaging process can also be used in the binary classification context. The average of the target variable for a given categorical value could be the average of 0’s and 1’s of the target label (i.e., negatives and positives).",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Encoding Categorical Features</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-variables.html#information-leakage",
    "href": "chapters/categorical-variables.html#information-leakage",
    "title": "16  Encoding Categorical Features",
    "section": "16.5 Information Leakage",
    "text": "16.5 Information Leakage\nAs mentioned in the introductory section, categorical encoding can be a source of information leakage between the training and test set. To avoid this leakage, it is critical to compute all these transformations on the training set only.\nIn One-Hot Encoding, only create columns for the categorical values seen in the training set. If a new value is found in the test set, it could be either excluded or flagged as missing (with another binary column). Using the entire dataset to encode categorical features would not simulate real prediction conditions.\nFor Target Encoding, the average target value for each category should only be computed on the training data. These averages can then be applied to the test set. If a new categorical value is found in the test set, it can be excluded or encoded with the global average of the target variable (over the entire dataset).\nWhy worry about this? When the model is used for prediction, the goal of splitting the data between train and test set is to estimate the performance of the model on unseen data. To do so, it is critical to compute preprocessing transformations on the training data. In the future, data will come one or more row at a time. The only averages available will be from the training data.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Encoding Categorical Features</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-variables.html#final-thoughts",
    "href": "chapters/categorical-variables.html#final-thoughts",
    "title": "16  Encoding Categorical Features",
    "section": "16.6 Final Thoughts",
    "text": "16.6 Final Thoughts\nWithout the appropriate encoding, categorical variables cannot be handled by Machine Learning models. ML models learn the relationships between inputs and a target variable using mathematical tricks applicable to lists of numbers (distance or splitting of space).\nEncoding categorical variables converts them to numbers that can then be used in the training and prediction process. The encoding methods in this chapter include:\n\nOrdinal Encoding: mapping categorical values with an intrinsic order to a number\nOne-Hot Encoding: representing categorical variables to a list of binary columns\nTarget Encoding: mapping categorical values to the average target value for this group\n\nThe next chapter will explore another data type: dates and times.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Encoding Categorical Features</span>"
    ]
  },
  {
    "objectID": "chapters/categorical-variables.html#solutions",
    "href": "chapters/categorical-variables.html#solutions",
    "title": "16  Encoding Categorical Features",
    "section": "16.7 Solutions",
    "text": "16.7 Solutions\n\nSolution 16.1. Exercise 16.1 The other two categorical variables are:\n\nStreet Name\nEnergy Rating\n\n\n\nSolution 16.2. Exercise 16.2\n\\(\\frac{420 + 310}{2} = 365\\)",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Encoding Categorical Features</span>"
    ]
  },
  {
    "objectID": "chapters/datetime.html",
    "href": "chapters/datetime.html",
    "title": "17  Representing Dates",
    "section": "",
    "text": "17.1 Dates and Prediction\nThe world is in a constant state of change. Time is the concept we have created to reason about the wild sequence of events that we order in past, present and future. At our human scale, this time is represented using standard units such as: year, month, day, hour, minute and second.\nI am writing these lines on Friday the 15th of August 2025 at 08:31 AM. Using the ISO 8601 standard, this point in time can be represented as: 2025-08-15T08:31:00. The following can be extended to also include my time zone: 2025-08-15T08:31:00+02:00, the +02:00 at the end.\nTime indications like dates can contain information that can improve the quality of Machine Learning model predictions.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Representing Dates</span>"
    ]
  },
  {
    "objectID": "chapters/datetime.html#dates-and-prediction",
    "href": "chapters/datetime.html#dates-and-prediction",
    "title": "17  Representing Dates",
    "section": "",
    "text": "17.1.1 Overall Trend\nKeeping the property pricing example, in which a Machine Learning model is trained to predict property prices from its characteristics like their surface area or number of rooms. Let’s use the following training data as an example:\n\n\n\n\n\n\n\n\n\n\n\nProperty\nSurface (m²)\nRooms\nPricing date\nSale date\nSale price (K€)\n\n\n\n\nA\n85\n3\n2024-12-15\n2025-02-13\n302\n\n\nB\n86\n3\n2025-01-15\n2025-03-16\n305\n\n\nC\n84\n3\n2025-02-15\n2025-04-16\n303\n\n\nD\n87\n3\n2025-03-15\n2025-05-14\n311\n\n\nE\n85\n3\n2025-04-15\n2025-06-14\n309\n\n\nF\n86\n3\n2025-05-15\n2025-07-14\n316\n\n\nG\n84\n3\n2025-06-15\n2025-08-14\n314\n\n\nH\n87\n3\n2025-07-15\n2025-09-13\n322\n\n\n\nWhich date column would you use to predict the sale price and why? The selling date could seem like a good idea. However, when pricing properties that have not been sold yet, this information is not available to the model. We do not yet know the sale date. The only date available to the pricing model would be the pricing date, date at which the property is put on the market.\nFor this reason, the sale date cannot be used in the model. The pricing date can still contain useful price information. Plotting the average sale price by pricing date (from the example data), we see the following trend:\n\n\n\nSale price by pricing date\n\n\n\n\nFigure code\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\n\ndata = [\n    (\"A\", 85, 3, \"2024-12-15\", \"2025-02-13\", 302000),\n    (\"B\", 86, 3, \"2025-01-15\", \"2025-03-16\", 305000),\n    (\"C\", 84, 3, \"2025-02-15\", \"2025-04-16\", 303000),\n    (\"D\", 87, 3, \"2025-03-15\", \"2025-05-14\", 311000),\n    (\"E\", 85, 3, \"2025-04-15\", \"2025-06-14\", 309000),\n    (\"F\", 86, 3, \"2025-05-15\", \"2025-07-14\", 316000),\n    (\"G\", 84, 3, \"2025-06-15\", \"2025-08-14\", 314000),\n    (\"H\", 87, 3, \"2025-07-15\", \"2025-09-13\", 322000),\n]\ndf = pd.DataFrame(data, columns=[\"Property\",\"Surface\",\"Rooms\",\"Pricing date\",\"Sale date\",\"Sale price\"])\ndf[\"Pricing date\"] = pd.to_datetime(df[\"Pricing date\"])\n\nfig, ax = plt.subplots(figsize=(7,4))\nax.plot(df[\"Pricing date\"], df[\"Sale price\"], marker=\"o\", linestyle=\"-\", color=\"tab:blue\")\nax.set_title(\"Sale price by pricing date\", fontsize=18)\nax.set_xlabel(\"Pricing date\", fontsize=16)\nax.set_ylabel(\"Sale price (£)\", fontsize=16)\nax.tick_params(axis=\"both\", labelsize=14)\nax.grid(True, alpha=0.2)\nax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\nfig.autofmt_xdate()\nplt.tight_layout()\nplt.show()\n\n\n\n17.1.2 Seasonality\nChanging example, let’s say that as the manager of a pub, you are trying to predict the quantity of beer the bar will sell next week. Having an accurate beer sales forecast could help you better plan your staff and inventory. If you predict to sell a lot of beer, you should buy a lot of inventory and hire a larger team.\nBeer sales over the last two years look like this:\n\n\n\nBeer sales over the last two years with seasonality and trend\n\n\n\n\nFigure code\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\ndates = pd.date_range(end=\"2025-08-15\", periods=104, freq=\"W-SUN\")\nt = np.arange(len(dates))\n\nbaseline = 800\ntrend = 5 * t\nseasonality = 150 * np.sin(2 * np.pi * t / 52)\nnoise = np.random.normal(0, 40, size=len(dates))\n\nsales = baseline + trend + seasonality + noise\nsales = np.clip(sales, a_min=0, a_max=None)\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.plot(dates, sales, color=\"tab:green\", linewidth=2)\nax.set_title(\"Weekly beer sales (last two years)\", fontsize=18)\nax.set_xlabel(\"Week\", fontsize=16)\nax.set_ylabel(\"Pints sold\", fontsize=16)\nax.tick_params(axis=\"both\", labelsize=14)\nax.grid(True, alpha=0.2)\nfig.autofmt_xdate()\nplt.tight_layout()\nplt.show()\n\nFrom this chart, we could see both an upward trend and seasonality effects. The next section will investigate how to encode this information into numbers.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Representing Dates</span>"
    ]
  },
  {
    "objectID": "chapters/datetime.html#encoding-dates-as-numbers",
    "href": "chapters/datetime.html#encoding-dates-as-numbers",
    "title": "17  Representing Dates",
    "section": "17.2 Encoding Dates as Numbers",
    "text": "17.2 Encoding Dates as Numbers\nAs described in previous chapters, Machine Learning models generate predictions by learning the relationships between inputs and outputs. To do so, they represent each observation as a list of numbers, and use mathematical tricks (like distance or space splitting) to map features to a target variable.\nTo make dates readable to a model, they need to be encoded as numbers. Take a moment to think about how you would represent 2025-08-15T08:31:00 as numbers before reading on.\n\n17.2.1 Splitting Dates into Parts\nThe date 2025-08-15T08:31:00 can be split into the following parts:\nYear: 2025\nMonth: August, or 8\nWeek number: 33\nDay: 15\nDay of week: Friday, or 5 given a start at 1 on Monday\nHour: 8\nMinute: 31\nSecond: 00\nThis date could then be represented as the following list of numbers: \\({2025, 8, 33, 15, 5, 8, 31, 00}\\). These values could be used as normal numeric features.\nEncoding the pricing date of the example data using this method, we get:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPricing date\nYear\nMonth\nISO week\nDay\nDay of week (Mon=1)\nHour\nMinute\nSecond\n\n\n\n\n2024-12-15 09:00:00\n2024\n12\n50\n15\n7\n9\n0\n0\n\n\n2025-01-15 09:00:00\n2025\n1\n3\n15\n3\n9\n0\n0\n\n\n2025-02-15 09:00:00\n2025\n2\n7\n15\n6\n9\n0\n0\n\n\n2025-03-15 09:00:00\n2025\n3\n11\n15\n6\n9\n0\n0\n\n\n2025-04-15 09:00:00\n2025\n4\n16\n15\n2\n9\n0\n0\n\n\n2025-05-15 09:00:00\n2025\n5\n20\n15\n4\n9\n0\n0\n\n\n2025-06-15 09:00:00\n2025\n6\n24\n15\n7\n9\n0\n0\n\n\n2025-07-15 09:00:00\n2025\n7\n29\n15\n2\n9\n0\n0\n\n\n\nThat should already achieve good performance. You may notice that some date features may be more relevant than others depending on the task.\nAs an example, when predicting ice cream sales, the month of year, week number and day of week (e.g., Friday, Saturday, Sunday) could be important. Whereas the day number (e.g., 1 or 15) may not be such a significant feature. When pricing properties, the year and month of year could also be very important while the day number is practically irrelevant.\n\n\n17.2.2 Representing Dates on Computers\nThe date splitting described above is a very good way to extract relevant information from a date. But is this the only way to represent dates as numbers?\nMost computers use the number of seconds since a predefined point in time, also called the “epoch”. For systems based on UNIX, this starting point is the 1st of January 1970. The UNIX timestamp for 2025-08-15T08:31:00 is the number of seconds since the 1st of January 1970: \\(1755239460\\).\nThe more you know. Spreadsheet systems like Excel and Google Sheets store dates as the number of days since an epoch, and the time of day as the decimal part of a number between 0 and 1. This way, a datetime can be stored as a decimal number. Using Google Sheets to store the example date, we get: \\(45 884.35\\). With \\(45884\\) being the number of days since the 1st of January 1900 and \\(.35\\) representing 08:31, very close to a third of the 24-hour day.\nWhy go through the trouble of learning about this alternative representation? It turns out that representing time as a continuous number like the number of days since a given date can help model trends.\nGoing back to the example of property prices over time, imagine that the average price evolves in the following way:\n\n\n\nUpward linear trend\n\n\n\n\nFigure code\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\n\nnp.random.seed(7)\nn = 36\ndates = pd.date_range(start=\"2024-01-01\", periods=n, freq=\"MS\")\nt = np.arange(n)\nbaseline = 300000\ntrend = 1200 * t \nnoise = np.cumsum(np.random.normal(0, 2000, size=n))\ny = baseline + trend + noise\n\nfig, ax = plt.subplots(figsize=(7,4))\nax.plot(dates, y, color=\"tab:orange\", linewidth=2)\nax.set_title(\"Upward Trend in Property Prices\", fontsize=18)\nax.set_xlabel(\"Date\", fontsize=16)\nax.set_ylabel(\"Average price (€)\", fontsize=16)\nax.tick_params(axis=\"both\", labelsize=14)\nax.grid(True, alpha=0.2)\nax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\nfig.autofmt_xdate()\nplt.tight_layout()\nplt.show()\n\nHaving a feature that represents a point in time, can help determine the price of a property taking this upward trend into account. For example, a model could learn that the average price of property increases by 1% every 130 days.\nReplacing the date features by their UNIX timestamp, we get the following table:\n\n\n\nProperty\nPricing date (UTC 09:00)\nUNIX timestamp\n\n\n\n\nA\n2024-12-15 09:00:00\n1734253200\n\n\nB\n2025-01-15 09:00:00\n1736931600\n\n\nC\n2025-02-15 09:00:00\n1739610000\n\n\nD\n2025-03-15 09:00:00\n1742029200\n\n\nE\n2025-04-15 09:00:00\n1744707600\n\n\nF\n2025-05-15 09:00:00\n1747386000\n\n\nG\n2025-06-15 09:00:00\n1749978000\n\n\nH\n2025-07-15 09:00:00\n1752570000",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Representing Dates</span>"
    ]
  },
  {
    "objectID": "chapters/datetime.html#final-thoughts",
    "href": "chapters/datetime.html#final-thoughts",
    "title": "17  Representing Dates",
    "section": "17.3 Final Thoughts",
    "text": "17.3 Final Thoughts\nIt was about time to end this section. We covered two different ways to encode date features for a Machine Learning model:\n\nSplitting dates into their parts\nRepresent dates as number of seconds or days since a given epoch\n\nThere is no optimal way to encode date features, it always depends on the problem at hand. For what you are trying to predict, how does time enter the picture?\nThat is all on alternative data types. Now that we can convert any data type to numbers, the next two chapters will explore scenarios in which data is missing.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Representing Dates</span>"
    ]
  },
  {
    "objectID": "chapters/missing-numerical-values.html",
    "href": "chapters/missing-numerical-values.html",
    "title": "18  Dealing with Missing Numeric Data",
    "section": "",
    "text": "18.1 Are All Missing Values Equal?\nData can sometimes be missing. These missing points are often represented as null values.\nThis is problematic for most Machine Learning models, as they cannot handle missing values.\nNote: many modern implementations of Machine Learning models, like LightGBM, do natively handle missing values using some of the methods shown in this chapter. This is due to smart implementation; the underlying models still cannot read missing values.\nAs shown in previous chapters, these Machine Learning models learn the relationship between inputs and outputs:\n\\[\n\\text{Input} \\longrightarrow \\text{Model} \\longrightarrow \\text{Prediction}\n\\]\nThey use mathematical tricks (e.g., distance, data splitting) to learn the relationship between the features of each observation (e.g., property surface area) and the target variable (e.g., sell price).\nThe following chapter will explore different strategies to deal with missing values in a Machine Learning project.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dealing with Missing Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-numerical-values.html#are-all-missing-values-equal",
    "href": "chapters/missing-numerical-values.html#are-all-missing-values-equal",
    "title": "18  Dealing with Missing Numeric Data",
    "section": "",
    "text": "18.1.1 Starting with Why\nWhy is this data point missing? There could be many reasons which may inform our approach to missing data.\nWith data problems, it is a good idea to start with the source. Is there an issue in the data collection process? Could there be a problem with the database? How many observations is this affecting?\nBefore developing a strategy, it is critical to understand if there is an issue with the data source. Data source problems could affect many other applications, such as reporting or operations.\nMissing values can have different meanings:\n\nMissing at Random: the observation is missing for an unknown reason, without any apparent pattern\nNone: a missing transaction count could mean that no (or 0) transactions were made\nNot Applicable: Missing product sub-category could mean that the current product does not have a sub-category\nMissing Measurement: a missing temperature reading in some sensor data may mean that the measurement was skipped\n\nBefore coming up with a strategy, it is critical to understand why these values are missing.\n\n\n18.1.2 Excluding Rows with Missing Values\nIs the missing data point critical? In the context of property pricing with only two features (a very simplified example), the surface area is a critical data point. You may want to exclude the rows that have a key feature value missing.\n\n\n\n\n\n\n\n\n\n\n\nFlat\nSurface Area (sq m)\nDistance to Centre (km)\nNeighbourhood\nEnergy Rating\nSell Price (K€)\n\n\n\n\nA\n85\n4.2\nNeukölln\nB\n420\n\n\nB\n\n2.5\nKreuzberg\nA\n610\n\n\nC\n95\n6.1\nCharlott.\nC\n390\n\n\n\nLooking at the example above, you would remove property B from the training data.\n\n\n18.1.3 Excluding Features with Missing Values\n\n\n\n\n\n\n\n\n\n\n\n\nProperty\nSurface Area (sq m)\nDistance to Centre (km)\nNeighbourhood\nEnergy Rating\nSell Price (K€)\nYears since Build\n\n\n\n\nA\n85\n4.2\nNeukölln\nB\n420\n\n\n\nB\n120\n2.5\nKreuzberg\nA\n610\n8\n\n\nC\n95\n6.1\nCharlott.\nC\n390\n\n\n\nD\n70\n3.8\nKreuzberg\nD\n370\n22\n\n\nE\n110\n1.2\nCharlott.\nB\n700\n12\n\n\nF\n60\n5.0\nNeukölln\nF\n310\n\n\n\n\nIn the above example, the column Years since Build has many missing values. It would make sense to either review the underlying data or remove it from the dataset.\nFiltering out both rows and columns containing missing values may reduce the amount of data available to the model. For this reason, more sophisticated approaches are sometimes needed.\n\n\n18.1.4 Imputation of Missing Numerical Values\nFor numerical features, missing values can be handled with a method called imputation. This fills missing values with another substitute value.\n\n18.1.4.1 Imputation by 0\nWhen the missing value means “None”, like the missing number of transactions, it is a good idea to replace missing values with 0.\nImagine the data above also included “Balcony Area” in square meters. A missing value could mean “no balcony” and could safely be replaced by 0. Before doing so, ensure that a missing “Balcony Area” really means “no balcony”. In general, these data processing methods rely on a solid understanding of the data and problem.\n\n\n\nProperty\nTotal Area (sq m)\nBalcony Area (sq m)\nSell Price (K€)\n\n\n\n\nA\n95\n12\n450\n\n\nB\n55\n\n280\n\n\nC\n110\n15\n510\n\n\n\n\n\n18.1.4.2 Imputation by the Mean\nWhen values are missing at random, the mean or median could be good candidates for imputation.\nIn a dataset containing student heights:\n\n\n\nStudent\nHeight (cm)\nGender\nGrade\n\n\n\n\nA\n170\nM\n10\n\n\nB\n\nF\n10\n\n\nC\n165\nF\n9\n\n\nD\n180\nM\n11\n\n\nE\n172\nM\n\n\n\nF\n160\nF\n9\n\n\n\nA good strategy to deal with missing heights is to impute student height using the average height of all students. With a large enough dataset, a more fine-grained approach could be used. You could, for example, impute the missing heights based on students with the same grade.\nTo impute the Height of student B, we calculate the average height over all students:\n\\[\n\\text{Mean} = \\frac{170 + 165 + 180 + 172 + 160}{5} = 169.4\n\\]\nThe Height of student B would be replaced by \\(169.4\\).\n\nExercise 18.1 Using mean imputation to fill missing Grade values, what would be the Grade of student E?\n\n\n\n18.1.4.3 Imputation by the Median\nThe median could also be a good choice of substitute value for imputation. The median of a series is the middle number in a sorted list. Taking the list: \\([2,3,4,5,7,9,50]\\) as example, the median is \\(5\\). In other words, it is the value such that at least 50% of the values are inferior or equal to. One advantage of the median is that it is not sensitive to outliers.\n\n\nMean, median and outliers\n\nAn outlier is a data point that is very different to the others\nConsidering the following series:\n\\([2,3,4,5,7,9,50]\\)\n\n\n\nSeries with outlier\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([2,3,4,5,7,9,50])\n\nsort_idx = np.argsort(series1)\nareas_sorted = series1[sort_idx]\n\nplt.figure(figsize=(10, 2.5))\nplt.axhline(0, color='grey', linewidth=1, zorder=1)  # horizontal line\nplt.scatter(areas_sorted, np.zeros_like(areas_sorted), s=200, color='royalblue', zorder=2)\n\nplt.yticks([])\nplt.title('Series with Outlier', fontsize=18)\nplt.grid(True, axis='x', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\nThe point at \\(50\\) is an outlier of this distribution, as it is very far from the others. The mean of the series is affected by this number:\n\nExercise 18.2 Calculate the mean and median of this series. Show that the mean is \\(11.43\\), and that the median is \\(5\\).\n\n\nExercise 18.3 Show that after removing the item \\(50\\), the mean is \\(5.0\\), and the median is \\(4.5\\).\n\nAs you may have observed, the mean varies widely, whereas the median remains stable.\n\n\nExercise 18.4 Find the median of the following series:\n\n\\({5, 7, 9, 12, 15}\\)\n\\({1, 2, 2, 4, 8, 10, 13}\\)\n\n\nWhat happens when the length of the series is an even number?\nLooking at the following series: \\([1,2,3,4]\\), what would be the middle value? As the series contains an even number of items, there is no middle value.\nBy convention, the median of these series is the average between the two middle values. Here, the median would be \\(\\frac{2+3}{2} = 2.5\\).\n\nExercise 18.5 Find the median of the following series:\n\n\\({2, 3, 5, 8}\\)\n\\({4, 6, 8, 10, 12, 14}\\)\n\n\nGoing back to the student data:\n\n\n\nStudent\nHeight (cm)\nGender\nGrade\n\n\n\n\nA\n170\nM\n10\n\n\nB\n\nF\n10\n\n\nC\n165\nF\n9\n\n\nD\n180\nM\n11\n\n\nE\n172\nM\n\n\n\nF\n160\nF\n9\n\n\n\nTo impute the Height of student B with the median, we find the median. The median of the heights is \\(170\\), as it is the middle number in the sorted Height series: \\({160, 165, 170, 172, 180}\\).\n\nExercise 18.6 Impute the missing Grade of student E using median imputation\n\n\n\n18.1.4.4 Imputation of Time-Series Data\nTime-series data is data with a time dimension. As an example, the following data could be the temperature of a machine in a factory:\n\n\n\nTime (s)\nTemperature (°C)\n\n\n\n\n1\n18.0\n\n\n2\n18.5\n\n\n3\n19.1\n\n\n4\n20.0\n\n\n5\n20.8\n\n\n6\n\n\n\n7\n22.1\n\n\n8\n22.8\n\n\n9\n23.5\n\n\n10\n\n\n\n11\n26.2\n\n\n\nTime-series data is also often represented as a line chart:\n\n\n\nTemperature time series with missing values\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntime = np.arange(1, 12)\ntemp = [18.0, 18.5, 19.1, 20.0, 20.8, np.nan, 22.1, 22.8, 23.5, np.nan, 26.2]\n\nplt.figure(figsize=(8,4))\nplt.plot(time, temp, marker='o', linestyle='-', color='royalblue', label='Temperature')\nplt.xlabel('Time', fontsize=16)\nplt.ylabel('Temperature (°C)', fontsize=16)\nplt.title('Temperature Time Series with Missing Values', fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\nBut what is time?\n\nTime can be defined as the continuous and apparently irreversible progress of existence. Just like “truth” or “space”, the definitions of the foundational aspects of experience can sometimes be disappointing.\n\n\n18.1.4.4.1 Using the Mean and Median\nImputing null values with the mean and median could lead to some unintuitive results.\n\nExercise 18.7 Impute the null values of the following series using mean, then the median. To do so, show that the mean of the series is \\(21.22\\) and the median is \\(20.8\\).\n\n\n\nTime (s)\nTemperature (°C)\n\n\n\n\n1\n18.0\n\n\n2\n18.5\n\n\n3\n19.1\n\n\n4\n20.0\n\n\n5\n20.8\n\n\n6\n\n\n\n7\n22.1\n\n\n8\n22.8\n\n\n9\n23.5\n\n\n10\n\n\n\n11\n26.2\n\n\n\n\n\n\n\nImputation of missing values by mean\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntime = np.arange(1, 12)\ntemp = [18.0, 18.5, 19.1, 20.0, 20.8, np.nan, 22.1, 22.8, 23.5, np.nan, 26.2]\nmean_temp = np.nanmean(temp)\ntemp_mean = [v if not np.isnan(v) else mean_temp for v in temp]\n\nplt.figure(figsize=(8,4))\nplt.plot(time, temp_mean, marker='o', linestyle='-', color='royalblue', label=f'Imputed by Mean ({mean_temp:.2f}°C)')\nplt.xlabel('Time', fontsize=16)\nplt.ylabel('Temperature (°C)', fontsize=16)\nplt.title('Imputation by Mean', fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.tight_layout()\nplt.show()\n\nAs you can see, the mean imputation works very well for the observation 6. However, observation 10 is not imputed in a realistic way and introduces a jump in the series.\n\n\n18.1.4.4.2 Alternative Methods\nWhat would be a better method? In these time-series tasks, imputation by the previous value could be a good idea. This method is called forward-fill. It assumes that the value remains constant until a new measurement is taken.\n\n\n\nImputation by previous value\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ntime = np.arange(1, 12)\ntemp = [18.0, 18.5, 19.1, 20.0, 20.8, np.nan, 22.1, 22.8, 23.5, np.nan, 26.2]\ntemp_series = pd.Series(temp)\ntemp_ffill = temp_series.ffill()\n\nplt.figure(figsize=(8,4))\nplt.plot(time, temp_ffill, marker='o', linestyle='-', color='royalblue', label='Imputed by Previous')\nplt.xlabel('Time', fontsize=16)\nplt.ylabel('Temperature (°C)', fontsize=16)\nplt.title('Imputation by Previous Value', fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.tight_layout()\nplt.show()\n\nThe series looks more natural than with mean-imputation. Still, we can do better.\nAnother approach is to impute the missing values by computing the average of the two neighboring values. This method, often called linear interpolation, is particularly useful for data with a clear trend, as it smoothly connects the known data points.\n\n\n\nImputation by neighbours\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntime = np.arange(1,12)\ntemp = [18.0, 18.5, 19.1, 20.0, 20.8, np.nan, 22.1, 22.8, 23.5, np.nan, 26.2]\ntemp_neigh = temp.copy()\nfor i in range(1, len(temp_neigh)-1):\n    if np.isnan(temp_neigh[i]):\n        temp_neigh[i] = (temp_neigh[i-1] + temp_neigh[i+1]) / 2\n\nplt.figure(figsize=(8,4))\nplt.plot(time, temp_neigh, marker='o', linestyle='-', color='royalblue', label='Imputed by Neighbours')\nplt.xlabel('Time', fontsize=16)\nplt.ylabel('Temperature (°C)', fontsize=16)\nplt.title('Imputation by Neighbouring Values', fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.tight_layout()\nplt.show()\n\nThat looks more like it!\n\n\n\n18.1.4.5 Wrapping Up\nThere are many ways to impute numerical values, and no optimal solution. The choice comes down to the specific problem at hand. Remember that before coming up with an imputation strategy, it is critical to understand why the values are missing.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dealing with Missing Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-numerical-values.html#information-leakage",
    "href": "chapters/missing-numerical-values.html#information-leakage",
    "title": "18  Dealing with Missing Numeric Data",
    "section": "18.2 Information Leakage",
    "text": "18.2 Information Leakage\nAs mentioned in the introduction to this chapter, data preprocessing is a step in which information leakage commonly occurs. Dealing with missing values is no exception.\nIn the process of filling numerical null values with the median or the average (or anything calculated from the data), it is important to calculate these numbers from the training set only.\nThen, null values in the test set would be imputed with the metrics calculated with the training data.\nWhy is this the case? Because this is exactly what would happen as the model is used to predict new observations. When the model will predict the price of a new property, and that property has a missing value, how will it be imputed?\nAt this point, the only information available will be the training data. The missing values will be imputed using the mean or median of the training data.\nThe purpose of generating predictions on the test set is to estimate model performance on unseen data. Null values in the test set should be treated in the same way as missing values in unseen data.\nIn Machine Learning practice, these preprocessing methods are said to be fitted on the training set and applied to the test set. This fitting is the calculation of descriptive statistics.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dealing with Missing Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-numerical-values.html#final-thoughts",
    "href": "chapters/missing-numerical-values.html#final-thoughts",
    "title": "18  Dealing with Missing Numeric Data",
    "section": "18.3 Final Thoughts",
    "text": "18.3 Final Thoughts\nThis chapter walked through handling missing numerical values with the following steps:\n\nUnderstand why the values are missing\nDevelop a strategy: filtering or different types of imputation\nFit the imputation method to the training data\nImpute null values in the test set using training data statistics\n\nIt is important to remember to avoid information leakage in null value handling. This can be done by a clear separation of the training and test set, before data preprocessing.\nLooking at the previous example, what if the “Neighbourhood” column was missing? The following section will explore how to handle missing categorical values.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dealing with Missing Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-numerical-values.html#solutions",
    "href": "chapters/missing-numerical-values.html#solutions",
    "title": "18  Dealing with Missing Numeric Data",
    "section": "18.4 Solutions",
    "text": "18.4 Solutions\n\nSolution 18.1. Exercise 18.1\n\n\n\nStudent\nHeight (cm)\nGender\nGrade\n\n\n\n\nA\n170\nM\n10\n\n\nB\n\nF\n10\n\n\nC\n165\nF\n9\n\n\nD\n180\nM\n11\n\n\nE\n172\nM\n\n\n\nF\n160\nF\n9\n\n\n\nThe average Grade is: \\[\n\\text{Mean} = \\frac{10 + 10 + 9 + 11 + 9}{5} = \\frac{49}{50} = 9.8\n\\]\nThe missing Grade of student E would be replaced by \\(9.8\\).\n\n\nSolution 18.2. Exercise 18.2\nSeries: \\([2,3,4,5,7,9,50]\\)\n\\[\n\\text{Mean} = \\frac{2 + 3 + 4 + 5 + 7 + 9 + 50}{7} = \\frac{80}{7} \\approx 11.43\n\\]\nThe median is the middle number of the sorted series. Here, \\(5\\).\n\n\nSolution 18.3. Exercise 18.3\nSeries: \\([2,3,4,5,7,9]\\)\n\\[\n\\text{Mean} = \\frac{2 + 3 + 4 + 5 + 7 + 9}{6} = \\frac{30}{6} = 5\n\\]\nAs the series now contains an even number of observations, the median is the average of the two middle numbers, here \\(\\frac{4+5}{2} = 4.5\\).\n\n\nSolution 18.4. Exercise 18.4\n\n\\({5, 7, 9, 12, 15}, \\text{Median} = 9\\)\n\\({1, 2, 2, 4, 8, 10, 13}, \\text{Median} = 4\\)\n\n\n\nSolution 18.5. Exercise 18.5\n\n\\({2, 3, 5, 8}, \\text{Median} = \\frac{3 + 5}{2} = 4\\)\n\\({4, 6, 8, 10, 12, 14}, \\text{Median} = \\frac{8 + 10}{2} = 9\\)\n\n\n\nSolution 18.6. Exercise 18.6\nThe median of the Grade series is the middle number of the sorted series: \\({9, 9, 10, 10, 11}\\). Here, it is \\(10\\). The grade of student E would therefore be filled with \\(10\\).\n\n\nSolution 18.7. Exercise 18.7\nFor imputation, we need to compute both the mean and median:\n\\[\n\\text{Mean} = \\frac{18 + 18.5 + 19.1 + 20 + 20.8 + 22.1 + 22.8 + 23.5 + 26.2}{9} = \\frac{191}{9} = 21.22\n\\]\nThe median is the middle number of the sorted series: \\({18, 18.5, 19.1, 20.0, 20.8, 22.1, 22.8, 23.5, 26.2}\\), here, \\(20.8\\)\n\n\n\n\n\n\n\n\n\nTime\nTemperature (°C)\nTemperature (Mean Imputation)\nTemperature (Median Imputation)\n\n\n\n\n1\n18.0\n18.0\n18.0\n\n\n2\n18.5\n18.5\n18.5\n\n\n3\n19.1\n19.1\n19.1\n\n\n4\n20.0\n20.0\n20.0\n\n\n5\n20.8\n20.8\n20.8\n\n\n6\n\n21.22\n20.8\n\n\n7\n22.1\n22.1\n22.1\n\n\n8\n22.8\n22.8\n22.8\n\n\n9\n23.5\n23.5\n23.5\n\n\n10\n\n21.22\n20.8\n\n\n11\n26.2\n26.2\n26.2",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dealing with Missing Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-categorical-values.html",
    "href": "chapters/missing-categorical-values.html",
    "title": "19  Dealing with Missing Categorical Data",
    "section": "",
    "text": "19.1 Excluding Rows with Missing Observations\nCategorical values can also be missing. As a reminder, categorical features represent groups or categories. In the Berlin property pricing example, the neighbourhood of the property (e.g., “Neukölln”, “Kreuzberg” or “Charlottenburg”) is a categorical feature. As neighbourhood is such an important feature in property pricing, it may make sense to filter out observations for which it is missing.\nJust like numerical features, there are many reasons why a categorical value may be missing:\nIt is important to understand why data may be missing before coming up with an imputation strategy.\nIf a missing value is critical to the prediction task, filtering out rows with missing values is the safest strategy. You would not want to price a property without knowing its neighbourhood or its post code.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dealing with Missing Categorical Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-categorical-values.html#excluding-columns-with-missing-observations",
    "href": "chapters/missing-categorical-values.html#excluding-columns-with-missing-observations",
    "title": "19  Dealing with Missing Categorical Data",
    "section": "19.2 Excluding Columns with Missing Observations",
    "text": "19.2 Excluding Columns with Missing Observations\nIf a column contains many missing values, removing the column or reviewing the data processing script should be the preferred strategy. This is similar to the method used for numerical missing values.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dealing with Missing Categorical Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-categorical-values.html#excluding-rows-with-missing-observations-1",
    "href": "chapters/missing-categorical-values.html#excluding-rows-with-missing-observations-1",
    "title": "19  Dealing with Missing Categorical Data",
    "section": "19.3 Excluding Rows with Missing Observations",
    "text": "19.3 Excluding Rows with Missing Observations\nLikewise, if the missing categorical value is critical to the prediction task, excluding the observation is the safest approach. Could you consider a property without knowing if it is a house or a flat? Or without knowing its neighbourhood?",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dealing with Missing Categorical Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-categorical-values.html#creating-a-new-null-category",
    "href": "chapters/missing-categorical-values.html#creating-a-new-null-category",
    "title": "19  Dealing with Missing Categorical Data",
    "section": "19.4 Creating a new Null Category",
    "text": "19.4 Creating a new Null Category\nIn other cases, all missing values could be replaced by a “missing” category and treated as another value for that categorical variable. Let’s make this more concrete with this example:\n\n\n\n\n\n\n\n\n\n\n\n\n\nProperty\nSurface Area (sq m)\nDistance to Centre (km)\nNeighbourhood\nEnergy Rating\nOutdoor Space\nSell Price (K€)\nYears since Build\n\n\n\n\nA\n85\n4.2\nNeukölln\nB\nBalcony\n420\n\n\n\nB\n120\n2.5\nKreuzberg\nA\nTerrace\n610\n8\n\n\nC\n95\n6.1\nCharlott.\nC\n\n390\n\n\n\nD\n70\n3.8\nKreuzberg\nD\nGarden\n370\n22\n\n\nE\n110\n1.2\nCharlott.\nB\nBalcony\n700\n12\n\n\nF\n60\n5.0\nNeukölln\nF\n\n310\n\n\n\n\nHere, some of the “Outdoor Space” values are missing. As mentioned above, the missing values can be replaced by an “Unknown” value.\n\n\n\n\n\n\n\n\n\n\n\n\n\nProperty\nSurface Area (sq m)\nDistance to Centre (km)\nNeighbourhood\nEnergy Rating\nOutdoor Space\nSell Price (K€)\nYears since Build\n\n\n\n\nA\n85\n4.2\nNeukölln\nB\nBalcony\n420\n\n\n\nB\n120\n2.5\nKreuzberg\nA\nTerrace\n610\n8\n\n\nC\n95\n6.1\nCharlott.\nC\nUnknown\n390\n\n\n\nD\n70\n3.8\nKreuzberg\nD\nGarden\n370\n22\n\n\nE\n110\n1.2\nCharlott.\nB\nBalcony\n700\n12\n\n\nF\n60\n5.0\nNeukölln\nF\nUnknown\n310\n\n\n\n\nThis value can then be treated in the same way as all the other categorical values, with methods like One-Hot Encoding or Target Encoding.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dealing with Missing Categorical Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-categorical-values.html#information-leakage",
    "href": "chapters/missing-categorical-values.html#information-leakage",
    "title": "19  Dealing with Missing Categorical Data",
    "section": "19.5 Information Leakage",
    "text": "19.5 Information Leakage\nJust like categorical variable encoding, handling missing categorical values can create information leakage.\nThis process should be done on the training data only, and applied to the test set. Any new category seen in the test set only should be either excluded or labelled as missing.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dealing with Missing Categorical Data</span>"
    ]
  },
  {
    "objectID": "chapters/missing-categorical-values.html#final-thoughts",
    "href": "chapters/missing-categorical-values.html#final-thoughts",
    "title": "19  Dealing with Missing Categorical Data",
    "section": "19.6 Final Thoughts",
    "text": "19.6 Final Thoughts\nThis chapter walked through handling missing values with the following steps:\n\nUnderstand why the values are missing\nExplore whether filtering is needed\nFlag missing values with a new categorical value\n\nIt is important to remember to avoid information leakage in null value handling. This can be done by a clear separation of the training and test set, before data preprocessing.\nThe next chapter will go back to numerical data and explore the issue of scaling.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Dealing with Missing Categorical Data</span>"
    ]
  },
  {
    "objectID": "chapters/scaling.html",
    "href": "chapters/scaling.html",
    "title": "20  Numerical Feature Scaling",
    "section": "",
    "text": "20.1 The Mechanics\nLists of numbers come in all shapes and sizes. But why should we care?\nMany Machine Learning algorithms operate on data as vectors or points in space. KNN generates predictions based on the distance or similarities between observations. Decision Trees split the feature space to separate data from different classes or labels.\nThinking of nearest neighbours, feature scaling does matter. Let’s imagine you are building a model to predict property prices, using two features:\nExample properties in the training data include:\nWhen plotting this data over a two-dimensional space on the interval [0, 200], we get:\nAs you can see, most of the variation is generated by the surface area. This is why the data appears so compressed.\nThe dominance of surface area over the number of rooms can also be seen when calculating the Euclidean Distance between properties.\n\\[\n\\begin{aligned}\n\\text{Distance between property A and B:} \\\\\nd_{AB} &= \\sqrt{(80 - 60)^2 + (3 - 2)^2} \\\\\n       &= \\sqrt{20^2 + 1^2} \\\\\n       &= \\sqrt{400 + 1} \\quad \\text{surface area dwarfs the difference in number of rooms}\\\\\n       &= \\sqrt{401} \\\\\n       &\\approx 20.02 \\\\\n\\\\\n\\text{Distance between property A and C:} \\\\\nd_{AC} &= \\sqrt{(120 - 60)^2 + (5 - 2)^2} \\\\\n       &= \\sqrt{60^2 + 3^2} \\\\\n       &= \\sqrt{3600 + 9} \\quad \\text{here again}\\\\\n       &= \\sqrt{3609} \\\\\n       &\\approx 60.08\n\\end{aligned}\n\\]\nAs you can see, in both cases, the distance between two properties is nearly fully determined by the surface area difference. The distance between number of rooms is dwarfed by the difference in surface area. We could get a more even distribution by rescaling the axes:\nWhile rescaling the axes is a better way to visualise the data, it does not solve the problem in calculating the distance between two observations.\nTo do so, we need to scale both features so that they (roughly) have the same range, mean and variance:\nAs an example, the surface area column has a higher variance than the number of rooms column. The numbers fluctuate more from one to the other. As a consequence, surface area has a larger impact than the number of rooms on the distance function.\nWhat if we could transform both the number of rooms and the surface area to series of number between 0 and 1? This would solve the issue of distance calculation. The following section will explore a way to do this.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Numerical Feature Scaling</span>"
    ]
  },
  {
    "objectID": "chapters/scaling.html#the-mechanics",
    "href": "chapters/scaling.html#the-mechanics",
    "title": "20  Numerical Feature Scaling",
    "section": "",
    "text": "Range is the distance between the minimum and the maximum of a series.\nMean is the average of a series\nVariance can have many definitions, for now, let’s define it as the degree to which numbers fluctuate",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Numerical Feature Scaling</span>"
    ]
  },
  {
    "objectID": "chapters/scaling.html#min-max-scaling",
    "href": "chapters/scaling.html#min-max-scaling",
    "title": "20  Numerical Feature Scaling",
    "section": "20.2 Min-Max Scaling",
    "text": "20.2 Min-Max Scaling\nLet’s imagine the following series of numbers: \\(\\{0, 30, 40, 60, 90, 100\\}\\)\n\n\n\nSeries 0-100\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([0, 30, 40, 60, 90, 100])\n\n# Sort for better label spacing\nsort_idx = np.argsort(series1)\nareas_sorted = series1[sort_idx]\n\nplt.figure(figsize=(10, 2.5))\nplt.axhline(0, color='grey', linewidth=1, zorder=1)  # horizontal line\nplt.scatter(areas_sorted, np.zeros_like(areas_sorted), s=200, color='royalblue', zorder=2)\n\nplt.yticks([])\nplt.title('Series on the Number Line', fontsize=18)\nplt.grid(True, axis='x', linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\nA simple way to get these numbers in the 0-1 interval would be to divide all of them by 100:\n\n\n\nValue\nScaled\n\n\n\n\n0\n0.00\n\n\n30\n0.30\n\n\n40\n0.40\n\n\n60\n0.60\n\n\n90\n0.90\n\n\n100\n1.00\n\n\n\n\n\n\nSeries scaled to 0-1\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([0, 30, 40, 60, 90, 100])/100\n\nsort_idx = np.argsort(series1)\nareas_sorted = series1[sort_idx]\n\nplt.figure(figsize=(10, 2.5))\nplt.axhline(0, color='grey', linewidth=1, zorder=1)\nplt.scatter(areas_sorted, np.zeros_like(areas_sorted), s=200, color='royalblue', zorder=2)\n\nplt.yticks([])\nplt.title('Series divided by 100', fontsize=18)\nplt.grid(True, axis='x', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\nIf the original series has numbers going from 0-150: \\(\\{0, 10, 60, 70, 120, 150\\}\\)\n\n\n\nSeries 0-150\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([0, 10, 60, 70, 120, 150])\n\nsort_idx = np.argsort(series1)\nareas_sorted = series1[sort_idx]\n\nplt.figure(figsize=(10, 2.5))\nplt.axhline(0, color='grey', linewidth=1, zorder=1)\nplt.scatter(areas_sorted, np.zeros_like(areas_sorted), s=200, color='royalblue', zorder=2)\n\nplt.yticks([])\nplt.title('Series with a Maximum of 150', fontsize=18)\nplt.grid(True, axis='x', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\nWe could also get it back to the 0-1 interval by dividing all numbers by 150, the maximum of the series:\n\n\n\nValue\nScaled\n\n\n\n\n0\n0.00\n\n\n10\n0.07\n\n\n60\n0.40\n\n\n70\n0.47\n\n\n120\n0.80\n\n\n150\n1.00\n\n\n\n\n\n\nSeries 0-1 (by 150)\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([0, 10, 60, 70, 120, 150])/150\n\nsort_idx = np.argsort(series1)\nareas_sorted = series1[sort_idx]\n\nplt.figure(figsize=(10, 2.5))\nplt.axhline(0, color='grey', linewidth=1, zorder=1)  # horizontal line\nplt.scatter(areas_sorted, np.zeros_like(areas_sorted), s=200, color='royalblue', zorder=2)\n\nplt.yticks([])\nplt.title('Series with a Maximum of 150, divided by 150', fontsize=18)\nplt.grid(True, axis='x', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\nThis is a good start. For series ranging from 0 to a given number, we can divide all numbers in the series by their maximum. This scales the series to the range 0-1.\nNow, what if the numbers in the series range from 50 to 250?\nSeries: \\(\\{50, 90, 120, 170, 220, 250\\}\\)\n\n\n\nSeries 50-250\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([50, 90, 120, 170, 220, 250])\n\n# Sort for better label spacing\nsort_idx = np.argsort(series1)\nareas_sorted = series1[sort_idx]\n\nplt.figure(figsize=(10, 2.5))\nplt.axhline(0, color='grey', linewidth=1, zorder=1)  # horizontal line\nplt.scatter(areas_sorted, np.zeros_like(areas_sorted), s=200, color='royalblue', zorder=2)\n\nplt.yticks([])\nplt.title('Series in the Range 50-250', fontsize=18)\nplt.grid(True, axis='x', linestyle='--', alpha=0.5)\nplt.xlim(0,260)\n\nplt.tight_layout()\nplt.show()\n\nDividing the numbers of the series by the maximum would not make the full use of the 0-1 interval:\n\n\n\nValue\nScaled (÷250)\n\n\n\n\n50\n0.20\n\n\n90\n0.36\n\n\n120\n0.48\n\n\n170\n0.68\n\n\n220\n0.88\n\n\n250\n1.00\n\n\n\n\n\n\nSeries 0.2-1 (by 250)\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([50, 90, 120, 170, 220, 250])/250\n\nsort_idx = np.argsort(series1)\nareas_sorted = series1[sort_idx]\n\nplt.figure(figsize=(10, 2.5))\nplt.axhline(0, color='grey', linewidth=1, zorder=1)\nplt.scatter(areas_sorted, np.zeros_like(areas_sorted), s=200, color='royalblue', zorder=2)\n\nplt.yticks([])\nplt.title('Series in the Range 50-250, divided by 250', fontsize=18)\nplt.grid(True, axis='x', linestyle='--', alpha=0.5)\nplt.xlim(-0.1,1.1)\n\nplt.tight_layout()\nplt.show()\n\nThe minimum achieved there is \\(50/250 = 0.2\\). This means that 20% of the allowed range will be left empty, a suboptimal outcome. How could this problem be solved?\nIn the end, we want the minimum of the series to be mapped to 0, and the maximum to 1. To achieve this, we can use the following formula:\n\\[\n\\text{Scaled Value} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n\\]\nWith:\n\n\\(x\\) the value to be scaled\n\\(x_{\\text{min}}\\) and \\(x_{\\text{max}}\\) the minimum and maximum of the series\n\nLet’s take this formula part by part:\n\nThe numerator: \\(x - x_{\\text{min}}\\), computes the distance between the value and the minimum\nThe denominator: \\(x_{\\text{max}} - x_{\\text{min}}\\), divides this distance by the largest distance in the series, the distance between the maximum and minimum values\n\nThis formula will be maximised when the value is the maximum of the series, \\(\\text{original value} = x_{\\text{max}}\\):\n\\[\n\\text{Scaled Value of Max} = \\frac{x_{\\text{max}} - x_{\\text{min}}}{x_{\\text{max}}} - x_{\\text{min}} = 1\n\\]\nAnd be minimised when the value is the minimum of the series, \\(\\text{original value} = x_{\\text{min}}\\):\n\\[\n\\text{Scaled Value of Min} = \\frac{x_{\\text{min}} - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}} = 0\n\\]\nWith this formula, any series of numbers can be mapped to the range \\([0,1]\\), solving the problems.\nThe following table shows its application to the example series above:\n\n\n\nValue\nScaled (÷250)\nScaled (MinMax)\n\n\n\n\n50\n0.20\n0.00\n\n\n90\n0.36\n0.20\n\n\n120\n0.48\n0.35\n\n\n170\n0.68\n0.60\n\n\n220\n0.88\n0.85\n\n\n250\n1.00\n1.00\n\n\n\n\nExercise 20.1 Use Min-Max Scaling to map both surface area and number of rooms to the range \\([0,1]\\)\n\n\n\nProperty\nSize (m²)\nNumber of rooms\n\n\n\n\nA\n60\n2\n\n\nB\n80\n3\n\n\nC\n120\n5\n\n\nD\n70\n2\n\n\nE\n150\n6\n\n\nF\n90\n4",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Numerical Feature Scaling</span>"
    ]
  },
  {
    "objectID": "chapters/scaling.html#standardisation",
    "href": "chapters/scaling.html#standardisation",
    "title": "20  Numerical Feature Scaling",
    "section": "20.3 Standardisation",
    "text": "20.3 Standardisation\nThe following section will get a bit more mathematical and can be skipped (next section).\nMin-Max Scaling is not the only way to preprocess a series of numbers in different scales. Another approach is called Standardisation. The idea of standardisation is to make the distributions of the series similar. The distribution is a representation of how frequently the values of a variable occur.\nLet’s make this more concrete with an example:\n\n\n\nThree series with different means and variances\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([30, 55, 60, 50, 45, 75, 65])\nseries2 = np.array([35, 37, 36, 34, 45, 30, 37])\nseries3 = np.array([ 20,  50, 100,  80,  70, 120, 150])\n\nseries = [series1, series2, series3]\nlabels = ['Series 1', 'Series 2', 'Series 3']\ncolors = ['royalblue', 'crimson', 'darkorange']\n\nfig, axes = plt.subplots(3, 1, figsize=(10, 6), sharex=True, gridspec_kw={'hspace': 0.3})\n\nall_data = np.concatenate(series)\nxmin, xmax = 0, all_data.max() + 10\n\nfor i, ax in enumerate(axes):\n    y = np.zeros_like(series[i])\n    ax.scatter(series[i], y, s=100, color=colors[i], zorder=2)\n    ax.axhline(0, color='grey', linewidth=1, zorder=1)\n    ax.set_yticks([])\n    ax.set_xlim(xmin, xmax)\n    ax.set_title(labels[i], fontsize=14, loc='left')\n    ax.grid(True, axis='x', linestyle='--', alpha=0.5)\n    if i &lt; 2:\n        ax.tick_params(labelbottom=False)\n    else:\n        ax.set_xlabel('Value', fontsize=16)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n\nfig.suptitle('Three Series with Different Means and Variances', fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\nThese series have very different distributions. The difference of scale would have a negative impact on Machine Learning algorithms like KNN.\nWhy can we say that these two series have different distributions?\n\nThey do not have the same mean or centre\nThey do not have the same spread or variance around their respective means\n\nThese are the two discrepancies that Standardisation aims to correct.\n\n\n20.3.1 Describing a series\nBefore we start, it is important to define some important concepts of descriptive statistics.\nBreaking this term down:\n\nStatistics is a branch of mathematics focussing on the collection and analysis of collections of data\nDescriptive statistics is a branch of statistics concerned with the summarisation and description of a collection of data\n\nThe Minimum and Maximum of a series are descriptive statistics, representing the lowest and highest number of a collection of numbers.\nThis section will introduce further concepts in descriptive statistics:\n\nMean\nVariance\nStandard Deviation\n\nIf you are already familiar with these, feel free to skip to the next section.\n\n20.3.1.1 Mean\nThe mean represents the “centre” of a collection of numbers. There are different types of means in mathematics. This book will focus on the arithmetic mean, also referred to as the “average”. It is noted \\(\\bar{x}\\) or \\(\\mu\\).\n\n\nOther kinds of mean\n\nThere are other types of means, used to avearge different types of values. They all have different properties, advantages and drawbacks\nGeometric mean: \\(\\left(\\prod_{i=1}^n x_i\\right)^{1/n}\\). It is useful for averaging ratios, growth rates, or percentages. The scary \\(\\prod_{i=1}^n\\) is the letter pi in capital letter, representing a product. \\(\\prod_{i=1}^n i = 1 \\cdot 2 \\cdot \\dots \\cdot n\\)\nUsing geometric means, a single 0 in the series will make the mean 0. It is still widely used in economics to average multiplicative phenomena like rates of growth.\nHarmonic mean: \\(\\left(\\frac{1}{n}\\sum_{i=1}^n \\frac{1}{x_i}\\right)^{-1}\\). It is useful for rates, such as model Precision or Recall (Model Evaluation chapter).\nThe harmonic mean of model Precision and Recall is called the F1-score. It is commonly used to find models that have a good trade-off of both metrics.\n\\[\nF_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\nThe arithmetic mean of a series of \\(n\\) observations, \\(\\{x_1, x_2, \\dots, x_n\\}\\) is computed with the following formula:\n\\[\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\nUsing the \\(\\Sigma\\) summation operator: \\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nBoth notations are equivalent. If you find the \\(\\Sigma\\) notation intimidating, refer to the Distance chapter for an intuitive explanation.\nIf you ever calculated your average grade at school, you probably used an arithmetic mean or a weighted average.\nAs an exmaple, these two series have different means:\n\n\n\nSeries 1\nSeries 2\n\n\n\n\n1\n4\n\n\n2\n8\n\n\n4\n10\n\n\n5\n12\n\n\n8\n16\n\n\n\n\n\n\nTwo series with different means\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.ticker import MaxNLocator\n\nseries1 = np.array([7, 8, 10, 11, 14]) - 6\nseries2 = [4, 8, 10, 12, 16]\n\nseries = [series1, series2]\nlabels = ['Series 1', 'Series 2']\ncolors = ['royalblue', 'crimson']\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True, gridspec_kw={'hspace': 0.3})\n\nall_data = np.concatenate(series)\nxmin, xmax = 0, all_data.max() + 5\n\nfor i, ax in enumerate(axes):\n    y = np.zeros_like(series[i])\n    ax.scatter(series[i], y, s=100, color=colors[i], zorder=2)\n    ax.axhline(0, color='grey', linewidth=1, zorder=1)\n    ax.set_yticks([])\n    ax.set_xlim(xmin, xmax)\n    ax.set_title(labels[i], fontsize=14, loc='left')\n    ax.grid(True, axis='x', linestyle='--', alpha=0.5)\n    if i &lt; 1:\n        ax.tick_params(labelbottom=False)\n    else:\n        ax.set_xlabel('Value', fontsize=16)\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n\nfig.suptitle('Two Series with Different Distributions', fontsize=16)\nplt.tight_layout()\nplt.show()\n\nThe mean of the first series can be calculated with the formula shown above:\n\\[\n\\text{Mean}_{x_1} = \\frac{1 + 2 + 4 + 5 + 8}{5} = 4\n\\]\n\nExercise 20.2 Show that the mean of the second series is equal to \\(10\\).\n\n\n\n20.3.1.2 Variance and Standard Deviation\nThe following two series have the same mean, and yet, they look very different:\n\n\n\nSeries 1\nSeries 2\n\n\n\n\n7\n4\n\n\n8\n8\n\n\n10\n10\n\n\n11\n12\n\n\n14\n16\n\n\n\n\n\n\nTwo series, same mean, different variance\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.ticker import MaxNLocator\n\nseries1 = [7, 8, 10, 11, 14]\nseries2 = [4, 8, 10, 12, 16]\n\nseries = [series1, series2]\nlabels = ['Series 1', 'Series 2']\ncolors = ['royalblue', 'crimson']\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True, gridspec_kw={'hspace': 0.3})\n\nall_data = np.concatenate(series)\nxmin, xmax = 0, all_data.max() + 5\n\nfor i, ax in enumerate(axes):\n    y = np.zeros_like(series[i])\n    ax.scatter(series[i], y, s=100, color=colors[i], zorder=2)\n    ax.axhline(0, color='grey', linewidth=1, zorder=1)\n    ax.set_yticks([])\n    ax.set_xlim(xmin, xmax)\n    ax.set_title(labels[i], fontsize=14, loc='left')\n    ax.grid(True, axis='x', linestyle='--', alpha=0.5)\n    if i &lt; 1:\n        ax.tick_params(labelbottom=False)\n    else:\n        ax.set_xlabel('Value', fontsize=16)\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n\nfig.suptitle('Two Series with the same Means', fontsize=16)\nplt.tight_layout()\nplt.show()\n\nThe main difference between these two series is that the second has a much greater variance than the first. Variance refers to the spread of the individual values around the mean.\nHow would you quantify the spread of values around the mean? How would you summarise this spread in a single number?\nHint: You could refer to the Regression Model Evaluation chapter.\nThe distance between an individual value (\\(x\\)) and the mean (\\(\\bar{x}\\)) can be calculated as follows:\n\\[\nx - \\bar{x}\n\\]\nThis works for a single value. Now, how would you aggregate these distances? Remember that positive and negative distances should not cancel out.\nIf you thought of using the absolute value or squared differences, well done! This is the right intuition. The variance is the average square difference between individual values and the mean.\n\\[\n\\mathrm{Var}(x) = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n}\n\\]\nSigma notation: \\[\n\\mathrm{Var}(x) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]\nDoes this remind you of a formula? This is very similar to the Mean Squared Error (MSE), the average squared difference between model predictions and true labels. The variance is in a sense the MSE of the average.\nIf a model kept predicting the average for all observations, its Mean Squared Error would be the variance!\nThe variance for the first series can be computed as follows:\n\\[\n\\begin{aligned}\n\\bar{x}_1 &= \\frac{7 + 8 + 10 + 11 + 14}{5} = 10 \\\\\n\\mathrm{Var}(x_1) &= \\frac{(7-10)^2 + (8-10)^2 + (10-10)^2}{5} \\\\\n&\\quad + \\frac{(11-10)^2 + (14-10)^2}{5} \\\\\n                  &= \\frac{9 + 4 + 0 + 1 + 16}{5} = \\frac{30}{5} = 6\n\\end{aligned}\n\\]\n\nExercise 20.3 Compute the variance of the second series. Prove that it is \\(16\\).\n\nIs there something strange with these variance numbers? Similar to the Mean Squared Errors, they are much larger than the scale of the original series.\nHow would you solve this problem? Here again, the Regression Model Evaluation chapter could be helpful. Just like the Mean Squared Error and the Root Mean Squared Error, you could simply take the square root of the variance.\nThis number gives you a much more interpretable idea of the typical spread between individual values and the mean of a series. This measure is called the Standard Deviation, commonly noted \\(\\sigma\\).\n\\[\n\\sigma = \\sqrt{\\mathrm{Var}(x)}\n\\]\nThis standard deviation measures the average spread from the mean in a series.\n\n\nAre descriptive statistics this descriptive?\n\nAs surprising as it may seem, all the datasets have the same mean, variance and standard deviation for both \\(x\\) and \\(y\\). This is the famous Ascombe Quartet.\n\n\n\nAnscombe’s quartet\n\n\n\n\nFigure code\n\nimport seaborn as sns\nfrom seaborn import load_dataset\nimport matplotlib.pyplot as plt\n\nanscombe = sns.load_dataset(\"anscombe\")\nplt.figure(figsize=(10,6))\nfor i, group in enumerate(['I', 'II', 'III', 'IV']):\n    subset = anscombe[anscombe['dataset'] == group]\n    plt.subplot(2,2,i+1)\n    plt.scatter(subset['x'], subset['y'], s=60)\n    plt.title(f\"Dataset {group}\", fontsize=18)\n    plt.xlabel('x', fontsize=16)\n    plt.ylabel('y', fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\nplt.tight_layout()\nplt.show()\n\nDescriptive statistics for Anscombe’s quartet:\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nMean x\nMean y\nVar x\nVar y\nStd x\nStd y\n\n\n\n\nI\n9\n7.5\n10\n3.75\n3.16\n1.94\n\n\nII\n9\n7.5\n10\n3.75\n3.16\n1.94\n\n\nIII\n9\n7.5\n10\n3.75\n3.16\n1.94\n\n\nIV\n9\n7.5\n10\n3.75\n3.16\n1.94\n\n\n\nThis statistical paradox is good reminder that descriptive statistics are remain a reduction of reality.\n\n\n\n\n20.3.2 Standardising series with Mean and Standard Deviation\n\n20.3.2.1 Mean Centre\nGoing back to the three example series:\n\n\n\nThree series with different means and variances\n\n\nHow could these be standardised so that they have the same mean and spread?\nFirst, by subtracting the mean to each observation, the two series could both be centred around 0.\n\\[\nx_{\\text{mean centred}} = x - \\bar{x}\n\\]\nA good first step:\n\n\n\nThree series, mean centred\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([30, 55, 60, 50, 45, 75, 65])\nseries2 = np.array([35, 37, 36, 34, 45, 30, 37])\nseries3 = np.array([ 20,  50, 100,  80,  70, 120, 150])\n\nseries = [series1, series2, series3]\nlabels = ['Series 1', 'Series 2', 'Series 3']\ncolors = ['royalblue', 'crimson', 'darkorange']\n\nfor idx, ind_series in enumerate(series):\n    mean = np.mean(ind_series)\n    series[idx] = ind_series.astype(float) - mean\n\nfig, axes = plt.subplots(3, 1, figsize=(10, 6), sharex=True, gridspec_kw={'hspace': 0.3})\n\n# Find global min/max for consistent x-axis\nall_data = np.concatenate(series)\nxmin, xmax = all_data.min() - 10, all_data.max() + 10\n\nfor i, ax in enumerate(axes):\n    y = np.zeros_like(series[i])\n    ax.scatter(series[i], y, s=100, color=colors[i], zorder=2)\n    ax.axhline(0, color='grey', linewidth=1, zorder=1)\n    ax.set_yticks([])\n    ax.set_xlim(xmin, xmax)\n    ax.set_title(labels[i], fontsize=14, loc='left')\n    ax.grid(True, axis='x', linestyle='--', alpha=0.5)\n    if i &lt; 2:\n        ax.tick_params(labelbottom=False)\n    else:\n        ax.set_xlabel('Value', fontsize=16)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n\nfig.suptitle('Three Series, Mean-Centred', fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\nBut the three series still have widely different spreads.\n\n\n20.3.2.2 Variance Scaling\nTo make sure both series have the same variance, or spread around the mean, you could simply divide all numbers of the resulting series by the standard deviation:\n\\[\nx_{\\text{standardised}} = \\frac{x - \\bar{x}}{\\sigma}\n\\]\n\n\n\nThree series, standardised\n\n\n\n\nFigure code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nseries1 = np.array([30, 55, 60, 50, 45, 75, 65])\nseries2 = np.array([35, 37, 36, 34, 45, 30, 37])\nseries3 = np.array([ 20,  50, 100,  80,  70, 120, 150])\n\nseries = [series1, series2, series3]\nlabels = ['Series 1', 'Series 2', 'Series 3']\ncolors = ['royalblue', 'crimson', 'darkorange']\n\nfor idx, ind_series in enumerate(series):\n    mean = np.mean(ind_series)\n    std = np.std(ind_series)\n    series[idx] = (ind_series.astype(float) - mean)/std\n\nfig, axes = plt.subplots(3, 1, figsize=(10, 6), sharex=True, gridspec_kw={'hspace': 0.3})\n\n# Find global min/max for consistent x-axis\nall_data = np.concatenate(series)\nxmin, xmax = all_data.min() - 10, all_data.max() + 10\n\nfor i, ax in enumerate(axes):\n    y = np.zeros_like(series[i])\n    ax.scatter(series[i], y, s=100, color=colors[i], zorder=2)\n    ax.axhline(0, color='grey', linewidth=1, zorder=1)\n    ax.set_yticks([])\n    ax.set_xlim(xmin, xmax)\n    ax.set_title(labels[i], fontsize=14, loc='left')\n    ax.grid(True, axis='x', linestyle='--', alpha=0.5)\n    if i &lt; 2:\n        ax.tick_params(labelbottom=False)\n    else:\n        ax.set_xlabel('Value', fontsize=16)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n\nfig.suptitle('Three Series, Standardised', fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\nStandardising series in this way, we preserve the information they contain while aligning their mean and variance.\n\n\n20.3.2.3 Example\nLet’s go back to the example data and scale the Size (m²) series:\n\n\n\nProperty\nSize (m²)\nNumber of rooms\nPrice (k €)\n\n\n\n\nA\n60\n2\n320\n\n\nB\n80\n3\n400\n\n\nC\n120\n5\n350\n\n\nD\n70\n2\n310\n\n\nE\n150\n6\n600\n\n\nF\n90\n4\n330\n\n\n\nTo do so, we need to compute the series mean and variance:\n\\[\\begin{aligned}\n\\text{Mean} &= \\frac{60 + 80 + 120 + 70 + 150 + 90}{6} = \\frac{570}{6} = 95 \\\\\n\\text{Variance} &= \\frac{(60 - 95)^2 + (80 - 95)^2 + (120 - 95)^2}{6} \\\\\n&\\quad + \\frac{(70 - 95)^2 + (150 - 95)^2 + (90 - 95)^2}{6} \\\\\n&= \\frac{5750}{6} \\approx 958.3 \\\\\n\\text{Standard Deviation} &= \\sqrt{\\text{Variance}} = \\sqrt{958.3} \\approx 31\n\\end{aligned}\n\\]\nStandardised series:\n\n\n\nProperty\nSize (m²)\nScaled Size\n\n\n\n\nA\n60\n\\(\\frac{60-95}{31} = -1.13\\)\n\n\nB\n80\n\\(\\frac{80-95}{31} = -0.48\\)\n\n\nC\n120\n\\(\\frac{120-95}{31} = 0.81\\)\n\n\nD\n70\n\\(\\frac{70-95}{31} = -0.81\\)\n\n\nE\n150\n\\(\\frac{150-95}{31} = 1.77\\)\n\n\nF\n90\n\\(\\frac{90-95}{31} = -0.16\\)\n\n\n\n\nExercise 20.4  \n\n\n\nProperty\nSize (m²)\nNumber of rooms\nPrice (k €)\n\n\n\n\nA\n60\n2\n320\n\n\nB\n80\n3\n400\n\n\nC\n120\n5\n350\n\n\nD\n70\n2\n310\n\n\nE\n150\n6\n600\n\n\nF\n90\n4\n330\n\n\n\nStandardise the “Number of Rooms” series.\n\nThat is it for scaling methods, well done for making it this far! The following section will cover some of the practical details, but no more maths, I promise.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Numerical Feature Scaling</span>"
    ]
  },
  {
    "objectID": "chapters/scaling.html#information-leakage",
    "href": "chapters/scaling.html#information-leakage",
    "title": "20  Numerical Feature Scaling",
    "section": "20.4 Information Leakage",
    "text": "20.4 Information Leakage\nData preprocessing is a common source of information leakage. Descriptive statistics like the minimum, maximum, mean and standard deviation of a series are a source of information.\nWhen applying Min-Max Scaling or Standardisation, use descriptive statistics (like min and max) computed from the training set only. When generating predictions on the test set, preprocess the data using the descriptive statistics of the training set.\nAs an example, for Min-Max Scaling, you would use the minimum and maximum of the training set to preprocess the test data. This sounds counter-intuitive and a lot of work, why should we bother?\nThe answer to that question can be found in the reason we split the data into train and test sets. We want to estimate how the model would perform on unseen data.\nIn the case of property pricing, imagine you have trained your model using Min-Max Scaler, and that a new property comes in, ready to be priced. How would you scale the features (area and number of rooms) of this new property?\nThe descriptive statistics of this property are irrelevant as you only have a single observation. You would use the min and max of the training data, the only data you have. Because the purpose of the test set is to simulate what would happen at prediction time, it makes sense to treat it in the same way.\nIn Machine Learning jargon, these scaling methods are fitted to the training data, and applied to the test set. The transformation is based on metrics calculated on the training set and applied to the test set.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Numerical Feature Scaling</span>"
    ]
  },
  {
    "objectID": "chapters/scaling.html#final-thoughts",
    "href": "chapters/scaling.html#final-thoughts",
    "title": "20  Numerical Feature Scaling",
    "section": "20.5 Final Thoughts",
    "text": "20.5 Final Thoughts\nSome models like KNN require numerical features to be on the same scale. Otherwise, features with the largest variance (here surface area) will dominate distance calculations. This chapter reviewed two ways to scale numerical features:\n\nMin-Max Scaling\nStandardisation\n\nBoth of these need to be applied with care to avoid information leakage between the train and test sets. The way to do so is to use the descriptive statistics of the training set to preprocess the test data. This is what happens at inference.\nThis is it, you made it! This was the last chapter of the Data Preprocessing section. The next section will put everything together - applying the book’s content to an end-to-end Machine Learning project.",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Numerical Feature Scaling</span>"
    ]
  },
  {
    "objectID": "chapters/scaling.html#solutions",
    "href": "chapters/scaling.html#solutions",
    "title": "20  Numerical Feature Scaling",
    "section": "20.6 Solutions",
    "text": "20.6 Solutions\n\nSolution 20.1. Exercise 20.1\nGiven the data:\n\n\n\nProperty\nSize (m²)\nNumber of rooms\n\n\n\n\nA\n60\n2\n\n\nB\n80\n3\n\n\nC\n120\n5\n\n\nD\n70\n2\n\n\nE\n150\n6\n\n\nF\n90\n4\n\n\n\nMin-Max Scaling formula: \\[\n\\text{Scaled Value} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n\\]\n\nSurface Area (Size in m²)\n\n\n\\(x_{\\text{min}} = 60\\)\n\\(x_{\\text{max}} = 150\\)\n\n\n\n\n\n\n\n\n\nProperty\nSize (m²)\nScaled Size (\\(x'\\))\n\n\n\n\nA\n60\n\\(\\frac{60-60}{150-60} = 0\\)\n\n\nB\n80\n\\(\\frac{80-60}{150-60} = \\frac{20}{90} \\approx 0.222\\)\n\n\nC\n120\n\\(\\frac{120-60}{150-60} = \\frac{60}{90} \\approx 0.667\\)\n\n\nD\n70\n\\(\\frac{70-60}{150-60} = \\frac{10}{90} \\approx 0.111\\)\n\n\nE\n150\n\\(\\frac{150-60}{150-60} = 1\\)\n\n\nF\n90\n\\(\\frac{90-60}{150-60} = \\frac{30}{90} \\approx 0.333\\)\n\n\n\n\n\nNumber of Rooms\n\n\n\\(x_{\\text{min}} = 2\\)\n\\(x_{\\text{max}} = 6\\)\n\n\n\n\nProperty\nNumber of rooms\nScaled Rooms (\\(x'\\))\n\n\n\n\nA\n2\n\\(\\frac{2-2}{6-2} = 0\\)\n\n\nB\n3\n\\(\\frac{3-2}{6-2} = \\frac{1}{4} = 0.25\\)\n\n\nC\n5\n\\(\\frac{5-2}{6-2} = \\frac{3}{4} = 0.75\\)\n\n\nD\n2\n\\(\\frac{2-2}{6-2} = 0\\)\n\n\nE\n6\n\\(\\frac{6-2}{6-2} = 1\\)\n\n\nF\n4\n\\(\\frac{4-2}{6-2} = \\frac{2}{4} = 0.5\\)\n\n\n\nScaled Data\n\n\n\nProperty\nScaled Size\nScaled Rooms\n\n\n\n\nA\n0\n0\n\n\nB\n0.222\n0.25\n\n\nC\n0.667\n0.75\n\n\nD\n0.111\n0\n\n\nE\n1\n1\n\n\nF\n0.333\n0.5\n\n\n\n\n\nSolution 20.2. Exercise 20.2\n\n\n\nSeries 1\nSeries 2\n\n\n\n\n1\n4\n\n\n2\n8\n\n\n4\n10\n\n\n5\n12\n\n\n8\n16\n\n\n\n\\[\n\\text{Mean}_{x_2} = \\frac{4 + 8 + 10 + 12 + 16}{5} = 10\n\\]\n\n\nSolution 20.3. Exercise 20.3\n\n\n\nSeries 1\nSeries 2\n\n\n\n\n7\n4\n\n\n8\n8\n\n\n10\n10\n\n\n11\n12\n\n\n14\n16\n\n\n\nFor Series 2: [4, 8, 10, 12, 16]:\n\\[\n\\bar{x}_2 = \\frac{4 + 8 + 10 + 12 + 16}{5} = 10\n\\]\nVariance: \\[\\begin{aligned}\n\\mathrm{Var}(x_2) &= \\frac{(4-10)^2 + (8-10)^2 + (10-10)^2 + (12-10)^2 + (16-10)^2}{5} \\\\\n&= \\frac{36 + 4 + 0 + 4 + 36}{5} = \\frac{80}{5} = 16\n\\end{aligned}\n\\]\n\n\nSolution 20.4. Exercise 20.4\n\n\n\nProperty\nSize (m²)\nNumber of rooms\nPrice (k €)\n\n\n\n\nA\n60\n2\n320\n\n\nB\n80\n3\n400\n\n\nC\n120\n5\n350\n\n\nD\n70\n2\n310\n\n\nE\n150\n6\n600\n\n\nF\n90\n4\n330\n\n\n\n\\[\n\\text{Mean} = \\frac{2 + 3 + 5 + 2 + 6 + 4}{6} = \\frac{22}{6} \\approx 3.67\n\\] \\[\n\\begin{aligned}\n\\text{Variance} &= \\frac{(2 - 3.67)^2 + (3 - 3.67)^2 + (5 - 3.67)^2}{6} \\\\\n&\\quad + \\frac{(2 - 3.67)^2 + (6 - 3.67)^2 + (4 - 3.67)^2}{6} \\\\\n&= \\frac{13.33}{6} \\approx 2.22\n\\end{aligned}\n\\] \\[\\text{Standard Deviation} = \\sqrt{\\text{Variance}} = \\sqrt{2.22} \\approx 1.49\\]\nStandardised series\n\n\n\nProperty\nNumber of rooms\nScaled Number of Rooms\n\n\n\n\nA\n2\n\\(\\frac{2 - 3.67}{1.49} = -1.12\\)\n\n\nB\n3\n\\(\\frac{3 - 3.67}{1.49} = -0.45\\)\n\n\nC\n5\n\\(\\frac{5 - 3.67}{1.49} = 0.89\\)\n\n\nD\n2\n\\(\\frac{2 - 3.67}{1.49} = -1.12\\)\n\n\nE\n6\n\\(\\frac{6 - 3.67}{1.49} = 1.56\\)\n\n\nF\n4\n\\(\\frac{4 - 3.67}{1.49} = 0.22\\)",
    "crumbs": [
      "Data Preprocessing",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Numerical Feature Scaling</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html",
    "href": "chapters/end-to-end.html",
    "title": "21  End-to-End Machine Learning Project",
    "section": "",
    "text": "21.1 Problem Formulation\nIt’s now time to bring everything we’ve studied together to build a Machine Learning model to predict anything.\nThe most important step is to define the problem you want to solve as a supervised learning problem. Using examples studied in this book:\nThere are many other examples. The important part is to follow the supervised learning paradigm:\n\\[\n\\text{Input Features} \\rightarrow \\text{Model} \\rightarrow \\text{Prediction}\n\\]\nWhat would you like to predict to solve an everyday problem?",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#problem-formulation",
    "href": "chapters/end-to-end.html#problem-formulation",
    "title": "21  End-to-End Machine Learning Project",
    "section": "",
    "text": "What is the diagnosis of this suspicious mass?\nWhat is the price of this property?\n\n\n\n\n\nExercise 21.1 How would you solve these problems with a Machine Learning model? What would you predict?\n\nHow many bartenders do I need to hire for that date?\nShould I increase the stock of a particular item at my shop?\nIs this online transaction fraudulent?",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#evaluation-metric",
    "href": "chapters/end-to-end.html#evaluation-metric",
    "title": "21  End-to-End Machine Learning Project",
    "section": "21.2 Evaluation Metric",
    "text": "21.2 Evaluation Metric\nNow that the problem is formulated as a supervised learning task, let’s select an error metric to minimise. This error metric should reflect the real-world consequences of an error.\nIn the tumour diagnosis case, False Negatives, malignant tumours diagnosed as “benign”, can have fatal consequences. For that reason, Recall and F1 Score could be interesting metrics to track.\nIn the property pricing example, extreme pricing errors can have a negative impact on any real estate business. For this reason, the Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) could be good choices.\nYou may want to select several error metrics, but you will generally have to choose one to rank different models.\n\nExercise 21.2 Which error metrics would you choose for the following problems?\n\nSpam detection\nCredit card fraud detection\nCustomer churn prediction",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#data-collection",
    "href": "chapters/end-to-end.html#data-collection",
    "title": "21  End-to-End Machine Learning Project",
    "section": "21.3 Data Collection",
    "text": "21.3 Data Collection\nNow that you have a problem, gather as much relevant data as possible. Keeping supervised learning in mind, the goal is to give the model enough data to learn the relationship between input features and the target variable.\nLooking at the property pricing example, the model should have access to as many price-relevant features as possible:\n\nSurface Area\nNumber of Rooms\nNeighbourhood\nBalcony\nFloor\netc…\n\nIt is generally a good idea to include whatever information about the property that would help humans to price it, and a bit more. Why more? Because models can sometimes learn patterns that we cannot spot.\n\nExercise 21.3 Beer sales forecasting: Imagine you want to forecast beer sales at your bar. What features would you choose?",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#partitioning-the-data",
    "href": "chapters/end-to-end.html#partitioning-the-data",
    "title": "21  End-to-End Machine Learning Project",
    "section": "21.4 Partitioning the Data",
    "text": "21.4 Partitioning the Data\nOnce you have gathered the data, and before you start data preprocessing, it is critical to set aside a portion of the data for testing. You could either take a random sample of the data or use a time cut-off to mimic the model’s prediction conditions.\nIf you develop a property pricing model, you may want to keep the latest weeks of your training set as a test set, to make sure that the model does not have access to future price trends in training. For the tumour diagnosis case, a random sample of the entire dataset should be enough.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#data-preprocessing",
    "href": "chapters/end-to-end.html#data-preprocessing",
    "title": "21  End-to-End Machine Learning Project",
    "section": "21.5 Data Preprocessing",
    "text": "21.5 Data Preprocessing\nYou will most likely have to clean and preprocess the data you have gathered:\n\nIs there missing data?\nAre the numeric values on the same scale?\nHow do you want to handle date features?\nAre there categorical variables to process?\n\nNote: different models sometimes require different preprocessing. As this is an introductory text, we will leave finer distinctions to more advanced material. As a quick example, unlike K-Nearest Neighbours, Decision Trees do not require numerical feature scaling.\nOnce you’ve preprocessed the training data, apply the same transformations on the test set, using the statistics computed with the training data. If this is not clear enough, you can refer to the Data Preprocessing section.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#model-evaluation-and-selection",
    "href": "chapters/end-to-end.html#model-evaluation-and-selection",
    "title": "21  End-to-End Machine Learning Project",
    "section": "21.6 Model Evaluation and Selection",
    "text": "21.6 Model Evaluation and Selection\nThis is already a lot of work, and we still haven’t trained a single Machine Learning model. Welcome to the reality of Machine Learning professionals. A lot of our time is spent formulating problems, gathering and preprocessing data.\nNow, train a KNN model and a Decision Tree model on the training data. You can then use both of these models to generate predictions on the test set.\nWith these predictions, compare the error metric of both models and pick the best one! You can then use this model to generate predictions on unseen observations. The goal of this book.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#final-thoughts",
    "href": "chapters/end-to-end.html#final-thoughts",
    "title": "21  End-to-End Machine Learning Project",
    "section": "21.7 Final Thoughts",
    "text": "21.7 Final Thoughts\nThis chapter reviewed the main steps of solving a problem with Machine Learning predictions:\n\nProblem Formulation\nEvaluation Metric\nData Collection\nData Partition\nData Preprocessing\nModel Evaluation and Selection\n\nThat’s it! The purpose of this book was to give an idea of what building Machine Learning solutions can look like. Interested readers can explore how to put this knowledge in practice with further resources like: Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow (Géron 2022).\nThe Appendix below explores some of the differences between the description made above and Machine Learning in practice.\nThe following chapter links Generative AI models to traditional Machine Learning; showing how the models we use every day were built upon everything studied in this book.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#appendix-some-nuances",
    "href": "chapters/end-to-end.html#appendix-some-nuances",
    "title": "21  End-to-End Machine Learning Project",
    "section": "21.8 Appendix: Some Nuances",
    "text": "21.8 Appendix: Some Nuances\nThis chapter presents a simplified view of Machine Learning practice. If you are not interested in the details, you can skip directly to the conclusion.\nFirst, a lot of time would be spent on “Feature Engineering”, the task of computing features from raw data to make models more accurate.\nThen, model selection would involve more than just two models. The book compared two model families: KNN and Decision Trees. There are many more. Model selection would also involve “Hyperparameter Tuning”. These hyperparameters are the settings or configuration of the models. They determine how the models work. Some examples of hyperparameters include:\n\nKNN: The number of neighbours used to generate predictions. The text used 5, but 3, 10 or 15 can also be viable choices\nDecision Tree: the maximum depth of a tree, to avoid making too many data splits\n\nTo choose between all of these model families and hyperparameter combinations, a single test set is not enough. ML practitioners generally use cross-validation over the training set. The interested reader can find more on this at (scikit-learn contributors 2025).",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#final-thoughts-1",
    "href": "chapters/end-to-end.html#final-thoughts-1",
    "title": "21  End-to-End Machine Learning Project",
    "section": "21.9 Final Thoughts",
    "text": "21.9 Final Thoughts\nThis is it. You now have an intuition for the Machine Learning workflow. This was the core purpose of this book.\nBut what about Generative AI and LLMs? I thought you would never ask. If you are interested in understanding the many parallels between traditional Machine Learning and LLMs, read the next chapters.\nOtherwise, I wish you all the best on your Machine Learning journey.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/end-to-end.html#solutions",
    "href": "chapters/end-to-end.html#solutions",
    "title": "21  End-to-End Machine Learning Project",
    "section": "21.10 Solutions",
    "text": "21.10 Solutions\n\nSolution 21.1. Exercise 21.1\n\nHow much staff do I need to hire for that date at my bar? Beer sales forecasting\nShould I increase the stock of a particular item at my shop? Unit sales forecasting\nIs this online transaction fraudulent? Transaction classification as “legitimate”/“fraudulent”\n\n\n\nSolution 21.2. Exercise 21.2\n\nSpam detection: Precision, Recall Accuracy. A False Positive, a legitimate email marked as “spam” can be more problematic than a False Negative, a spam email classified as “legitimate”.\nCredit card fraud detection: Recall, Precision, F1 Score. The model should catch most fraudulent transactions (high Recall) while not having too many False Positives, as they could be an inconvenience to customers.\nCustomer churn prediction: F1 Score. A balance between Precision and Recall is needed to identify customers who are likely to churn without having too many False Positives.\n\n\n\nSolution 21.3. Exercise 21.3\nSome potential features for beer sales forecasting could be:\n\nDate/Time: Month, day of the week, time of day.\nWeather: Temperature, rain, sun.\nEvents: Are there any major events happening in the city, like a football match or a concert?\nPromotions: Is there a special offer on beer?\nHistorical sales data: Sales from previous days, weeks, or months.\n\n\n\n\n\n\nGéron, Aurélien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, & TensorFlow. O’Reilly Media. https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/.\n\n\nscikit-learn contributors. 2025. “Cross-Validation: Evaluating Estimator Performance.” scikit-learn. 2025. https://scikit-learn.org/stable/modules/cross_validation.html.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>End-to-End Machine Learning Project</span>"
    ]
  },
  {
    "objectID": "chapters/generative-ai.html",
    "href": "chapters/generative-ai.html",
    "title": "22  Machine Learning and Generative AI",
    "section": "",
    "text": "22.1 Starting with the Foundation\nThese days, it is hard to write anything about Machine Learning without mentioning Generative Artificial Intelligence (GenAI). GenAI took the world by storm with the public release of ChatGPT in 2022.\nBreaking down this term:\nMore generally, artificial intelligence can be thought of as the emulation of human intelligence, defined above. Further developments may invalidate this definition as the algorithms developed acquire capabilities going beyond human understanding.\nGenAI was one of the major technological breakthroughs of the early 21st century. It was built on a combination of the different Machine Learning types listed earlier.\nThe foundation of GenAI models is a next word (or token) predictor. When we send a request to a GenAI model, we send the following input:\nThe model takes this input and predicts The as output.\nThe answer would be a bit disappointing if it ended there. To continue, the model uses the previous input sequence and adds the The token it predicted:\nand outputs capital. This process goes on until the model predicts an &lt;end&gt; token, meaning that the response is over. The practice of appending a prediction to the original input sequence to generate another prediction is called auto-regression.\nNote: Large Language Models do not predict the next word but the next token. A token is a string of characters which could be a part of a word (“ing”) or a full word (“the”). We will stick to words in this simple explanation.\nEven though this is a classification task, the foundational training of Large Language Models is generally referred to as self-supervised learning, instead of just supervised learning. This is because, unlike the classification tasks listed in the previous section, there is no dataset of input and outputs. The model is simply trained to predict the next word of every sentence it finds.\nAs an example, if the training corpus contains the sentence:\nIt would include the following input/output pairs in its training:\nYou may see that some guesses are much easier than others.\nThe foundation model is trained with supervised learning as it learns from input/output pairs. Yet, this is self-supervised as these input/output pairs do not require special curation.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Machine Learning and Generative AI</span>"
    ]
  },
  {
    "objectID": "chapters/generative-ai.html#starting-with-the-foundation",
    "href": "chapters/generative-ai.html#starting-with-the-foundation",
    "title": "22  Machine Learning and Generative AI",
    "section": "",
    "text": "User: What is the capital of France?\n\nAssistant:\n\n\nUser: What is the capital of France?\n\nAssistant: The\n\n\n\nExercise 22.1 What type of prediction is the task of next word/token prediction?\n\n\nSolution 22.1. It is a classification task, with as many labels as possible words/tokens. This number is generally called the vocabulary size, which is just above 100,000 for OpenAI’s GPT-4 model.\n\n\n\n\n“The Second World War ended in 1945.”\n\n\n\n\n\nInput\nOutput\n\n\n\n\nThe\nSecond\n\n\nThe Second\nWorld\n\n\nThe Second World\nWar\n\n\nThe Second World War\nended\n\n\nThe Second World War ended\nin\n\n\nThe Second World War ended in\n1945\n\n\nThe Second World War ended in 1945\n.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Machine Learning and Generative AI</span>"
    ]
  },
  {
    "objectID": "chapters/generative-ai.html#is-next-word-prediction-enough",
    "href": "chapters/generative-ai.html#is-next-word-prediction-enough",
    "title": "22  Machine Learning and Generative AI",
    "section": "22.2 Is Next Word Prediction Enough?",
    "text": "22.2 Is Next Word Prediction Enough?\nIf you simply train a model to predict the next word using all of the text of the internet, you may come across surprising behaviours. As an example, the request:\n\n“3 x 1=”\n\ncould be answered:\n\n“3, 3 x 2 = 6, 3 x 3 = 9” …\n\nThis is helpful, but only “3” was needed there; unless you are in the business of writing schoolbooks.\nIn this case, only the answer of “3 x 1” was needed, and yet, most texts including the string “3 x 1” simply list the multiplication table for 3. You could validate this by looking for this sequence of characters in the books you have at home, many of which should be elementary Maths textbooks.\nMore work is needed to build a model that helps users and answers queries. There are two main ways to do this.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Machine Learning and Generative AI</span>"
    ]
  },
  {
    "objectID": "chapters/generative-ai.html#learning-from-questions-and-answers",
    "href": "chapters/generative-ai.html#learning-from-questions-and-answers",
    "title": "22  Machine Learning and Generative AI",
    "section": "22.3 Learning from Questions and Answers",
    "text": "22.3 Learning from Questions and Answers\nIn addition to using the text published on the internet for training, one could further train a foundation model to predict the next word of texts involving useful questions and answers. This could solve the issue described above. This additional training is called fine-tuning. It is called supervised fine-tuning as instead of learning from raw internet texts, it learns from selected question/answer interactions.\nAs an example, we can fine-tune the model with the following interactions:\nUser: 3 x 2 =\nAssistant: 3 x 2 = 6\nUser: 7 x 2 =\nAssistant: 7 x 2 = 14\netc.\nThis should train the model to reply to the user query instead of simply completing a text. Supervised fine-tuning does help, but is not enough to build the GenAI models we use every day.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Machine Learning and Generative AI</span>"
    ]
  },
  {
    "objectID": "chapters/generative-ai.html#optimising-for-helpfulness",
    "href": "chapters/generative-ai.html#optimising-for-helpfulness",
    "title": "22  Machine Learning and Generative AI",
    "section": "22.4 Optimising for Helpfulness",
    "text": "22.4 Optimising for Helpfulness\nThe goal of GenAI model providers is to offer models that are as helpful as possible. Achieving high degrees of helpfulness cannot be done through the pretraining of foundation models or supervised fine-tuning.\nFrom these two steps, helpfulness can emerge as a by-product (see previous section). Instead, could we optimise a model for helpfulness? If we could, what type of Machine Learning task could we use?\nHint: consider the degree of helpfulness as a reward the model can maximise by choosing the word to use in its reply.\nIf this makes you think of reinforcement learning, well done. Helpfulness is the reward the model would try to maximise, and the words it uses are its actions. But how would you measure helpfulness?\nFirst, you could ask human judges to rate each model output on a scale of 0 (not helpful) to 100 (unbelievably helpful). These may be problematic as the helpfulness scales of different humans may vary. Can we do better?\nEasier than a rating, human judges could simply choose the most helpful of two model outputs. This is called pairwise ranking. It has several advantages:\n\nIt is less cognitively demanding than rating or grading\nIt is more robust to variations of individual helpfulness scales\n\nDuring training, the model would learn to generate more helpful output. This is called Reinforcement Learning from Human Feedback (RLHF), the last building block of today’s GenAI models.\n\n\nBeyond human pairwise ranking\n\nOnce we gather enough examples of human pairwise assessments, we can train a model to predict the winner of two candidate suggestions.\nIn doing so, we get back to the supervised learning territory.\n\\[\n\\text{Input} \\longrightarrow \\text{Model} \\longrightarrow \\text{Prediction}\n\\]\nThe input here would be the two candidate suggestions, and the prediction would be the winning prediction (\\(0\\) for the first and \\(1\\) for the second). The training data is all the candidate suggestions and human assessments collected in the process described above.\nHowever, this is not a tabular Machine Learning problem. The candidate suggestions are two variable-length sequences of text. This is part of Natural Language Processing, a fascinating area of research.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Machine Learning and Generative AI</span>"
    ]
  },
  {
    "objectID": "chapters/generative-ai.html#final-thoughts",
    "href": "chapters/generative-ai.html#final-thoughts",
    "title": "22  Machine Learning and Generative AI",
    "section": "22.5 Final Thoughts",
    "text": "22.5 Final Thoughts\nIn essence, Generative AI models are next word predictors, further trained to provide more helpful answers to user questions. They are built on the same principles studied in this book. Next word prediction is yet another (complex) classification task.\nThis description of Large Language Models is simplified. Please refer to Hands-on Large Language Models (Alammar and Grootendorst 2023) for a more rigorous presentation.\n\n\n\n\nAlammar, Jay, and Maarten Grootendorst. 2023. Hands-on Large Language Models: Understand, Build, and Use LLMs. O’Reilly Media. https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/.\n\n\nCambridge Dictionary. 2024. “Intelligence.” Cambridge Dictionary. 2024. https://dictionary.cambridge.org/dictionary/english/intelligence.",
    "crumbs": [
      "Bringing it all Together",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Machine Learning and Generative AI</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alammar, Jay, and Maarten Grootendorst. 2023. Hands-on Large\nLanguage Models: Understand, Build, and Use LLMs. O’Reilly Media.\nhttps://www.oreilly.com/library/view/hands-on-large-language/9781098150952/.\n\n\nCambridge Dictionary. 2024a. “Algorithm.” Cambridge\nDictionary. 2024. https://dictionary.cambridge.org/dictionary/english/algorithm.\n\n\n———. 2024b. “Intelligence.” Cambridge Dictionary. 2024. https://dictionary.cambridge.org/dictionary/english/intelligence.\n\n\n———. 2024c. “Model.” Cambridge Dictionary. 2024. https://dictionary.cambridge.org/dictionary/english/model.\n\n\n———. 2024d. “True.” Cambridge Dictionary. 2024. https://dictionary.cambridge.org/dictionary/english/true.\n\n\n———. 2024e. “Truth.” Cambridge Dictionary. 2024. https://dictionary.cambridge.org/dictionary/english/truth.\n\n\nCDLI contributors. 2025. “Terms of Use.”\nhttps://cdli.earth/terms-of-use. https://cdli.earth/terms-of-use.\n\n\nDavenport, Thomas H., and D. J. Patil. 2012. “Data Scientist: The\nSexiest Job of the 21st Century.” Harvard Business\nReview 90 (10): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\nDhar, Vasant. 2013. “Data Science and Prediction.”\nCommunications of the ACM 56 (12): 64–73. https://doi.org/10.1145/2500499.\n\n\n“Exclusive Or.” n.d. https://en.wikipedia.org/wiki/Exclusive_or.\n\n\nGéron, Aurélien. 2022. Hands-on Machine Learning with Scikit-Learn,\nKeras, & TensorFlow. O’Reilly Media. https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/.\n\n\nMerriam-Webster Dictionary. 2024a. “Extrapolate.”\nMerriam-Webster. 2024. https://www.merriam-webster.com/dictionary/extrapolate.\n\n\n———. 2024b. “Intuition.” Merriam-Webster. 2024. https://www.merriam-webster.com/dictionary/intuition.\n\n\nParrish, Shane. 2016. “Daniel Kahneman and Herbert Simon on\nIntuition.” Farnam Street. 2016. https://fs.blog/daniel-kahneman-on-intuition/.\n\n\nRussell, Stuart J., and Peter Norvig. 2021. Artificial Intelligence:\nA Modern Approach. 4th ed. Hoboken: Pearson.\n\n\nscikit-learn contributors. 2025. “Cross-Validation: Evaluating\nEstimator Performance.” scikit-learn. 2025. https://scikit-learn.org/stable/modules/cross_validation.html.",
    "crumbs": [
      "References"
    ]
  }
]