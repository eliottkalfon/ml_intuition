---
title: "Voisins"
---

## Intuition

Tout commence à prendre forme. Il est maintenant temps de construire le premier modèle de prédiction de ce livre.
$$
\text{Entrée} \longrightarrow \text{Modèle} \longrightarrow \text{Prédictions}
$$

Un modèle de Machine Learning apprend la **relation** entre les paires entrée/sortie pour générer des prédictions sur de nouvelles entrées. Pour la première fois dans ce livre, ce chapitre explorera **comment ces modèles apprennent**.

En utilisant l'exemple du diagnostic de tumeurs :

![Jeu de données sur les tumeurs : Périmètre Moyen vs Surface Moyenne](/images/intro/scatter_unknown.png){width=60%}

Nous voulons prédire le diagnostic de l'observation marquée d'un point d'interrogation ($?$). Comment pourrait-on faire cela en utilisant la distance ?

Un bon début serait d'obtenir les **voisins les plus proches** de la nouvelle observation. Cela peut être fait en calculant la **distance** entre la nouvelle observation et les observations existantes, et en sélectionnant celles avec les distances les plus courtes.

En utilisant votre intuition, comment classeriez-vous cette nouvelle observation ?

Sur quoi notre intuition s'est-elle appuyée pour faire ce jugement ? Quand j'ai fait une prédiction, j'ai regardé les **voisins** de l'observation et appliqué un **vote majoritaire**.

## Algorithme

Comment pouvons-nous transformer cette intuition en un **programme**, un **algorithme** ?

L'objectif est de créer une liste de règles qui pourraient être exécutées par un ordinateur. Une telle liste pourrait ressembler à ceci :

- Trouver les 5 voisins les plus proches de l'observation
- Compter le nombre de voisins par classe
- La nouvelle observation est étiquetée avec la classe qui a le **plus grand nombre de voisins**

En Machine Learning, ce modèle est appelé K-Nearest Neighbours ou KNN. $K$ signifie simplement n'importe quel entier positif, faisant référence au nombre de voisins considérés (ici 5).

Ce modèle agit comme une **correspondance entre les valeurs des caractéristiques et une prédiction**. En utilisant l'algorithme décrit ci-dessus, **n'importe quelle tumeur** peut être assignée à un diagnostic. Cette correspondance peut être visualisée en appliquant ce vote majoritaire à chaque point et en le colorant selon son diagnostic :

![Carte de prédiction KNN](/images/neighbours/classification_boundary.png)

## Ajouter de la nuance

Tout cela est très excitant. Mais considérons l'exemple suivant du **point de vue d'un patient**. Imaginons que nous devions prédire le diagnostic des tumeurs suivantes :

![Voisins avec différents ratios de classes](/images/intro/five_neighbours.png)

Là, un simple vote majoritaire signifierait que l'observation avec 2 voisins malins pourrait être classée comme « bénigne ». En tant que patient, je préférerais avoir un deuxième avis.

Comme 2 des 5 voisins de cette observation sont malins, nous sommes **moins certains** que cette tumeur soit bénigne. La prédiction du modèle pourrait-elle refléter cette incertitude ?

Une façon de faire cela serait, au lieu de prendre le gagnant d'un vote majoritaire, d'estimer la **probabilité** qu'une tumeur soit maligne.

::: {.callout-note collapse="true"}
## Note : Définition de la Probabilité

La probabilité attribue un degré de croyance à un événement, entre 0 et 1. Par exemple, la probabilité d'obtenir un 4 sur un dé équilibré à six faces est $1/6 \approx 0.167$. La probabilité d'obtenir pile sur une pièce équilibrée est $1/2 = 0.5$.

:::

Avant de calculer la probabilité prédite de malignité, nous pourrions commencer par l'intuition. Si les 5 voisins de l'observation sont malins, le modèle devrait être sûr à 100% que la tumeur est maligne. Inversement, si aucun de ses 5 voisins n'est malin, le modèle assignerait une probabilité de 0% à la malignité.

Jusqu'ici tout va bien. Mais qu'en est-il de 3 et 2, ou 1 et 4 ? En calculant la moyenne, nous pourrions obtenir une probabilité prédite.

Si 3 voisins sont malins et 2 sont bénins, nous pourrions calculer la probabilité de malignité avec l'expression :

$$
\text{Probabilité} = \frac{3}{3+2} = \frac{3}{5} = 0.6 = 60\%
$$

:::{.exercise #exr-proba-neighbours}
Calculez la probabilité de malignité si quatre voisins sur cinq sont malins. Montrez qu'elle est de $80\%$
:::

En utilisant cette approche, le modèle peut produire des prédictions plus nuancées qui reflètent l'*incertitude* dans les données d'entraînement.

Cette approche peut être utilisée pour réviser la carte visualisée dans la section précédente. Au lieu d'être colorée par l'étiquette prédite, elle est colorée par la **probabilité prédite** de malignité ($1$ malin, $0$ bénin).

![Carte de probabilité KNN](/images/neighbours/classification_boundary_proba.png)

## De la classification à la régression

La section ci-dessus n'a montré que l'application de KNN à un problème de **classification**, prédisant le diagnostic d'une masse suspecte.

Le même modèle pourrait-il être utilisé pour un problème de régression (prédire une quantité continue) ?

Pour ce faire, tournons-nous vers le problème de l'estimation immobilière. Lors de la vente d'un bien, les clients ont besoin de savoir combien vaut leur propriété. Cela peut servir de base pour calculer le prix de mise en vente, prix auquel le bien est initialement proposé. Il est également dans l'intérêt de l'acheteur d'avoir une très bonne estimation de la valeur d'un bien avant d'accepter la transaction.

Comme expliqué dans le [chapitre Définir la Prédiction](/chapters_fr/defining-prediction.qmd), cette estimation peut être faite avec :

1. L'intuition : Par expérience, les agents immobiliers ont une compréhension du marché immobilier dans leur zone de spécialisation. Ils pourraient « savoir » combien vaut un bien
2. Systèmes basés sur des règles : Les analystes immobiliers pourraient construire des modèles qui évaluent les biens selon leurs caractéristiques. Une règle simple qui fonctionne étonnamment bien est : prix moyen au mètre carré * surface du bien

Pouvons-nous utiliser le modèle des Plus Proches Voisins pour apprendre à partir de données historiques et générer de nouvelles prédictions de prix pour des données non vues ? Une autre question orientée. Oui.

Pour simplifier, utilisons les caractéristiques suivantes : nombre de pièces et distance au centre ($km$).

Les données d'entraînement ressemblent à ceci :

![Données d'entraînement pour l'estimation immobilière](/images/neighbours/property_price_data.png)

Chaque point représente une propriété, colorée selon son prix. Le point d'interrogation ($?$) est le nouveau bien que nous voulons estimer.

Comment prédiriez-vous le prix de la nouvelle observation ? Vous pourriez commencer par sélectionner ses **5 voisins les plus proches**. Les cinq voisins les plus proches sont mis en évidence sur le graphique et présentés dans le tableau ci-dessous :

| Nombre de Pièces | Distance au Centre (km) | Prix (k€) |
|:---:|:---:|:---:|
| 3 | 2.7 | 341 |
| 3 | 2.4 | 401 |
| 3 | 2.9 | 358 |
| 4 | 3.0 | 377 |
| 4 | 2.3 | 425 |

Maintenant, comment pourrions-nous générer une seule prédiction à partir de cette liste ?

L'approche la plus simple serait de calculer une **moyenne** de tous les prix voisins et d'utiliser cela comme prédiction :

$$\text{Prix Prédit} = \frac{451 + 467 + 457 + 468 + 436}{5} = \frac{2279}{5} = 456$$

Et voilà, nous avons nos prédictions ! Ce processus devrait vous rappeler les prédictions de probabilité.

En utilisant cette méthode, nous pourrions également calculer une **carte des prix immobiliers** sur les deux caractéristiques : nombre de pièces et distance au centre.

![Carte de régression KNN](/images/neighbours/knn_regression_map.png)

La carte montre qu'il existe une **relation** positive entre le nombre de pièces et le prix, et une relation négative entre la distance au centre et le prix. Les propriétés les plus chères sont à la fois centrales et ont un nombre élevé de pièces. Le modèle (simple) construit nous permet de faire correspondre **n'importe quelle** combinaison donnée de nombre de pièces et de distance au centre à un prix.

## Réflexions Finales

KNN n'est que le premier des modèles de Machine Learning étudiés dans ce livre. Ce modèle apprend la relation entre les caractéristiques d'entrée et une cible, en utilisant des **calculs de distance** et la **moyenne sur les voisins**. Pouvez-vous penser à une façon d'utiliser KNN pour prédire quelque chose dans votre vie quotidienne ?

Nous avons notre premier modèle, mais est-il bon ? Nous explorerons cela dans le prochain chapitre.

## Exercice Pratique

:::{.exercise #exr-fraud-neighbours}
Supposons que vous construisez un modèle pour détecter les transactions frauduleuses. Vous utilisez deux caractéristiques, toutes deux mesurées sur une échelle de 0 à 100 :

- **Montant de la Transaction (\$)** (0–100)
- **Âge du Client (années)** (0–100)

Vous avez les 10 transactions suivantes dans vos données d'entraînement :

| Montant de la Transaction | Âge du Client | Frauduleuse ? |
|:------------------:|:------------:|:-----------:|
| 95                 | 22           | Oui         |
| 90                 | 25           | Oui         |
| 92                 | 23           | Oui         |
| 97                 | 21           | Oui         |
| 93                 | 24           | Oui         |
| 94                 | 23           | Non         |
| 20                 | 80           | Non         |
| 25                 | 78           | Non         |
| 18                 | 82           | Non         |
| 23                 | 77           | Non         |

Une nouvelle transaction se produit avec un montant de **93** et un âge client de **23**.

**Question :**
1. Calculez la distance entre chaque observation et la nouvelle transaction.
2. Identifiez les 5 voisins les plus proches et leurs étiquettes.
3. Quelle est la probabilité prédite que la nouvelle transaction soit frauduleuse ?

:::

## Solutions

:::{.solution #sol-proba-neighbours}
@exr-proba-neighbours

$$
\text{Probabilité} = \frac{4}{4+1} = \frac{4}{5} = 0.8 = 80\%
$$
:::

:::{.solution #sol-fraud-neighbours}
@exr-fraud-neighbours

La distance entre deux observations est calculée comme suit :

$$
\text{Distance} = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
$$

où $(x_1, y_1)$ sont les caractéristiques de l'observation d'entraînement, et $(x_2, y_2)$ sont les caractéristiques de la nouvelle transaction (93, 23).

Calculons la distance pour chaque observation :

| Montant Transaction | Âge Client | Frauduleuse ? | Distance à (93,23) |
|:--------:|:-------:|:-----------:|:-------------------:|
| 95 | 22 | Oui | $\sqrt{(95-93)^2 + (22-23)^2} \approx 2.24$ |
| 90 | 25 | Oui | $\sqrt{(90-93)^2 + (25-23)^2} \approx 3.61$ |
| 92 | 23 | Oui | $\sqrt{(92-93)^2 + (23-23)^2} = 1.00$ |
| 97 | 21 | Oui | $\sqrt{(97-93)^2 + (21-23)^2} \approx 4.47$ |
| 93 | 24 | Oui | $\sqrt{(93-93)^2 + (24-23)^2} = 1.00$ |
| 94 | 23 | Non | $\sqrt{(94-93)^2 + (23-23)^2} = 1.00$ |
| 20 | 80 | Non | $\sqrt{(20-93)^2 + (80-23)^2} \approx 92.6$ |
| 25 | 78 | Non | $\sqrt{(25-93)^2 + (78-23)^2} \approx 87.46$ |
| 18 | 82 | Non | $\sqrt{(18-93)^2 + (82-23)^2} \approx 95.43$ |
| 23 | 77 | Non | $\sqrt{(23-93)^2 + (77-23)^2} \approx 88.41$ |

Triées par distance (les plus proches en premier) :

| ID | Distance | Frauduleuse ? |
|:-:|:--------:|:-----------:|
| 3 | 1.00     | Oui         |
| 5 | 1.00     | Oui         |
| 6 | 1.00     | Non         |
| 1 | 2.24     | Oui         |
| 2 | 3.61     | Oui         |

Les 5 voisins les plus proches sont :

- 4 frauduleuses : ID 3, 5, 1, 2
- 1 non-frauduleuse : ID 6

Probabilité prédite de fraude :

$$
P(\text{Fraude}) = \frac{4}{5} = 0.8 = 80\%
$$
:::
