---
title: "Évaluer les Modèles de Classification"
format:
  html:
    code-fold: true
    number-sections: true
    exercise:
      solutions: true
---

Le chapitre précédent a décrit le processus de séparation d'un jeu de données entre ensembles d'entraînement et de test pour estimer la performance d'un modèle sur des **données non vues**. Ce processus d'évaluation comporte les quatre étapes suivantes :

- Mettre de côté une fraction du jeu de données d'entraînement comme ensemble de test
- Entraîner le modèle sur l'ensemble d'entraînement, ne pas utiliser l'ensemble de test
- Utiliser le modèle entraîné pour générer des prédictions sur l'ensemble de test
- Calculer la distance entre les prédictions et les étiquettes du test

Le chapitre précédent a laissé ce dernier point ouvert : comment déterminer si les prédictions d'un modèle sont **proches de la vérité** ? Ce chapitre abordera ce problème pour les modèles de classification. Le chapitre suivant explorera les modèles de régression.

Pour rappel, les tâches de classification concernent la prédiction d'étiquettes de catégorie telles que « spam » ou « malin ».

## Évaluation des Distances

Imaginons que nous avons entraîné deux modèles, Modèle A et Modèle B. Nous voulons maintenant déterminer lequel des deux est le meilleur.

Pour ce faire, nous retirons un ensemble de test des données d'entraînement, et générons des prédictions avec les deux modèles. Pour ces observations, nous avons également la vérité terrain, la vraie étiquette de chacune.

Nous obtenons les résultats suivants en générant des prédictions pour l'ensemble de test :

| Observation | Modèle A | Modèle B | Vérité |
|:-----------:|:-------:|:-------:|:-----:|
|      1      | $\times$ | $\circ$ | $\circ$ |
|      2      | $\times$ | $\circ$ | $\times$ |
|      3      | $\times$ | $\times$ | $\times$ |
|      4      | $\circ$ | $\circ$ | $\circ$ |
|      5      | $\circ$ | $\times$ | $\times$ |

Lequel des modèles est le plus exact ?

Vous pourriez commencer par compter le nombre d'erreurs de chaque modèle :

- Le Modèle A a fait deux erreurs : observations 1 et 5
- Le Modèle B a fait une erreur : observation 2

En d'autres termes, le Modèle A était correct trois fois sur cinq alors que le Modèle B était correct quatre fois sur cinq. Le Modèle B semble **plus correct en moyenne**.

::: {.callout-note collapse="true"}
## Taille de l'Échantillon

Un échantillon de cinq observations est trop petit pour tirer des conclusions sur la performance de ces deux modèles. En règle générale, **plus il y a d'échantillons de test, mieux c'est**. Cela nous permet d'obtenir une meilleure estimation de la performance du modèle. En pratique, les praticiens allouent entre **5% et 20%** des données d'entraînement comme ensemble de test.

Il y a toujours un **compromis** entre la quantité de données donnée au modèle pour l'entraînement et le nombre d'observations dans l'ensemble de test. Les données mises de côté pour les tests réduisent la taille des données d'entraînement disponibles pour le modèle, ce qui peut entraîner une performance moindre du modèle.

Les sections ultérieures de ce livre couvriront certaines méthodes conçues pour relever ce défi.

:::

Plus que simplement « trois sur cinq », nous pouvons dire que le Modèle A était correct **60% du temps**. Cette métrique est appelée l'**exactitude** d'un modèle de Machine Learning.

$$
\text{Exactitude} = \frac{\text{Nombre d'observations correctement prédites}}{\text{Nombre total d'observations}}
$$


:::{.exercise #exr-acc}
Calculez l'exactitude du Modèle B.
:::

Quand vous entendez les mots « Exactitude du Modèle » dans les médias, c'est de cela qu'il s'agit.

### Au-delà de l'Exactitude : Rappel et Précision

Mais l'exactitude est-elle suffisante ? Pour répondre à cette question, nous devons en poser une autre : Toutes les erreurs sont-elles identiques ? Ont-elles les mêmes **conséquences sur le monde** ?

Pour ce faire, allons au-delà des simples $\times$ et $\circ$ et entrons dans le monde du diagnostic tumoral. Là, une masse bénigne mal diagnostiquée comme tumeur maligne générerait du stress et des désagréments. Une tumeur maligne mal diagnostiquée comme bénigne pourrait avoir des **conséquences fatales**.

Considérons maintenant les deux modèles suivants, avec $\circ$ représentant une **masse bénigne** et $\times$ représentant une **tumeur maligne** :

| Observation | Modèle A  | Modèle B  | Vérité    |
|:-----------:|:--------:|:--------:|:-------:|
|      1      | $\circ$  | $\circ$  | $\circ$  |
|      2      | $\circ$  | $\circ$  | $\circ$  |
|      3      | $\circ$  | $\times$ | $\circ$  |
|      4      | $\circ$  | $\times$ | $\circ$  |
|      5      | $\circ$  | $\times$ | $\times$ |
|      6      | $\times$ | $\times$ | $\times$ |
|      7      | $\times$ | $\times$ | $\times$ |
|      8      | $\times$ | $\times$ | $\times$ |


:::{.exercise #exr-acc-ab}
Montrez que le modèle A a une exactitude plus élevée que le modèle B.
:::

Même si le modèle A a une exactitude plus élevée que le modèle B, il a mal classifié une tumeur maligne comme **bénigne** (Observation 5).

D'autre part, le modèle B a classifié deux masses bénignes comme malignes (Observations 3 et 4) mais **a détecté toutes les tumeurs malignes**.

Cela montre que l'Exactitude n'est qu'une partie du tableau. Comment pouvons-nous passer de la description ci-dessus à des métriques concrètes ?

#### Vocabulaire Utile

Avant d'entrer dans les métriques d'erreur, il est important d'introduire un certain vocabulaire.

En classification binaire, le modèle apprend à assigner des observations dans deux catégories, comme « bénin » et « malin » dans le cas du diagnostic tumoral, ou « spam » et « non-spam » pour le filtrage des emails.

Mathématiquement, ces deux étiquettes sont représentées par $1$ et $0$. Généralement, la classe que le modèle a été construit pour détecter se voit assigner $1$ et l'autre $0$. Dans le diagnostic tumoral, l'étiquette maligne se voit généralement attribuer le nombre $1$ car ce sont les cas pour lesquels le modèle a été conçu. De même, dans le filtrage des emails, l'étiquette « spam » est assignée au nombre $1$, car le modèle vise à identifier les messages spam pour les filtrer de la boîte de réception.

:::{.exercise #exr-fraud}
Si vous construisiez un modèle de détection de fraude pour une société de paiements en ligne, quelles étiquettes prédiriez-vous ? Laquelle serait assignée à $1$ et $0$ ?
:::

Dans l'exemple du diagnostic tumoral, une tumeur maligne correctement classifiée comme « maligne » est appelée un **Vrai Positif**. D'autre part, une tumeur maligne mal classifiée comme masse bénigne est un **Faux Négatif**.

Ici, les mots « positif » ou « négatif » ne sont associés à aucun jugement de valeur. Ce sont simplement une autre façon de dire $1$ ou $0$. Penser aux exemples médicaux peut avoir plus de sens ici. Quand un test médical est positif, cela signifie que la substance ciblée est **présente**. La même chose s'applique à la détection de spam. Une détection de spam positive signifie classer un email comme « spam ».

Pour revenir au jargon, un **Vrai Positif** est quand la variable d'intérêt est correctement détectée (par exemple, « spam » ou « malin »). Un **Faux Négatif** est quand la variable d'intérêt passe **inaperçue**. Par exemple, une tumeur maligne est mal diagnostiquée comme « bénigne », ou un email spam atterrit dans la boîte de réception.

Sur cette base, que seraient un **Faux Positif** et un **Vrai Négatif** ? Réfléchissez-y en termes de tumeurs et d'emails spam avant de continuer la lecture.

- **Faux Positif** : le modèle prédit **à tort** la présence de la variable d'intérêt, par exemple, un email légitime prédit comme « spam »
- **Vrai Négatif** : le modèle prédit **correctement** l'absence de la variable d'intérêt, par exemple, une masse bénigne est correctement classifiée comme masse bénigne

Ceux-ci peuvent être résumés dans le tableau suivant :

|                | Prédit Positif | Prédit Négatif |
|----------------|-------------------|-------------------|
| Réel Positif| Vrai Positif                | Faux Négatif                |
| Réel Négatif| Faux Positif                | Vrai Négatif                |

Ceci est également appelé la **Matrice de Confusion**.

Pour rendre cela plus concret, ce tableau pourrait être adapté à l'exemple du diagnostic tumoral :

|                | Prédit Malin | Prédit Bénin      |
|----------------|--------------------|----------------------|
| Réel Malin| Tumeur maligne correctement classifiée comme « maligne » | Tumeur maligne incorrectement classifiée comme « bénigne » |
| Réel Bénin   | Masse bénigne incorrectement classifiée comme « maligne »    | Masse bénigne correctement classifiée comme « bénigne »        |

Ou pour le filtrage de spam :

|                | Prédit Spam     | Prédit Non Spam    |
|----------------|-------------------|-----------------------|
| Réel Spam    | Spam correctement classifié comme « spam » | Spam incorrectement classifié comme « non spam » |
| Réel Non Spam| Email légitime incorrectement classifié comme « spam » | Email légitime correctement classifié comme « non spam » |

Pour tester votre compréhension, essayez de construire une Matrice de Confusion pour un modèle de détection de fraude de paiement.

|                      | Prédit Frauduleux | Prédit Légitime |
|----------------------|---------------------|---------------------|
| Réel Frauduleux    | Transaction frauduleuse correctement classifiée comme « frauduleuse » | Transaction frauduleuse incorrectement classifiée comme « légitime » |
| Réel Légitime    | Transaction légitime incorrectement classifiée comme « frauduleuse » | Transaction légitime correctement classifiée comme « légitime »   |

Maintenant que nous comprenons clairement le langage des Vrais/Faux Négatifs/Positifs, revenons à la mesure de la performance d'un modèle de Machine Learning.

#### Rappel

Dans l'exemple du diagnostic tumoral, nous voudrions un modèle qui détecterait toutes les tumeurs malignes. C'est parce que les Faux Négatifs, c'est-à-dire diagnostiquer à tort une tumeur maligne comme bénigne, peuvent avoir des conséquences **fatales**. Nous voulons comparer les performances du Modèle A et B :

| Observation | Modèle A  | Modèle B  | Vérité    |
|:-----------:|:--------:|:--------:|:-------:|
|      1      | $\circ$  | $\circ$  | $\circ$  |
|      2      | $\circ$  | $\circ$  | $\circ$  |
|      3      | $\circ$  | $\times$ | $\circ$  |
|      4      | $\circ$  | $\times$ | $\circ$  |
|      5      | $\circ$  | $\times$ | $\times$ |
|      6      | $\times$ | $\times$ | $\times$ |
|      7      | $\times$ | $\times$ | $\times$ |
|      8      | $\times$ | $\times$ | $\times$ |

Le **Rappel** est la métrique qui répond à la question : parmi tous les cas positifs, combien le modèle en a-t-il détectés ?

Reformulé pour l'exemple du diagnostic tumoral : parmi toutes les tumeurs malignes, combien le modèle en a-t-il détectées ?

Cela se calcule comme suit :

$$
\text{Rappel} = \frac{\text{Exemples positifs détectés par le Modèle}}{\text{Tous les exemples positifs}}
$$

En reformulant cela en termes de Vrais/Faux Positifs, nous obtenons :

$$
\text{Rappel} = \frac{\text{Vrais Positifs}}{\text{Vrais Positifs} + \text{Faux Négatifs}}
$$

Calcul du Rappel pour le modèle B :

$$
\text{Rappel}_{\text{Modèle B}} = \frac{4}{4 + 0} = 100\%
$$

:::{.exercise #exr-recall-a}
Calculez le rappel du Modèle A, et prouvez qu'il est de $75\%$.
:::

Dans le cas du diagnostic de maladies, le Rappel est une métrique critique car **manquer des cas positifs** peut avoir des conséquences graves sur la vie d'un patient.

#### Précision

Dans d'autres scénarios, lorsque le coût d'un Faux Positif est élevé, nous nous soucions de la **Précision** du modèle. En d'autres termes, nous voulons **éviter les Faux Positifs**.

La Précision répond à la question : Parmi toutes les observations prédites comme positives, combien étaient des Vrais Positifs, c'est-à-dire réellement positives ?

En construisant un algorithme de détection de spam, l'objectif est de classer les emails entrants comme « spam » ou « non-spam ». Chaque email classifié comme « spam » serait **filtré de la boîte de réception**. Conformément à la section précédente, le cas positif serait « spam », car c'est le cas qui nécessiterait une action ; filtrer les emails de la boîte de réception.

Dans le filtrage de spam, les Faux Positifs peuvent avoir des conséquences négatives sérieuses. Un jour, l'email d'une recruteuse s'est retrouvé dans mon dossier spam. Sans son rappel, je n'aurais jamais travaillé dans mon entreprise actuelle à cause d'une erreur de filtrage de spam.

Pour cette raison, la métrique la plus importante ici est la Précision : parmi les messages classifiés comme spam, combien étaient réellement des messages spam ?

Cela peut être calculé comme suit :

$$
\text{Précision} = \frac{\text{Nombre d'emails correctement classifiés comme spam}}{\text{Nombre total d'emails classifiés comme spam}}
$$

En reformulant cette expression dans le jargon de la Matrice de Confusion :

$$
\text{Précision} = \frac{\text{Vrai Positif}}{\text{Vrai Positif} + \text{Faux Positif}}
$$

En regardant l'exemple ci-dessous :

| Observation | Modèle A  | Modèle B  | Vérité    |
|:-----------:|:--------:|:--------:|:-------:|
|      1      | $\circ$  | $\circ$  | $\circ$  |
|      2      | $\circ$  | $\circ$  | $\circ$  |
|      3      | $\circ$  | $\times$ | $\circ$  |
|      4      | $\circ$  | $\times$ | $\circ$  |
|      5      | $\circ$  | $\times$ | $\times$ |
|      6      | $\times$ | $\times$ | $\times$ |
|      7      | $\times$ | $\times$ | $\times$ |
|      8      | $\times$ | $\times$ | $\times$ |

la précision du Modèle A est :

$$
\text{Précision}_\text{Modèle A} = \frac{3}{3 + 0} = 100\%
$$

:::{.exercise #exr-prec-b}
Calculez la Précision du Modèle B.
:::

#### Revisiter l'Exactitude

L'Exactitude, la première métrique d'erreur explorée dans ce chapitre, peut également être calculée avec les termes de la Matrice de Confusion.

Pour rappel, l'exactitude se calcule comme suit :
$$
\text{Exactitude} = \frac{\text{Nombre d'observations correctement prédites}}{\text{Nombre total d'observations}}
$$

En utilisant le langage des Vrais/Faux Positifs/Négatifs, elle peut être calculée avec la formule suivante :
$$
\text{Exactitude} = \frac{\text{Vrai Positif} + \text{Vrai Négatif}}{\text{Nombre d'Observations}} =
{}\frac{\text{VP} + \text{VN}}{\text{VP} + \text{FP} + \text{VN} + \text{FN}}
$$

Cette section a décrit l'Exactitude, le Rappel, la Précision et la Matrice de Confusion. Il est maintenant temps de les appliquer à la sélection de modèles ; pour choisir le modèle le plus performant pour une tâche donnée.

## Sélection Pratique de Modèles

Après avoir construit deux modèles de Machine Learning différents pour le diagnostic tumoral (A et B), vous obtenez les Matrices de Confusion suivantes :

**Modèle A**

|                | Prédit Malin | Prédit Bénin |
|----------------|--------------------|-----------------|
| Réel Malin| 40                 | 10              |
| Réel Bénin   | 10                 | 40              |

**Modèle B**

|                | Prédit Malin | Prédit Bénin |
|----------------|--------------------|-----------------|
| Réel Malin| 45                 | 5               |
| Réel Bénin   | 5                  | 45              |

Quel modèle choisissez-vous ?

Si vous avez choisi B, c'est correct. Pourquoi l'avez-vous choisi ?

:::{.exercise #exr-mod-cho}
Si vous ne l'avez pas encore fait, calculez l'Exactitude, la Précision et le Rappel des deux modèles.
:::

Pour rendre cette décision plus complexe, lequel des deux modèles suivants choisiriez-vous ?

**Modèle A**

|                | Prédit Malin | Prédit Bénin |
|----------------|--------------------|-----------------|
| Réel Malin| 48                 | 2               |
| Réel Bénin   | 18                 | 32              |

**Modèle B**

|                | Prédit Malin | Prédit Bénin |
|----------------|--------------------|-----------------|
| Réel Malin| 50                 | 0               |
| Réel Bénin   | 20                 | 30              |

Si vous avez choisi B, c'est encore correct. Pourquoi avez-vous choisi le modèle B ? Dans ce cas, le modèle B a la même Exactitude et le **Rappel le plus élevé**. Dans le cas du diagnostic tumoral, c'est probablement la métrique la plus importante à considérer.

Choisiriez-vous un modèle différent pour la détection de spam ? Probablement, car la **Précision** devient alors plus importante. Vous ne voulez pas que des emails légitimes se retrouvent dans votre dossier spam.

:::{.exercise #exr-mod-cho-2}
Calculez le Rappel et la Précision pour les modèles A et B
:::

## Probabilités et Évaluation des Modèles

Par souci de simplicité, ce chapitre n'a considéré que des **prédictions binaires** : soit malin soit bénin, soit spam soit non-spam.

Comme nous l'avons vu dans le [chapitre KNN](neighbours.qmd){target=_blank}, les modèles de classification peuvent également produire des **probabilités prédites**. Au lieu de simplement prédire une observation comme « maligne » ou « bénigne », le modèle peut produire une probabilité prédite de malignité.

Pour convertir ces probabilités en une étiquette binaire, un **seuil** de 0,5 est généralement utilisé. Toute probabilité prédite au-delà de ce seuil (ici 0,5) serait classifiée comme « maligne », sinon elle serait classifiée comme « bénigne ». Par exemple, une probabilité prédite de 48% serait classifiée comme « bénigne », tandis qu'une probabilité prédite de 51% comme « maligne ».

Le seuil peut être n'importe quel nombre entre 0 et 1. Plus le seuil est bas, **plus élevé** sera le nombre de Positifs. Dans l'exemple du diagnostic tumoral, cela signifierait un nombre plus élevé d'observations prédites comme « malignes ». D'autre part, un seuil plus élevé conduirait à **moins** de positifs. Dans l'exemple de la détection de spam, cela conduirait à moins d'emails classifiés comme « spam ».

Montrons cela avec un exemple :

| Observation | Probabilité Prédite | Seuil 0.3 | Seuil 0.5 | Seuil 0.7 | Vérité Terrain |
|:-----------:|:--------------------:|:-------------:|:-------------:|:-------------:|:------------:|
|      1      |        0.6           |   $\times$    |   $\times$    |   $\circ$     |   $\times$   |
|      2      |        0.75          |   $\times$    |   $\times$    |   $\times$    |   $\times$   |
|      3      |        0.2           |   $\circ$     |   $\circ$     |   $\circ$     |   $\circ$    |
|      4      |        0.8           |   $\times$    |   $\times$    |   $\times$    |   $\times$   |
|      5      |        0.4           |   $\times$    |   $\circ$     |   $\circ$     |   $\circ$    |
|      6      |        0.1           |   $\circ$     |   $\circ$     |   $\circ$     |   $\circ$    |


Ce qui se traduit par les Matrices de Confusion suivantes :

**Seuil 0.3**

|                | Prédit $\times$ | Prédit $\circ$ |
|----------------|-------------------|-------------------|
| Réel $\times$| 3                 | 0                 |
| Réel $\circ$ | 1                 | 2                 |

**Seuil 0.5**

|                | Prédit $\times$ | Prédit $\circ$ |
|----------------|-------------------|-------------------|
| Réel $\times$| 3                 | 0                 |
| Réel $\circ$ | 0                 | 3                 |

**Seuil 0.7**

|                | Prédit $\times$ | Prédit $\circ$ |
|----------------|-------------------|-------------------|
| Réel $\times$| 2                 | 1                 |
| Réel $\circ$ | 0                 | 3                 |

Le graphique linéaire suivant montre l'évolution des différentes métriques décrites dans ce chapitre lorsque le **seuil de prédiction augmente** :

![Visualisation du rappel et de la précision lorsque le seuil augmente](/images/evaluation/metrics.png)

Nous voyons que **diminuer** le seuil de prédiction à 0,3 a les effets suivants sur les trois métriques que nous avons analysées :

- **Augmentation** du Rappel : avec plus d'observations prédites comme « malignes », le rappel ne peut qu'augmenter ou rester constant
- **Diminution** de la Précision : avec plus d'observations prédites comme « malignes » malgré une faible probabilité prédite, le risque de mal classifier des observations comme « malignes » augmente

:::{.exercise #exr-thresh}
Décrivez l'effet de l'augmentation du seuil sur la Précision, le Rappel et l'Exactitude.
:::

Pour le diagnostic tumoral, un **seuil plus bas** peut avoir plus de sens. En revenant à l'exemple des Plus Proches Voisins, du point de vue d'un patient, si l'observation a deux voisins malins et trois bénins, pour une probabilité prédite de $2/5 = 40\%$, je voudrais quand même des vérifications supplémentaires.

Pour le filtrage de spam, un **seuil plus élevé** pourrait être préféré, pour réduire le risque qu'un email légitime soit filtré. Vous pourriez filtrer les résultats uniquement lorsqu'ils ont une probabilité prédite de 0,8 ou 0,9.

Cet exemple illustre le **compromis Précision/Rappel**. Fixer un seuil plus élevé augmentera la Précision et réduira le Rappel. Fixer un seuil plus bas réduira la Précision et augmentera le Rappel.

## Réflexions Finales

Cette section a montré comment évaluer un modèle de classification binaire. Il y a trois étapes dans ce processus :

- Mettre de côté une part des données d'entraînement comme ensemble de test
- En utilisant le modèle entraîné, générer des prédictions sur cet ensemble de test
- Calculer la distance à la vérité des prédictions générées en utilisant des métriques de performance telles que l'Exactitude, le Rappel et la Précision

Il est important de se rappeler qu'il n'y a pas de métrique de performance optimale. La meilleure métrique pour un problème dépend des **conséquences de l'erreur du modèle** sur le monde réel.

Après cette description de l'évaluation des modèles de **classification**, la section suivante explorera l'évaluation des **modèles de régression**.

## Solutions

:::{.solution #sol-acc}
@exr-acc

Exactitude du Modèle B $= \frac{4}{5} = 80\%$
:::

:::{.solution #sol-acc-ab}
@exr-acc-ab

Modèle A : Correct sur les observations 1, 2, 3, 4, 6, 7, 8 (7 sur 8). Incorrect sur 5.

Modèle B : Correct sur 1, 2, 5, 6, 7, 8 (6 sur 8). Incorrect sur 3 et 4.

Exactitude du Modèle A $= \frac{7}{8} = 87,5\%$

Exactitude du Modèle B $= \frac{6}{8} = 75\%$
:::

:::{.solution #sol-fraud}
@exr-fraud
Étiquettes :

- transaction frauduleuse $1$
- transaction non frauduleuse ou légitime $0$
:::

:::{.solution #sol-recall-a}
@exr-recall-a

Modèle A : Sur 4 tumeurs malignes (observations 5, 6, 7, 8), le Modèle A en a détecté 3 (6, 7, 8), Vrais Positifs. A manqué 5, un Faux Négatif.

$\text{Rappel}_{\text{Modèle A}} = \frac{3}{3 + 1} = 75\%$
:::

:::{.solution #sol-prec-b}
@exr-prec-b

$$
\text{Précision}_\text{Modèle B} = \frac{4}{4 + 2} = \frac{4}{6} \approx 67\%
$$
:::

:::{.solution #sol-mod-cho}
@exr-mod-cho

Pour le Modèle A :

- VP = 40
- VN = 40
- FP = 10
- FN = 10

$\text{Exactitude} = \frac{40 + 40}{100} = 80\%$

$\text{Précision} = \frac{40}{40 + 10} = \frac{40}{50} = 80\%$

$\text{Rappel} = \frac{40}{40 + 10} = \frac{40}{50} = 80\%$

Pour le Modèle B :

- VP = 45  
- VN = 45  
- FP = 5  
- FN = 5  

$\text{Exactitude} = \frac{45 + 45}{100} = 90\%$

$\text{Précision} = \frac{45}{45 + 5} = \frac{45}{50} = 90\%$

$\text{Rappel} = \frac{45}{45 + 5} = \frac{45}{50} = 90\%$
:::

:::{.solution #sol-mod-cho-2}
@exr-mod-cho-2

Pour le Modèle A :

- VP = 48
- VN = 32
- FP = 18
- FN = 2

$\text{Précision} = \frac{48}{48 + 18} = \frac{48}{66} \approx 73\%$

$\text{Rappel} = \frac{48}{48 + 2} = \frac{48}{50} = 96\%$

Pour le Modèle B :

- VP = 50
- VN = 30
- FP = 20
- FN = 0

$\text{Précision} = \frac{50}{50 + 20} = \frac{50}{70} \approx 71\%$

$\text{Rappel} = \frac{50}{50 + 0} = \frac{50}{50} = 100\%$
:::

:::{.solution #sol-thresh}
@exr-thresh

Augmenter le seuil généralement :

- **Augmente** la Précision : moins d'observations sont classifiées comme positives, donc celles qui sont classifiées comme positives sont plus susceptibles d'être de vrais positifs
- **Diminue** le Rappel : plus de vrais positifs sont manqués, car le modèle devient plus conservateur
:::
